{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dangerous-lebanon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    div#notebook-container    { width: 95%; }\n",
       "    div#menubar-container     { width: 85%; }\n",
       "    div#maintoolbar-container { width: 99%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enlarging the screen is done!\n",
      "Libraries were imported successfully!\n",
      "Loading data from sql is done!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c91245aa8645a0a594c9c0da6017b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing the data set:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enlarging the screen\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 85%; }\n",
    "    div#maintoolbar-container { width: 99%; }\n",
    "</style>\n",
    "\"\"\"))\n",
    "\n",
    "print ('Enlarging the screen is done!')\n",
    "\n",
    "# Importing libraries\n",
    "\n",
    "import numpy\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "print ('Libraries were imported successfully!')\n",
    "\n",
    "# Loading data from sql\n",
    "\n",
    "Server = 'LAPTOP-I7NEB9V3\\SQLEXPRESS'\n",
    "Database = 'Geopattern'\n",
    "Driver = 'ODBC Driver 17 for SQL Server'\n",
    "Database_Connection = f'mssql://@{Server}/{Database}?driver={Driver}'\n",
    "\n",
    "engine = create_engine(Database_Connection)\n",
    "connection = engine.connect()\n",
    "\n",
    "df_my_data = pd.read_sql_query (\n",
    "    \"select * from my_data\", connection)\n",
    "\n",
    "df_Authors = pd.read_sql_query (\n",
    "    \"select * from my_data_Authors\", connection)\n",
    "\n",
    "df_North_America = pd.read_sql_query (\n",
    "    \"select distinct my_data.EID, my_data.Author_ID, my_data.Year, my_data.Country from my_data join \\\n",
    "        (select distinct EID from my_data \\\n",
    "        except \\\n",
    "        (select distinct EID from my_data \\\n",
    "        where Country not like 'Canada' and Country not like 'United States' or Country is Null)) a \\\n",
    "        on my_data.EID = a.EID\", connection)\n",
    "\n",
    "df_North_America = df_North_America.merge(df_Authors, on = 'Author_ID', how = 'left')\n",
    "df_North_America = df_North_America[~((df_North_America.Country_y != 'Canada') & (df_North_America.Country_y != 'United States'))]\n",
    "df_North_America = df_North_America[['EID', 'Author_ID', 'Year', 'Country_x']]\n",
    "df_North_America.rename(columns={'Country_x' : 'Country'}, inplace = True)\n",
    "\n",
    "df_North_America.Year = df_North_America.Year.astype(int)\n",
    "\n",
    "df_LDA = pd.read_csv(r'C:\\Users\\moham\\Dropbox\\QSE\\Thesis\\Geopattern\\My data\\df_LDA.csv')\n",
    "df_LDA.set_index('EID', inplace = True)\n",
    "\n",
    "\n",
    "print ('Loading data from sql is done!')\n",
    "\n",
    "width_ind = 3\n",
    "width_dep = 2\n",
    "\n",
    "df_data_set = pd.DataFrame()\n",
    "\n",
    "list_years = []\n",
    "\n",
    "for yrs in range(21 - width_ind - width_dep):\n",
    "    list_years.append(2000 + yrs)\n",
    "\n",
    "\n",
    "for yr in tqdm(list_years, desc = 'Preparing the data set'):\n",
    "\n",
    "\n",
    "    Ind_win_start = yr\n",
    "    Ind_win_end = Ind_win_start + width_ind - 1\n",
    "\n",
    "    dep_win_start = Ind_win_end + 1\n",
    "    dep_win_end = dep_win_start + width_dep - 1\n",
    "\n",
    "    df_ind = df_North_America[(df_North_America.Year > (Ind_win_start - 1)) & (df_North_America.Year <= Ind_win_end)]\n",
    "    df_dep = df_North_America[(df_North_America.Year > (dep_win_start - 1)) & (df_North_America.Year <= dep_win_end)]\n",
    "\n",
    "    #     df_ind.rename({'EID' : 'EIDs_ind'}, axis = 1, inplace = True)\n",
    "    #     df_dep.rename({'EID' : 'EIDs_dep'}, axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "    # Targets.................................................................................................. \n",
    "\n",
    "    list_authors_dep = df_dep.Author_ID.unique().tolist()\n",
    "    list_authors_ind = df_ind.Author_ID.unique().tolist()\n",
    "    \n",
    "    list_authors = list(set(list_authors_dep).intersection(list_authors_ind))\n",
    "    list_authors.sort()\n",
    "\n",
    "    df_authors_dep = pd.DataFrame(data = list_authors, columns = ['Author_ID'])\n",
    "    df_authors_dep.insert(1,'EIDs','')\n",
    "\n",
    "    df_authors_dep = df_authors_dep.merge(df_Authors, on = 'Author_ID', how = 'left')\n",
    "\n",
    "    authors_dep = []\n",
    "    for i in range (df_authors_dep.shape[0]):\n",
    "        for j in range (df_authors_dep.shape[0]):\n",
    "            if j > i:\n",
    "                authors_dep.append((df_authors_dep['Author_ID'][i] + df_authors_dep['Author_ID'][j], df_authors_dep['Author_ID'][i], df_authors_dep['Author_ID'][j]))\n",
    "\n",
    "    df_data_set_dep = pd.DataFrame(data = authors_dep, columns=['Author_1_2', 'Author_1', 'Author_2'])\n",
    "\n",
    "    df_data_set_dep = df_data_set_dep.set_index(['Author_1_2'])\n",
    "\n",
    "    df_data_set_dep.insert(2,'number_of_collaborations',0)\n",
    "    df_data_set_dep.insert(3,'collaboration_binary',0)\n",
    "\n",
    "    df_dep.reset_index(inplace = True)\n",
    "\n",
    "    df_authors_dep['EIDs'] = ''\n",
    "    for i in range (df_authors_dep.shape[0]):\n",
    "        for j in range (df_dep.shape[0]):\n",
    "            if df_authors_dep['Author_ID'][i] == df_dep['Author_ID'][j]:\n",
    "                df_authors_dep['EIDs'][i] = str (df_authors_dep['EIDs'][i]) + ';' + str (df_dep['EID'][j])\n",
    "\n",
    "    res = []\n",
    "\n",
    "    for i in range (df_authors_dep.shape[0]):\n",
    "        l = df_authors_dep['EIDs'][i].split(';')\n",
    "        l.remove('')\n",
    "        res.append((df_authors_dep['Author_ID'][i],set(l)))\n",
    "\n",
    "\n",
    "    collab_matrix = np.zeros((df_authors_dep.shape[0],df_authors_dep.shape[0]))\n",
    "\n",
    "    for i in range (len(res)):\n",
    "        for j in range (len(res)):\n",
    "            collab_matrix[i,j] = len(res[i][1].intersection(res[j][1]))\n",
    "\n",
    "    collab_list = []\n",
    "    for i in range (collab_matrix.shape[0]):\n",
    "        for j in range (collab_matrix.shape[0]):\n",
    "            if j > i:\n",
    "                if collab_matrix[i,j] != 0:\n",
    "                    collab_list.append((df_authors_dep['Author_ID'][i] + df_authors_dep['Author_ID'][j], df_authors_dep['Author_ID'][i], df_authors_dep['Author_ID'][j], collab_matrix[i,j]))\n",
    "\n",
    "    df_collab = pd.DataFrame(data = collab_list, columns=['Author_1_2', 'Author_1', 'Author_2', 'number_of_collaborations'])\n",
    "\n",
    "    df_collab = df_collab.set_index(['Author_1_2'])\n",
    "\n",
    "\n",
    "    for i in df_data_set_dep.index:\n",
    "        try:\n",
    "            df_data_set_dep.loc[i,'number_of_collaborations'] = df_collab.loc[i,'number_of_collaborations']\n",
    "        except:\n",
    "            df_data_set_dep.loc[i,'number_of_collaborations'] = 0\n",
    "\n",
    "\n",
    "    df_data_set_dep.collaboration_binary = df_data_set_dep.number_of_collaborations.map(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "\n",
    "    # Features............................................................................................\n",
    "\n",
    "    df_authors_ind = pd.DataFrame(data = list_authors, columns = ['Author_ID'])\n",
    "    df_authors_ind.insert(1,'EIDs','')\n",
    "    df_authors_ind.insert(2,'partners',0)\n",
    "    df_authors_ind.insert(3,'topic_1',0)\n",
    "    df_authors_ind.insert(4,'topic_2',0)\n",
    "    df_authors_ind.insert(5,'topic_3',0)\n",
    "    df_authors_ind.insert(6,'topic_4',0)\n",
    "    df_authors_ind.insert(7,'topic_5',0)\n",
    "    df_authors_ind.insert(8,'topic_6',0)\n",
    "    df_authors_ind.insert(9,'topic_7',0)\n",
    "    df_authors_ind.insert(10,'topic_8',0)\n",
    "    df_authors_ind.insert(11,'topic_9',0)\n",
    "\n",
    "\n",
    "    df_authors_ind = df_authors_ind.merge(df_Authors, on = 'Author_ID', how = 'left')\n",
    "\n",
    "    df_authors_ind.reset_index(inplace = True)\n",
    "    df_ind.reset_index(inplace = True)\n",
    "\n",
    "    df_authors_ind['EIDs'] = ''\n",
    "\n",
    "    for i in range (df_authors_ind.shape[0]):\n",
    "        for j in range (df_ind.shape[0]):\n",
    "            if df_authors_ind['Author_ID'][i] == df_ind['Author_ID'][j]:\n",
    "                df_authors_ind['EIDs'][i] = str (df_authors_ind['EIDs'][i]) + ';' + str (df_ind['EID'][j])\n",
    "                df_authors_ind.loc[i, 'topic_1'] += df_LDA.loc[str (df_ind['EID'][j]), 'topic_1']\n",
    "                df_authors_ind.loc[i, 'topic_2'] += df_LDA.loc[str (df_ind['EID'][j]), 'topic_2']\n",
    "                df_authors_ind.loc[i, 'topic_3'] += df_LDA.loc[str (df_ind['EID'][j]), 'topic_3']\n",
    "                df_authors_ind.loc[i, 'topic_4'] += df_LDA.loc[str (df_ind['EID'][j]), 'topic_4']\n",
    "                df_authors_ind.loc[i, 'topic_5'] += df_LDA.loc[str (df_ind['EID'][j]), 'topic_5']\n",
    "                df_authors_ind.loc[i, 'topic_6'] += df_LDA.loc[str (df_ind['EID'][j]), 'topic_6']\n",
    "                df_authors_ind.loc[i, 'topic_7'] += df_LDA.loc[str (df_ind['EID'][j]), 'topic_7']\n",
    "                df_authors_ind.loc[i, 'topic_8'] += df_LDA.loc[str (df_ind['EID'][j]), 'topic_8']\n",
    "                df_authors_ind.loc[i, 'topic_9'] += df_LDA.loc[str (df_ind['EID'][j]), 'topic_9']\n",
    "\n",
    "    for i in range (df_authors_ind.shape[0]):\n",
    "        summ = df_authors_ind.loc[i,'topic_1'] + df_authors_ind.loc[i,'topic_2'] + df_authors_ind.loc[i,'topic_3'] + df_authors_ind.loc[i,'topic_4'] + df_authors_ind.loc[i,'topic_5'] + df_authors_ind.loc[i,'topic_6'] + df_authors_ind.loc[i,'topic_7'] + df_authors_ind.loc[i,'topic_8'] + df_authors_ind.loc[i,'topic_9']\n",
    "        if summ > 0:\n",
    "            df_authors_ind.loc[i,'topic_1'] = df_authors_ind.loc[i,'topic_1']/summ\n",
    "            df_authors_ind.loc[i,'topic_2'] = df_authors_ind.loc[i,'topic_2']/summ\n",
    "            df_authors_ind.loc[i,'topic_3'] = df_authors_ind.loc[i,'topic_3']/summ\n",
    "            df_authors_ind.loc[i,'topic_4'] = df_authors_ind.loc[i,'topic_4']/summ\n",
    "            df_authors_ind.loc[i,'topic_5'] = df_authors_ind.loc[i,'topic_5']/summ\n",
    "            df_authors_ind.loc[i,'topic_6'] = df_authors_ind.loc[i,'topic_6']/summ\n",
    "            df_authors_ind.loc[i,'topic_7'] = df_authors_ind.loc[i,'topic_7']/summ\n",
    "            df_authors_ind.loc[i,'topic_8'] = df_authors_ind.loc[i,'topic_8']/summ\n",
    "            df_authors_ind.loc[i,'topic_9'] = df_authors_ind.loc[i,'topic_9']/summ\n",
    "\n",
    "    res = []\n",
    "\n",
    "    for i in range (df_authors_ind.shape[0]):\n",
    "        l = df_authors_ind['EIDs'][i].split(';')\n",
    "        l.remove('')\n",
    "        res.append((df_authors_ind['Author_ID'][i],set(l)))\n",
    "\n",
    "\n",
    "    collab_matrix = np.zeros((df_authors_ind.shape[0],df_authors_ind.shape[0]))\n",
    "\n",
    "    for i in range (len(res)):\n",
    "        for j in range (len(res)):\n",
    "            collab_matrix[i,j] = len(res[i][1].intersection(res[j][1]))\n",
    "\n",
    "    collab_list = []\n",
    "    for i in range (collab_matrix.shape[0]):\n",
    "        for j in range (collab_matrix.shape[0]):\n",
    "            if j > i:\n",
    "                if collab_matrix[i,j] != 0:\n",
    "                    collab_list.append((df_authors_ind['Author_ID'][i] + df_authors_ind['Author_ID'][j], df_authors_ind['Author_ID'][i], df_authors_ind['Author_ID'][j], collab_matrix[i,j]))\n",
    "\n",
    "    df_collab = pd.DataFrame(data = collab_list, columns=['Author_1_2', 'Author_1', 'Author_2', 'number_of_collaborations'])\n",
    "\n",
    "\n",
    "    df_collab = df_collab.set_index(['Author_1_2'])\n",
    "\n",
    "    df_authors_ind ['partners'] = ''\n",
    "    for i in range (collab_matrix.shape[0]):\n",
    "        for j in range (collab_matrix.shape[0]):\n",
    "            if i != j:\n",
    "                if collab_matrix[i,j] != 0:\n",
    "                    df_authors_ind ['partners'][i] = str (df_authors_ind ['partners'][i]) + ';' + str (df_authors_ind ['Author_ID'][j])\n",
    "\n",
    "\n",
    "    df_data_set_ind = pd.DataFrame(data = authors_dep, columns=['Author_1_2', 'Author_1', 'Author_2'])\n",
    "\n",
    "    df_data_set_ind = df_data_set_ind.set_index(['Author_1_2'])\n",
    "\n",
    "    df_data_set_ind.insert(2,'TENB',0)\n",
    "    df_data_set_ind.insert(3,'Cog_Dist', '')\n",
    "    df_data_set_ind.insert(4,'Geo_Dist',0)\n",
    "    df_data_set_ind.insert(5,'Prov_Border',0)\n",
    "    df_data_set_ind.insert(6,'NotContig',0)\n",
    "\n",
    "    # TENB\n",
    "\n",
    "    res_p = []\n",
    "\n",
    "    for i in range (df_authors_ind.shape[0]):\n",
    "        l = df_authors_ind['partners'][i].split(';')\n",
    "        l.remove('')\n",
    "        res_p.append((df_authors_ind['Author_ID'][i],set(l)))\n",
    "\n",
    "    common_partners_matrix = np.zeros((df_authors_ind.shape[0],df_authors_ind.shape[0]))\n",
    "\n",
    "    for i in range (len(res_p)):\n",
    "        for j in range (len(res_p)):\n",
    "            common_partners_matrix[i,j] = len(res_p[i][1].intersection(res_p[j][1]))\n",
    "\n",
    "\n",
    "    df_common_partners = pd.DataFrame([])\n",
    "    df_common_partners.insert(0,'Author_1_2','')\n",
    "    df_common_partners.insert(1,'Author_1','')\n",
    "    df_common_partners.insert(2,'Author_2','')\n",
    "    df_common_partners.insert(3,'Common_partners',{})\n",
    "    df_common_partners.insert(4,'TENB',float)\n",
    "\n",
    "\n",
    "    for i in range (common_partners_matrix.shape[0]):\n",
    "        for j in range (common_partners_matrix.shape[0]):\n",
    "            if j > i:\n",
    "                if common_partners_matrix[i,j] != 0:\n",
    "                    df_common_partners = df_common_partners.append({'Author_1_2': df_authors_ind['Author_ID'][i] + df_authors_ind['Author_ID'][j],'Author_1': df_authors_ind['Author_ID'][i], 'Author_2': df_authors_ind['Author_ID'][j], 'Common_partners': res_p[i][1].intersection(res_p[j][1])}, ignore_index = True)\n",
    "\n",
    "\n",
    "    number_of_articles = []\n",
    "    for i in range (collab_matrix.shape[0]):\n",
    "        number_of_articles.append((df_authors_ind['Author_ID'][i], collab_matrix[i,i]))\n",
    "\n",
    "    df_number_of_articles = pd.DataFrame(data = number_of_articles, columns=['Author', 'number_of_articles'])\n",
    "\n",
    "    df_number_of_articles_ = df_number_of_articles.set_index(['Author'])\n",
    "\n",
    "    df_collab_ = df_collab.set_index(['Author_1' , 'Author_2'])\n",
    "\n",
    "\n",
    "    for i in range (df_common_partners.shape[0]):\n",
    "        n = len(df_common_partners['Common_partners'][i])\n",
    "        list_ENB = []\n",
    "        for j in range (n):\n",
    "            common_partner = list (df_common_partners['Common_partners'][i])[j]\n",
    "            d = df_number_of_articles_.loc[common_partner,'number_of_articles']\n",
    "            num_article_common_partner = int(d)\n",
    "            Auth_1 = df_common_partners['Author_1'][i]\n",
    "            try:\n",
    "                x = df_collab_.loc[(Auth_1,common_partner),'number_of_collaborations']\n",
    "            except KeyError:\n",
    "                x = 0\n",
    "            if x != 0:\n",
    "                num_collab_Auth_1 = x\n",
    "            else:\n",
    "                num_collab_Auth_1 = df_collab_.loc[(common_partner,Auth_1),'number_of_collaborations']\n",
    "            Auth_2 = df_common_partners['Author_2'][i]\n",
    "            try:\n",
    "                y = df_collab_.loc[(Auth_2,common_partner),'number_of_collaborations']\n",
    "            except KeyError:\n",
    "                y = 0\n",
    "            if y != 0:\n",
    "                num_collab_Auth_2 = y\n",
    "            else:\n",
    "                num_collab_Auth_2 = df_collab_.loc[(common_partner,Auth_2),'number_of_collaborations']\n",
    "            ENB = ((int(num_collab_Auth_1)) * (int(num_collab_Auth_2))) / num_article_common_partner\n",
    "            list_ENB.append(ENB)\n",
    "            TENB = sum(list_ENB)\n",
    "        df_common_partners['TENB'][i] = TENB\n",
    "\n",
    "    df_common_partners = df_common_partners.set_index(['Author_1_2'])\n",
    "\n",
    "    for i in df_common_partners.index:\n",
    "        df_data_set_ind.loc[i,'TENB'] = df_common_partners.loc[i,'TENB']\n",
    "\n",
    "    df_authors_ind = df_authors_ind.set_index('Author_ID')\n",
    "\n",
    "\n",
    "    #Cog_Dist\n",
    "\n",
    "    for i in df_data_set_ind.index:\n",
    "        a1 = df_authors_ind.loc[df_data_set_ind.loc[i, 'Author_1'], ['topic_1', 'topic_2', 'topic_3', 'topic_4','topic_5', 'topic_6','topic_7', 'topic_8','topic_9']]\n",
    "        a1=a1.tolist()\n",
    "        a2 = df_authors_ind.loc[df_data_set_ind.loc[i, 'Author_2'], ['topic_1', 'topic_2', 'topic_3', 'topic_4','topic_5', 'topic_6','topic_7', 'topic_8','topic_9']]\n",
    "        a2=a2.tolist()\n",
    "        if a1 != [0,0,0,0,0,0,0,0,0] and a2 != [0,0,0,0,0,0,0,0,0]:\n",
    "            cor = numpy.corrcoef(a1, a2)\n",
    "            df_data_set_ind.loc[i, 'Cog_Dist'] = 1 - cor[0][1]\n",
    "\n",
    "    # Geo_Dist\n",
    "\n",
    "    import math\n",
    "\n",
    "    def Geo_Distance (lat_1,lon_1,lat_2,lon_2):\n",
    "        R = 6373.0\n",
    "        lat1 = math.radians(lat_1)\n",
    "        lon1 = math.radians(lon_1)\n",
    "        lat2 = math.radians(lat_2)\n",
    "        lon2 = math.radians(lon_2)\n",
    "        dlon = lon2 - lon1\n",
    "        dlat = lat2 - lat1\n",
    "        a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
    "        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "        distance = R * c\n",
    "        return distance\n",
    "\n",
    "    dist = []\n",
    "\n",
    "    for i in df_data_set_ind.index:\n",
    "        author_1 = df_data_set_ind.Author_1[i]\n",
    "        author_2 = df_data_set_ind.Author_2[i]\n",
    "        auth_1_lat = df_authors_ind.Latitude[author_1]\n",
    "        auth_1_lng = df_authors_ind.Longitude[author_1]\n",
    "        auth_2_lat = df_authors_ind.Latitude[author_2]\n",
    "        auth_2_lng = df_authors_ind.Longitude[author_2]\n",
    "\n",
    "        dist.append((author_1, author_2, Geo_Distance(auth_1_lat,auth_1_lng,auth_2_lat,auth_2_lng)))\n",
    "\n",
    "    df_geo_dist = pd.DataFrame(data = dist, columns=['Author_1', 'Author_2', 'GeoDist'])\n",
    "\n",
    "    df_geo_dist.insert(3,'Author_1_2','')\n",
    "\n",
    "    df_geo_dist.Author_1_2 = df_geo_dist.Author_1 + df_geo_dist.Author_2\n",
    "\n",
    "    df_geo_dist.set_index('Author_1_2', inplace = True)\n",
    "\n",
    "    df_data_set_ind.Geo_Dist = df_geo_dist.GeoDist\n",
    "\n",
    "    df_temp = pd.merge(df_data_set_ind, df_authors_ind, left_on = 'Author_1', right_index = True, how = 'left')\n",
    "    df_temp.rename({'Province_code':'Province_code_1'}, axis = 1, inplace = True)\n",
    "    df_temp.drop(['EIDs','partners', 'Aff_ID', 'Latitude', 'Longitude'], axis = 1, inplace = True)\n",
    "    df_temp2 = pd.merge(df_temp, df_authors_ind, left_on = 'Author_2', right_index = True, how = 'left')\n",
    "    df_temp2.drop(['EIDs','partners', 'Aff_ID', 'Latitude', 'Longitude'], axis = 1, inplace = True)\n",
    "    df_temp2.rename({'Province_code':'Province_code_2'}, axis = 1, inplace = True)\n",
    "\n",
    "    def comparison_(x, y):\n",
    "        if x == y:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    df_temp2.Prov_Border = df_temp2.apply(lambda x: comparison_(x.Province_code_1, x.Province_code_2), axis = 1)\n",
    "\n",
    "    df_data_set_ind.Prov_Border = df_temp2.Prov_Border\n",
    "\n",
    "    list_prov = list(df_authors_ind.Province.unique())\n",
    "\n",
    "    dic_contig = {'Nova Scotia':['New Brunswick'],\n",
    "                 'New Brunswick':['Nova Scotia','Quebec', 'Maine'],\n",
    "                 'Newfoundland and Labrador':['Quebec'],\n",
    "                 'Quebec':['New Brunswick', 'Newfoundland and Labrador', 'Ontario', 'Maine', 'New York', 'Vermont', 'New Hampshire'],\n",
    "                 'Ontario':['Quebec', 'Manitoba', 'Michigan', 'Minnesota', 'New York', 'Ohio', 'Pennsylvania'],\n",
    "                 'Manitoba':['Ontario','Saskatchewan', 'Minnesota', 'North Dakota'],\n",
    "                 'Saskatchewan':['Manitoba','Alberta', 'Montana', 'North Dakota'],\n",
    "                 'Alberta':['Saskatchewan','British Columbia', 'Montana'],\n",
    "                 'British Columbia':['Alberta', 'Alaska', 'Montana', 'Washington', 'Idaho'],\n",
    "                 'Alabama': ['Florida', ' Georgia', ' Mississippi', ' Tennessee'],\n",
    "                 'Alaska': ['British Columbia'],\n",
    "                 'Arizona': ['California', ' Colorado', ' Nevada', ' New Mexico', ' Utah'],\n",
    "                 'Arkansas': ['Louisiana',' Mississippi',' Missouri',' Oklahoma',' Tennessee',' Texas'],\n",
    "                 'California': ['Arizona', ' Nevada', ' Oregon'],\n",
    "                 'Colorado': ['Arizona',' Kansas',' Nebraska',' New Mexico',' Oklahoma',' Utah',' Wyoming'],\n",
    "                 'Connecticut': ['Massachusetts', ' New York', ' Rhode Island'],\n",
    "                 'Delaware': ['Maryland', ' New Jersey', ' Pennsylvania'],\n",
    "                 'Florida': ['Alabama', ' Georgia'],\n",
    "                 'Georgia': ['Alabama',' Florida',' North Carolina',' South Carolina',' Tennessee'],\n",
    "                 'Hawaii': [''],\n",
    "                 'Idaho': ['Montana',' Nevada',' Oregon',' Utah',' Washington',' Wyoming', 'British Columbia'],\n",
    "                 'Illinois': ['Indiana',' Iowa',' Michigan',' Kentucky',' Missouri',' Wisconsin'],\n",
    "                 'Indiana': ['Illinois', ' Kentucky', ' Michigan', ' Ohio'],\n",
    "                 'Iowa': ['Illinois',' Minnesota',' Missouri',' Nebraska',' South Dakota',' Wisconsin'],\n",
    "                 'Kansas': ['Colorado', ' Missouri', ' Nebraska', ' Oklahoma'],\n",
    "                 'Kentucky': ['Illinois',' Indiana',' Missouri',' Ohio',' Tennessee',' Virginia',' West Virginia'],\n",
    "                 'Louisiana': ['Arkansas', ' Mississippi', ' Texas'],\n",
    "                 'Maine': ['New Hampshire', 'Quebec', 'New Brunswick'],\n",
    "                 'Maryland': ['Delaware', ' Pennsylvania', ' Virginia', ' West Virginia', 'District of Columbia'],\n",
    "                 'Massachusetts': ['Connecticut',' New Hampshire',' New York',' Rhode Island',' Vermont'],\n",
    "                 'Michigan': ['Illinois',' Indiana',' Minnesota',' Ohio',' Wisconsin', 'Ontario'],\n",
    "                 'Minnesota': ['Iowa',' Michigan',' North Dakota',' South Dakota',' Wisconsin', 'Manitoba', 'Ontario'],\n",
    "                 'Mississippi': ['Alabama', ' Arkanssas', ' Louisiana', ' Tennessee'],\n",
    "                 'Missouri': ['Arkansas',' Illinois',' Iowa',' Kansas',' Kentucky',' Nebraska',' Oklahoma',' Tennessee'],\n",
    "                 'Montana': ['Idaho', ' North Dakota', ' South Dakota', ' Wyoming', 'British Columbia', 'Alberta', 'Saskatchewan'],\n",
    "                 'Nebraska': ['Colorado',' Iowa',' Kansas',' Missouri',' South Dakota',' Wyoming'],\n",
    "                 'Nevada': ['Arizona', ' California', ' Idaho', ' Oregon', ' Utah'],\n",
    "                 'New Hampshire': ['Maine', ' Massachusetts', ' Vermont', 'Quebec'],\n",
    "                 'New Jersey': ['Delaware', ' New York', ' Pennsylvania'],\n",
    "                 'New Mexico': ['Arizona', ' Colorado', ' Oklahoma', ' Texas', ' Utah'],\n",
    "                 'New York': ['Connecticut',' Massachusetts',' New Jersey',' Pennsylvania',' Rhode Island',' Vermont', 'Ontario', 'Quebec'],\n",
    "                 'North Carolina': ['Georgia', ' South Carolina', ' Tennessee', ' Virginia'],\n",
    "                 'North Dakota': ['Minnesota', ' Montana', ' South Dakota', 'Saskatchewan', 'Manitoba'],\n",
    "                 'Ohio': ['Indiana',' Kentucky',' Michigan',' Pennsylvania',' West Virginia', 'Ontario'],\n",
    "                 'Oklahoma': ['Arkansas',' Colorado',' Kansas',' Missouri',' New Mexico',' Texas'],\n",
    "                 'Oregon': ['California', ' Idaho', ' Nevada', ' Washington'],\n",
    "                 'Pennsylvania': ['Delaware',' Maryland',' New Jersey',' New York',' Ohio',' West Virginia', 'Ontario'],\n",
    "                 'Rhode Island': ['Connecticut', ' Massachusetts', ' New York'],\n",
    "                 'South Carolina': ['Georgia', ' North Carolina'],\n",
    "                 'South Dakota': ['Iowa',' Minnesota',' Montana',' Nebraska',' North Dakota',' Wyoming'],\n",
    "                 'Tennessee': ['Alabama',' Arkansas',' Georgia',' Kentucky',' Mississippi',' Missouri',' North Carolina',' Virginia'],\n",
    "                 'Texas': ['Arkansas', ' Louisiana', ' New Mexico', ' Oklahoma'],\n",
    "                 'Utah': ['Arizona',' Colorado',' Idaho',' Nevada',' New Mexico',' Wyoming'],\n",
    "                 'Vermont': ['Massachusetts', ' New Hampshire', ' New York', 'Quebec'],\n",
    "                 'Virginia': ['Kentucky',' Maryland',' North Carolina',' Tennessee',' West Virginia', 'District of Columbia'],\n",
    "                 'Washington': ['Idaho', ' Oregon', 'British Columbia'],\n",
    "                 'West Virginia': ['Kentucky',' Maryland',' Ohio',' Pennsylvania',' Virginia'],\n",
    "                 'Wisconsin': ['Illinois', ' Iowa', ' Michigan', ' Minnesota'],\n",
    "                 'Wyoming': ['Colorado',' Idaho',' Montana',' Nebraska',' South Dakota',' Utah'],\n",
    "                 'District of Columbia': ['Maryland', 'Virginia']}\n",
    "\n",
    "    df_temp = pd.merge(df_data_set_ind, df_authors_ind, left_on = 'Author_1', right_index = True, how = 'left')\n",
    "    df_temp.rename({'Province':'Province_1'}, axis = 1, inplace = True)\n",
    "    df_temp.drop(['EIDs','partners', 'Aff_ID', 'Latitude', 'Longitude'], axis = 1, inplace = True)\n",
    "    df_temp2 = pd.merge(df_temp, df_authors_ind, left_on = 'Author_2', right_index = True, how = 'left')\n",
    "    df_temp2.drop(['EIDs','partners', 'Aff_ID', 'Latitude', 'Longitude'], axis = 1, inplace = True)\n",
    "    df_temp2.rename({'Province':'Province_2'}, axis = 1, inplace = True)\n",
    "\n",
    "    def contiguity_ (x, y):\n",
    "        c = dic_contig[x]\n",
    "        if c.count(y) == 1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    df_temp2.NotContig = df_temp2.apply(lambda x: contiguity_(x.Province_1, x.Province_2), axis = 1)\n",
    "\n",
    "    df_data_set_ind.NotContig = df_temp2.NotContig\n",
    "\n",
    "        # Province dummies\n",
    "\n",
    "    one_hot = pd.get_dummies(df_temp2[['Province_1','Province_2']])\n",
    "    df_data_set_ind = df_data_set_ind.join(one_hot)\n",
    "\n",
    "    df_data_set_ = df_data_set_dep.merge(df_data_set_ind, right_index = True, left_index = True, how = 'left')\n",
    "    df_data_set = pd.concat([df_data_set,df_data_set_])\n",
    "\n",
    "    \n",
    "df_data_set.reset_index(inplace = True)\n",
    "df_data_set.drop(['Author_1_y', 'Author_2_y'], axis = 1, inplace = True)\n",
    "df_data_set.rename({'Author_1_x' : 'Author_1'}, axis = 1, inplace = True)\n",
    "df_data_set.rename({'Author_2_x' : 'Author_2'}, axis = 1, inplace = True)\n",
    "df_data_set['Log_Geo_Dist'] = df_data_set.Geo_Dist.apply(lambda x :math.log1p(x))\n",
    "df_data_set.fillna(0, inplace = True)\n",
    "\n",
    "df_data_set.insert(123,'Top_regions',0)\n",
    "\n",
    "def Top_regions (reg1, reg2):\n",
    "    if reg1 == 1 and reg2 == 1:\n",
    "        return 1\n",
    "        \n",
    "df_data_set.Top_regions = df_data_set.apply(lambda x: Top_regions(x.Province_1_California, x.Province_2_California), axis = 1)\n",
    "df_data_set.Top_regions = df_data_set.apply(lambda x: Top_regions(x.Province_1_Pennsylvania, x.Province_2_Pennsylvania), axis = 1)\n",
    "df_data_set.Top_regions = df_data_set.apply(lambda x: Top_regions(x['Province_1_New York'], x['Province_2_New York']), axis = 1)\n",
    "\n",
    "df_data_set.Top_regions = df_data_set.apply(lambda x: Top_regions(x.Province_1_California, x.Province_2_Pennsylvania), axis = 1)\n",
    "df_data_set.Top_regions = df_data_set.apply(lambda x: Top_regions(x.Province_1_Pennsylvania, x.Province_2_California), axis = 1)\n",
    "\n",
    "df_data_set.Top_regions = df_data_set.apply(lambda x: Top_regions(x.Province_1_California, x['Province_2_New York']), axis = 1)\n",
    "df_data_set.Top_regions = df_data_set.apply(lambda x: Top_regions(x['Province_1_New York'], x.Province_2_California), axis = 1)\n",
    "\n",
    "df_data_set.Top_regions = df_data_set.apply(lambda x: Top_regions(x.Province_1_Pennsylvania, x['Province_2_New York']), axis = 1)\n",
    "df_data_set.Top_regions = df_data_set.apply(lambda x: Top_regions(x['Province_1_New York'], x.Province_2_Pennsylvania), axis = 1)\n",
    "\n",
    "df_data_set.Top_regions.fillna(0, inplace = True)\n",
    "\n",
    "# df_data_set.to_csv(r'C:\\Users\\moham\\Dropbox\\QSE\\Thesis\\Geopattern\\My data\\df_data_set_North_America.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "108713df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_of_collaborations</th>\n",
       "      <th>collaboration_binary</th>\n",
       "      <th>TENB</th>\n",
       "      <th>Cog_Dist</th>\n",
       "      <th>Geo_Dist</th>\n",
       "      <th>Prov_Border</th>\n",
       "      <th>NotContig</th>\n",
       "      <th>Province_1_Alberta</th>\n",
       "      <th>Province_1_Arizona</th>\n",
       "      <th>Province_1_California</th>\n",
       "      <th>...</th>\n",
       "      <th>Province_1_New Hampshire</th>\n",
       "      <th>Province_1_Vermont</th>\n",
       "      <th>Province_2_Montana</th>\n",
       "      <th>Province_2_Nebraska</th>\n",
       "      <th>Province_2_Nevada</th>\n",
       "      <th>Province_2_New Brunswick</th>\n",
       "      <th>Province_2_New Hampshire</th>\n",
       "      <th>Province_2_Vermont</th>\n",
       "      <th>Log_Geo_Dist</th>\n",
       "      <th>Top_regions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>253075.000000</td>\n",
       "      <td>253075.000000</td>\n",
       "      <td>253075.000000</td>\n",
       "      <td>253075.000000</td>\n",
       "      <td>253075.000000</td>\n",
       "      <td>253075.000000</td>\n",
       "      <td>253075.000000</td>\n",
       "      <td>253075.000000</td>\n",
       "      <td>253075.000000</td>\n",
       "      <td>253075.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>253075.000000</td>\n",
       "      <td>253075.000000</td>\n",
       "      <td>253075.000000</td>\n",
       "      <td>253075.000000</td>\n",
       "      <td>253075.000000</td>\n",
       "      <td>253075.000000</td>\n",
       "      <td>253075.000000</td>\n",
       "      <td>253075.000000</td>\n",
       "      <td>253075.000000</td>\n",
       "      <td>253075.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.007472</td>\n",
       "      <td>0.006302</td>\n",
       "      <td>0.016669</td>\n",
       "      <td>0.908712</td>\n",
       "      <td>1952.527368</td>\n",
       "      <td>0.945490</td>\n",
       "      <td>0.968847</td>\n",
       "      <td>0.016331</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>0.153761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>0.001525</td>\n",
       "      <td>0.001770</td>\n",
       "      <td>0.002991</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>7.164184</td>\n",
       "      <td>0.004169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.102423</td>\n",
       "      <td>0.079138</td>\n",
       "      <td>0.296772</td>\n",
       "      <td>0.387966</td>\n",
       "      <td>1365.956952</td>\n",
       "      <td>0.227021</td>\n",
       "      <td>0.173731</td>\n",
       "      <td>0.126746</td>\n",
       "      <td>0.095935</td>\n",
       "      <td>0.360720</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033715</td>\n",
       "      <td>0.039025</td>\n",
       "      <td>0.042037</td>\n",
       "      <td>0.054610</td>\n",
       "      <td>0.014470</td>\n",
       "      <td>0.016747</td>\n",
       "      <td>0.030002</td>\n",
       "      <td>0.022659</td>\n",
       "      <td>1.222753</td>\n",
       "      <td>0.064431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.640933</td>\n",
       "      <td>766.045816</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.642547</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.996899</td>\n",
       "      <td>1625.795561</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.394367</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.219514</td>\n",
       "      <td>3141.461527</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.052762</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.843712</td>\n",
       "      <td>8690.095814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.070054</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       number_of_collaborations  collaboration_binary           TENB  \\\n",
       "count             253075.000000         253075.000000  253075.000000   \n",
       "mean                   0.007472              0.006302       0.016669   \n",
       "std                    0.102423              0.079138       0.296772   \n",
       "min                    0.000000              0.000000       0.000000   \n",
       "25%                    0.000000              0.000000       0.000000   \n",
       "50%                    0.000000              0.000000       0.000000   \n",
       "75%                    0.000000              0.000000       0.000000   \n",
       "max                    5.000000              1.000000      16.000000   \n",
       "\n",
       "            Cog_Dist       Geo_Dist    Prov_Border      NotContig  \\\n",
       "count  253075.000000  253075.000000  253075.000000  253075.000000   \n",
       "mean        0.908712    1952.527368       0.945490       0.968847   \n",
       "std         0.387966    1365.956952       0.227021       0.173731   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.640933     766.045816       1.000000       1.000000   \n",
       "50%         0.996899    1625.795561       1.000000       1.000000   \n",
       "75%         1.219514    3141.461527       1.000000       1.000000   \n",
       "max         1.843712    8690.095814       1.000000       1.000000   \n",
       "\n",
       "       Province_1_Alberta  Province_1_Arizona  Province_1_California  ...  \\\n",
       "count       253075.000000       253075.000000          253075.000000  ...   \n",
       "mean             0.016331            0.009290               0.153761  ...   \n",
       "std              0.126746            0.095935               0.360720  ...   \n",
       "min              0.000000            0.000000               0.000000  ...   \n",
       "25%              0.000000            0.000000               0.000000  ...   \n",
       "50%              0.000000            0.000000               0.000000  ...   \n",
       "75%              0.000000            0.000000               0.000000  ...   \n",
       "max              1.000000            1.000000               1.000000  ...   \n",
       "\n",
       "       Province_1_New Hampshire  Province_1_Vermont  Province_2_Montana  \\\n",
       "count             253075.000000       253075.000000       253075.000000   \n",
       "mean                   0.001138            0.001525            0.001770   \n",
       "std                    0.033715            0.039025            0.042037   \n",
       "min                    0.000000            0.000000            0.000000   \n",
       "25%                    0.000000            0.000000            0.000000   \n",
       "50%                    0.000000            0.000000            0.000000   \n",
       "75%                    0.000000            0.000000            0.000000   \n",
       "max                    1.000000            1.000000            1.000000   \n",
       "\n",
       "       Province_2_Nebraska  Province_2_Nevada  Province_2_New Brunswick  \\\n",
       "count        253075.000000      253075.000000             253075.000000   \n",
       "mean              0.002991           0.000209                  0.000281   \n",
       "std               0.054610           0.014470                  0.016747   \n",
       "min               0.000000           0.000000                  0.000000   \n",
       "25%               0.000000           0.000000                  0.000000   \n",
       "50%               0.000000           0.000000                  0.000000   \n",
       "75%               0.000000           0.000000                  0.000000   \n",
       "max               1.000000           1.000000                  1.000000   \n",
       "\n",
       "       Province_2_New Hampshire  Province_2_Vermont   Log_Geo_Dist  \\\n",
       "count             253075.000000       253075.000000  253075.000000   \n",
       "mean                   0.000901            0.000514       7.164184   \n",
       "std                    0.030002            0.022659       1.222753   \n",
       "min                    0.000000            0.000000       0.000000   \n",
       "25%                    0.000000            0.000000       6.642547   \n",
       "50%                    0.000000            0.000000       7.394367   \n",
       "75%                    0.000000            0.000000       8.052762   \n",
       "max                    1.000000            1.000000       9.070054   \n",
       "\n",
       "         Top_regions  \n",
       "count  253075.000000  \n",
       "mean        0.004169  \n",
       "std         0.064431  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 121 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_set.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
