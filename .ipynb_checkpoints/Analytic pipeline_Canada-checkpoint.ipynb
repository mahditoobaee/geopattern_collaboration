{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad06629d",
   "metadata": {},
   "source": [
    "### Loading data and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hired-richards",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    div#notebook-container    { width: 95%; }\n",
       "    div#menubar-container     { width: 85%; }\n",
       "    div#maintoolbar-container { width: 99%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 85%; }\n",
    "    div#maintoolbar-container { width: 99%; }\n",
    "</style>\n",
    "\"\"\"))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "from scipy.stats import uniform\n",
    "import statsmodels.api as sm\n",
    "import math\n",
    "import shap \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_score, cross_val_predict, GridSearchCV, RandomizedSearchCV, KFold, StratifiedKFold, validation_curve\n",
    "from sklearn.pipeline import Pipeline as Pipeline, make_pipeline as make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.utils import resample\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler, SMOTENC\n",
    "from imblearn.under_sampling import NearMiss, RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as Imbpipeline\n",
    "from imblearn.pipeline import make_pipeline as Imb_make_pipeline\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "# the first dataset includes authors with at least 0 article to form cognitive proximity.\n",
    "# df_data_set0 = pd.read_csv(r'C:\\Users\\moham\\Dropbox\\QSE\\Thesis\\Geopattern\\My data\\df_data_set_Canada_0.csv')\n",
    "\n",
    "df_data_set = pd.read_csv(r'C:\\Users\\moham\\Dropbox\\QSE\\Thesis\\Geopattern\\My data\\df_data_set_Canada_1a.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9713fad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_descriptive = df_data_set[['collaboration_binary', 'Geo_Dist', 'TENB', 'Cog_Dist', 'Prov_Border', 'NotContig']]\n",
    "\n",
    "df_descriptive.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508d4c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson correlation\n",
    "\n",
    "df_descriptive = df_data_set[['collaboration_binary', 'Geo_Dist', 'TENB', 'Cog_Dist', 'Prov_Border', 'NotContig']]\n",
    "df_descriptive.corr()\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "rho = df_descriptive.corr()\n",
    "pval = df_descriptive.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)\n",
    "p = pval.applymap(lambda x: ''.join(['*' for t in [0.01,0.05,0.1] if x<=t]))\n",
    "rho.round(2).astype(str) + p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74192247",
   "metadata": {},
   "source": [
    "### Regression analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b9e6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [  'Log_Geo_Dist',\n",
    "             'Log_TENB',\n",
    "             'Cog_Dist',\n",
    "             'Prov_Border',\n",
    "             'NotContig',\n",
    "             'Log_Geo_Dist X Log_TENB', \n",
    "             'Log_Geo_Dist_Sq X Log_TENB']\n",
    "\n",
    "\n",
    "Ind_v = df_data_set[columns]\n",
    "Dep_v = df_data_set.collaboration_binary\n",
    "\n",
    "columns_reg1 = [ 'Log_Geo_Dist',\n",
    "                 'Log_TENB',\n",
    "                 'Cog_Dist',\n",
    "                 'Prov_Border',\n",
    "                 'NotContig']\n",
    "\n",
    "columns_reg2 = [ 'Log_Geo_Dist',\n",
    "                 'Log_TENB',\n",
    "                 'Log_Geo_Dist X Log_TENB',  \n",
    "                 'Cog_Dist',\n",
    "                 'Prov_Border',\n",
    "                 'NotContig']\n",
    "\n",
    "X_reg1 = Ind_v[columns_reg1]\n",
    "X_reg2 = Ind_v[columns_reg2]\n",
    "\n",
    "\n",
    "\n",
    "X_reg_1 = sm.tools.tools.add_constant(X_reg1, prepend=True, has_constant='add')\n",
    "X_reg_2 = sm.tools.tools.add_constant(X_reg2, prepend=True, has_constant='add')\n",
    "\n",
    "\n",
    "models = [X_reg1, X_reg2]\n",
    "\n",
    "for model in models:\n",
    "    logit_model=sm.Logit(Dep_v, model)\n",
    "    result=logit_model.fit(method_kwargs={\"warn_convergence\": False})\n",
    "    print(result.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe12c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the geo distance that maximize the TENB's Elasticity\n",
    "\n",
    "np.exp(2.0578/(2*0.2179))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c5a178",
   "metadata": {},
   "outputs": [],
   "source": [
    "2.0578 * math.log1p(112.38) - 0.2179 *  math.log1p(112.38) *  math.log1p(112.38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50179ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dis = np.linspace (0,1000, num = 5000)\n",
    "\n",
    "X_dis = []\n",
    "for i in dis:\n",
    "    x_dis = math.log1p(i)\n",
    "    X_dis.append(x_dis)\n",
    "    \n",
    "Y_tenb = []\n",
    "Y_tenb_ = []\n",
    "\n",
    "for j in X_dis:\n",
    "    y_tenb_ = 2.0578 * j - 0.2179 * j * j\n",
    "    Y_tenb_.append(y_tenb_)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot (dis, Y_tenb_, color = 'red')\n",
    "plt.xlabel('Physical distance (km)', size = 20)\n",
    "plt.ylabel('Elasticity of TENB (network proximity)', size = 20)\n",
    "plt.xticks(fontsize= 15)\n",
    "plt.yticks(fontsize= 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db746dd9",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18bdaa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Log_Geo_Dist', 'Log_TENB', 'Cog_Dist' ,'Prov_Border', 'NotContig']\n",
    "\n",
    "X = df_data_set[features]\n",
    "y = df_data_set.collaboration_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd9ceea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset {0: 2621, 1: 81}\n",
      "\n",
      "\n",
      "Dataset Train {0: 2096, 1: 65}\n",
      "\n",
      "\n",
      "Dataset Test {0: 525, 1: 16}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique, count = np.unique (y, return_counts = True)\n",
    "\n",
    "y_value_count = {k : v for (k,v) in zip(unique,count)}\n",
    "\n",
    "print ('Dataset', y_value_count)\n",
    "print(\"\\n\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1, stratify = y)\n",
    "\n",
    "unique_train, count_train = np.unique (y_train, return_counts = True)\n",
    "\n",
    "y_train_value_count = {k : v for (k,v) in zip(unique_train,count_train)}\n",
    "\n",
    "print ('Dataset Train', y_train_value_count)\n",
    "print(\"\\n\")\n",
    "\n",
    "unique_test, count_test = np.unique (y_test, return_counts = True)\n",
    "\n",
    "y_test_value_count = {k : v for (k,v) in zip(unique_test,count_test)}\n",
    "\n",
    "print ('Dataset Test', y_test_value_count)\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "blond-tyler",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(max_iter = 500)\n",
    "gnb = GaussianNB()\n",
    "knn = KNeighborsClassifier(n_jobs = -1)\n",
    "svm = LinearSVC(loss = 'hinge')\n",
    "rnd = RandomForestClassifier(random_state = 123, n_jobs = -1)\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric = 'logloss', random_state = 123, n_jobs = -1)\n",
    "\n",
    "rus = RandomUnderSampler()\n",
    "ros = RandomOverSampler()\n",
    "smt = SMOTENC(categorical_features = [3,4], random_state = 123, n_jobs = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64e0e3e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.74 (+/- 0.07) [logreg]\n",
      "F1: 0.77 (+/- 0.06) [logreg_pipe_smt]\n",
      "F1: 0.70 (+/- 0.04) [logreg_pipe_ros]\n",
      "F1: 0.63 (+/- 0.03) [logreg_pipe_rus]\n",
      "F1: 0.78 (+/- 0.07) [gnb]\n",
      "F1: 0.62 (+/- 0.12) [gnb_pipe_smt]\n",
      "F1: 0.69 (+/- 0.05) [gnb_pipe_ros]\n",
      "F1: 0.73 (+/- 0.12) [gnb_pipe_rus]\n",
      "F1: 0.77 (+/- 0.07) [knn]\n",
      "F1: 0.83 (+/- 0.03) [knn_pipe_smt]\n",
      "F1: 0.85 (+/- 0.04) [knn_pipe_ros]\n",
      "F1: 0.52 (+/- 0.10) [knn_pipe_rus]\n",
      "F1: 0.83 (+/- 0.03) [svm]\n",
      "F1: 0.78 (+/- 0.07) [svm_pipe_smt]\n",
      "F1: 0.72 (+/- 0.05) [svm_pipe_ros]\n",
      "F1: 0.68 (+/- 0.06) [svm_pipe_rus]\n",
      "F1: 0.90 (+/- 0.03) [rnd]\n",
      "F1: 0.86 (+/- 0.05) [rnd_pipe_smt]\n",
      "F1: 0.90 (+/- 0.02) [rnd_pipe_ros]\n",
      "F1: 0.59 (+/- 0.10) [rnd_pipe_rus]\n",
      "F1: 0.90 (+/- 0.05) [xgb]\n",
      "F1: 0.89 (+/- 0.07) [xgb_pipe_smt]\n",
      "F1: 0.90 (+/- 0.04) [xgb_pipe_ros]\n",
      "F1: 0.62 (+/- 0.11) [xgb_pipe_rus]\n",
      "Wall time: 5.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Cross Validation\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "\n",
    "logreg_pipe_smt = Imbpipeline(steps = [['SMOTENC', smt], ['LogisticRegression', logreg]])\n",
    "logreg_pipe_ros = Imbpipeline(steps = [['RandomOverSampler', ros], ['LogisticRegression', logreg]])\n",
    "logreg_pipe_rus = Imbpipeline(steps = [['RandomUnderSampler', rus], ['LogisticRegression', logreg]])\n",
    "\n",
    "gnb_pipe_smt = Imbpipeline(steps = [['SMOTENC', smt], ['GaussianNB', gnb]])\n",
    "gnb_pipe_ros = Imbpipeline(steps = [['RandomOverSampler', ros], ['GaussianNB', gnb]])\n",
    "gnb_pipe_rus = Imbpipeline(steps = [['RandomUnderSampler', rus], ['GaussianNB', gnb]])\n",
    "\n",
    "knn_pipe_smt = Imbpipeline(steps = [['SMOTENC', smt], ['KNeighborsClassifier', knn]])\n",
    "knn_pipe_ros = Imbpipeline(steps = [['RandomOverSampler', ros], ['KNeighborsClassifier', knn]])\n",
    "knn_pipe_rus = Imbpipeline(steps = [['RandomUnderSampler', rus], ['KNeighborsClassifier', knn]])\n",
    "\n",
    "svm_pipe_smt = Imbpipeline(steps = [['SMOTENC', smt], ['svm', svm]])\n",
    "svm_pipe_ros = Imbpipeline(steps = [['RandomOverSampler', ros], ['svm', svm]])\n",
    "svm_pipe_rus = Imbpipeline(steps = [['RandomUnderSampler', rus], ['svm', svm]])\n",
    "\n",
    "rnd_pipe_smt = Imbpipeline(steps = [['SMOTENC', smt], ['Random Forest', rnd]])\n",
    "rnd_pipe_ros = Imbpipeline(steps = [['RandomOverSampler', ros], ['Random Forest', rnd]])\n",
    "rnd_pipe_rus = Imbpipeline(steps = [['RandomUnderSampler', rus], ['Random Forest', rnd]])\n",
    "\n",
    "xgb_pipe_smt = Imbpipeline(steps = [['SMOTENC', smt], ['RandomForestClassifier', xgb]])\n",
    "xgb_pipe_ros = Imbpipeline(steps = [['RandomOverSampler', ros], ['RandomForestClassifier', xgb]])\n",
    "xgb_pipe_rus = Imbpipeline(steps = [['RandomUnderSampler', rus], ['RandomForestClassifier', xgb]])\n",
    "\n",
    "clfs = [logreg, logreg_pipe_smt, logreg_pipe_ros, logreg_pipe_rus,\n",
    "        gnb, gnb_pipe_smt, gnb_pipe_ros, gnb_pipe_rus,\n",
    "        knn, knn_pipe_smt, knn_pipe_ros, knn_pipe_rus, \n",
    "        svm, svm_pipe_smt, svm_pipe_ros, svm_pipe_rus,\n",
    "        rnd, rnd_pipe_smt, rnd_pipe_ros, rnd_pipe_rus,\n",
    "        xgb, xgb_pipe_smt, xgb_pipe_ros, xgb_pipe_rus]\n",
    "\n",
    "labels = ['logreg', 'logreg_pipe_smt', 'logreg_pipe_ros', 'logreg_pipe_rus',\n",
    "          'gnb', 'gnb_pipe_smt', 'gnb_pipe_ros', 'gnb_pipe_rus',\n",
    "          'knn', 'knn_pipe_smt', 'knn_pipe_ros', 'knn_pipe_rus', \n",
    "          'svm', 'svm_pipe_smt', 'svm_pipe_ros', 'svm_pipe_rus',\n",
    "          'rnd', 'rnd_pipe_smt', 'rnd_pipe_ros', 'rnd_pipe_rus', \n",
    "          'xgb', 'xgb_pipe_smt', 'xgb_pipe_ros', 'xgb_pipe_rus']\n",
    "\n",
    "       \n",
    "clfs = zip(clfs,labels)\n",
    "\n",
    "for clf, label in clfs:\n",
    "    scores_F1 = cross_val_score(clf, X_train, y_train, scoring='f1', cv=skf, n_jobs=-1)\n",
    "    print(\"F1: %0.2f (+/- %0.2f) [%s]\" % (scores_F1.mean(), scores_F1.std(), label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1d083cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.22019021, 0.66087422, 0.44196063, 0.58571977, 0.98789463,\n",
       "       0.64244277, 0.32588686, 0.33453418, 0.31249677, 0.53515561,\n",
       "       0.37668802, 0.31655895, 0.95782323, 0.52309584, 0.96678097,\n",
       "       0.6118112 , 0.98548649, 0.72795375, 0.09066843, 0.95305149])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.uniform(size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f57490d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter subsample for estimator RandomForestClassifier(n_jobs=-1, random_state=123). Check the list of available parameters with `estimator.get_params().keys()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\moham\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 431, in _process_worker\n    r = call_item()\n  File \"C:\\Users\\moham\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 285, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"C:\\Users\\moham\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\moham\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\moham\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\moham\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 222, in __call__\n    return self.function(*args, **kwargs)\n  File \"C:\\Users\\moham\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 581, in _fit_and_score\n    estimator = estimator.set_params(**cloned_parameters)\n  File \"C:\\Users\\moham\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 230, in set_params\n    raise ValueError('Invalid parameter %s for estimator %s. '\nValueError: Invalid parameter subsample for estimator RandomForestClassifier(n_jobs=-1, random_state=123). Check the list of available parameters with `estimator.get_params().keys()`.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-f17abc704f7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m                            random_state = 123)\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mxgb_srch_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Best Parameters'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxgb_srch_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Best Score'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxgb_srch_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    839\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    842\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1618\u001b[0m         \u001b[1;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1619\u001b[1;33m         evaluate_candidates(ParameterSampler(\n\u001b[0m\u001b[0;32m   1620\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1621\u001b[0m             random_state=self.random_state))\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    793\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[0;32m    796\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1054\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1055\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    931\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    437\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    440\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid parameter subsample for estimator RandomForestClassifier(n_jobs=-1, random_state=123). Check the list of available parameters with `estimator.get_params().keys()`."
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning - RandomSearch\n",
    "\n",
    "param_dist = { 'max_depth': [3, 5, 6, 10, 15, 20],\n",
    "           'learning_rate': np.random.uniform(size=10),\n",
    "           'subsample': np.random.uniform(size=10),\n",
    "           'colsample_bytree': np.random.uniform(size=10),\n",
    "           'colsample_bylevel': np.random.uniform(size=10),\n",
    "           'n_estimators': [100, 500, 1000]}\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 5)\n",
    "\n",
    "xgb_srch_model = RandomizedSearchCV(estimator = xgb,\n",
    "                           param_distributions = param_dist, \n",
    "                           cv = skf, n_jobs = -1, \n",
    "                           verbose = 3,\n",
    "                           n_iter = 10,\n",
    "                           scoring = 'f1',\n",
    "                           random_state = 123)\n",
    "\n",
    "xgb_srch_model.fit(X_train, y_train)\n",
    "print ('Best Parameters', xgb_srch_model.best_params_)\n",
    "print ('Best Score', xgb_srch_model.best_score_)\n",
    "\n",
    "# y_pred = rnd_srch_model.best_estimator_.predict(X_test)\n",
    "# print ('Presicion:' , precision_score(y_test, y_pred))\n",
    "# print ('Recall:' , recall_score(y_test, y_pred))\n",
    "# print ('F1_Score:', f1_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dbeed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test scores\n",
    "\n",
    "# for clf, label in zip([logreg, gnb, knn, svm, rnd, xgb], ['LogisticRegression', 'GaussianNB', 'KNeighborsClassifier', 'svm', 'Random Forest', 'XGB Classifier', 'CatBoostClassifier']):\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     y_pred = clf.predict(X_test)\n",
    "# #     print(\"Presicion: %0.3f [%s]\" % (precision_score(y_test, y_pred), label))\n",
    "# #     print(\"Recall: %0.3f [%s]\" % (recall_score(y_test, y_pred), label))\n",
    "#     print(\"F1: %0.2f [%s]\" % (f1_score(y_test, y_pred), label))\n",
    "# #     print(\"Accuracy: %0.2f [%s]\" % (accuracy_score(y_test, y_pred), label))\n",
    "# #     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24963150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance - mean decrease in impurity\n",
    "\n",
    "importances = xgb.feature_importances_ * 100\n",
    "\n",
    "\n",
    "df_importance = pd.DataFrame()\n",
    "\n",
    "df_importance.insert(0,'Feature','')\n",
    "df_importance.insert(1,'importance',0)\n",
    "\n",
    "j = 0\n",
    "for i in features:\n",
    "    df_importance.loc[j,'Feature'] = i\n",
    "    df_importance.loc[j,'importance'] = importances[j]\n",
    "    j += 1\n",
    "\n",
    "df_importance.sort_values(by=['importance'], ascending=False, inplace = True)\n",
    "\n",
    "df_importance[:10].plot(x = 'Feature', y = 'importance', kind = 'bar', rot = 90, ylabel = 'mean decrease in impurity (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e95eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Feature importance - SHAP\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "xgb_explainer = shap.TreeExplainer(xgb)\n",
    "shap_values = xgb_explainer.shap_values(X_train, y_train)\n",
    "shap_values2 = xgb_explainer(X_train)\n",
    "shap_interaction_values = xgb_explainer.shap_interaction_values(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e2ba7a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values2)\n",
    "shap.summary_plot(shap_values2, X_train, plot_type=\"bar\")\n",
    "shap.summary_plot(shap_values2, X_train, plot_size=2, cmap=plt.get_cmap(\"cool\"))\n",
    "# shap.plots.waterfall(shap_values2[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a092ac80",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Dependence plot\n",
    "\n",
    "shap.dependence_plot('Log_TENB', shap_values, X_train, interaction_index=None, show = False)\n",
    "\n",
    "plt.gcf().set_size_inches(5, 5)\n",
    "\n",
    "shap.dependence_plot(('Log_TENB', 'Log_Geo_Dist'), shap_interaction_values, X_train, show = False, cmap=plt.get_cmap(\"hsv\"), dot_size = 70)\n",
    "\n",
    "plt.gcf().set_size_inches(5, 5)\n",
    "# ax = plt.axes()\n",
    "# ax.set_facecolor(\"black\")\n",
    "\n",
    "# shap.dependence_plot('Cog_Dist', shap_values, X_train, interaction_index=None)\n",
    "# shap.dependence_plot('Country_Border', shap_values, X_train, interaction_index=None)\n",
    "# shap.dependence_plot('Prov_Border', shap_values, X_train, interaction_index=None)\n",
    "# shap.dependence_plot('NotContig', shap_values, X_train, interaction_index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36af29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Search Best parameters result (test data)\n",
    "\n",
    "rnd_best = RandomForestClassifier(random_state = 123, n_jobs = -1,\n",
    "                                       n_estimators = 170,\n",
    "                                       min_samples_split = 5,\n",
    "                                       min_samples_leaf = 1,\n",
    "                                       max_features = 'sqrt',\n",
    "                                       criterion = 'gini',\n",
    "                                       max_depth = 55,\n",
    "                                       bootstrap = True)\n",
    "\n",
    "rnd_best.fit(X_train, y_train)\n",
    "y_pred_best = rnd_best.predict(X_test)\n",
    "print ('F1_Score:', f1_score(y_test, y_pred_best))\n",
    "print ('Presicion:' , precision_score(y_test, y_pred_best))\n",
    "print ('Recall:' , recall_score(y_test, y_pred_best))\n",
    "print ('Accuracy:', accuracy_score(y_test, y_pred_best))\n",
    "print ('confusion_matrix:')\n",
    "print (confusion_matrix(y_test, y_pred_best))\n",
    "print ('Classification Report:')\n",
    "print (classification_report(y_test, y_pred_best))\n",
    "\n",
    "# rfc_cv_score = cross_val_score(rnd_best, X_train, y_train, cv = skf, scoring = 'f1')\n",
    "\n",
    "# print ('F1_Score_cv_score:', rfc_cv_score.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bdce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning - Randomforestclassifier - Grid search\n",
    "\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start = 160, stop = 180, num = 3)]\n",
    "\n",
    "max_features = ['auto','sqrt']\n",
    "\n",
    "max_depth = [int(x) for x in np.linspace(53, 57, num = 5)]\n",
    "max_depth.append(None)\n",
    "\n",
    "min_samples_split = [4,5,6]\n",
    "\n",
    "min_samples_leaf = [1]\n",
    "\n",
    "bootstrap = [True, False]\n",
    "\n",
    "criterion = ['gini', 'entropy']\n",
    "\n",
    "\n",
    "param_grid = { 'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap,\n",
    "               'criterion' : criterion}\n",
    "\n",
    "print('Parameters used in the grid:\\n')\n",
    "pprint(param_grid)\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 5)\n",
    "\n",
    "grid_model = GridSearchCV(estimator = rnd,\n",
    "                         param_grid = param_grid, \n",
    "                         cv = skf, n_jobs = -1, \n",
    "                         verbose = 1,\n",
    "                         scoring = 'f1')\n",
    "\n",
    "grid_model.fit(X_train, y_train)\n",
    "\n",
    "print ('Best Parameters', grid_model.best_params_)\n",
    "print ('Best Score', grid_model.best_score_)\n",
    "\n",
    "\n",
    "# Best Parameters {'bootstrap': True, 'criterion': 'gini', 'max_depth': 53, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 170}\n",
    "# Best Score 0.8852592313348533"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4177b5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid Search Best parameters result (test data)\n",
    "\n",
    "grd_best = RandomForestClassifier(random_state = 123, n_jobs = -1,\n",
    "                                       criterion = 'gini',\n",
    "                                       n_estimators = 170,\n",
    "                                       min_samples_split = 5,\n",
    "                                       min_samples_leaf = 1,\n",
    "                                       max_features = 'auto',\n",
    "                                       max_depth = 53,\n",
    "                                       bootstrap = True)\n",
    "\n",
    "grd_best.fit(X_train, y_train)\n",
    "y_pred_best_grd = grd_best.predict(X_test)\n",
    "print ('F1_Score:', f1_score(y_test, y_pred_best_grd))\n",
    "print ('Presicion:' , precision_score(y_test, y_pred_best_grd))\n",
    "print ('Recall:' , recall_score(y_test, y_pred_best_grd))\n",
    "print ('Accuracy:', accuracy_score(y_test, y_pred_best_grd))\n",
    "print ('roc_auc_score:', roc_auc_score(y_test, y_pred_best_grd))\n",
    "print ('confusion_matrix:')\n",
    "print (confusion_matrix(y_test, y_pred_best_grd))\n",
    "print ('Classification Report:')\n",
    "print (classification_report(y_test, y_pred_best_grd))\n",
    "\n",
    "# rfc_cv_score = cross_val_score(rnd_best, X_train, y_train, cv = skf, scoring = 'f1')\n",
    "\n",
    "# print ('F1_Score_cv_score:', rfc_cv_score.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97ae93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat = CatBoostClassifier(random_state = 123, verbose = False)\n",
    "# params = {'iterations':1000,\n",
    "#         'learning_rate':0.01,\n",
    "#         'depth':3,\n",
    "#         'eval_metric':'F1',\n",
    "#         'loss_function': \"Logloss\",\n",
    "#         'verbose':False,\n",
    "#         'od_type':\"Iter\", # overfit detector\n",
    "#         'od_wait':500, # most recent best iteration to wait before stopping\n",
    "#         'random_state' : 123}\n",
    "\n",
    "# cat1 = CatBoostClassifier(**params)\n",
    "# logreg = LogisticRegression(max_iter = 500)\n",
    "# rnd = RandomForestClassifier(random_state = 123, n_jobs = -1)\n",
    "# gbc = GradientBoostingClassifier(random_state = 123)\n",
    "# xgb = XGBClassifier(use_label_encoder=False, eval_metric = 'logloss', random_state = 123, n_jobs = -1)\n",
    "# cat = CatBoostClassifier()\n",
    "# knn = KNeighborsClassifier(n_jobs = -1)\n",
    "# gnb = GaussianNB()\n",
    "# smtk = SMOTETomek(n_jobs = 6)\n",
    "# smnn = SMOTEENN(n_jobs = 6)\n",
    "# scl = StandardScaler()\n",
    "# feature_selection_selbest = SelectKBest(chi2, k=7)\n",
    "# feature_selection_selmodel = SelectFromModel(rnd)\n",
    "# voth = VotingClassifier(estimators=[('LogisticRegression', logreg), ('RandomForestClassifier', rnd), ('GradientBoostingClassifier', gbc), ('XGBClassifier', xgb), ('KNeighborsClassifier', knn),('GaussianNB', gnb)], voting='hard')\n",
    "# vots = VotingClassifier(estimators=[('LogisticRegression', logreg), ('RandomForestClassifier', rnd), ('GradientBoostingClassifier', gbc), ('XGBClassifier', xgb), ('KNeighborsClassifier', knn),('GaussianNB', gnb)], voting='soft')\n",
    "# voth = VotingClassifier(estimators=[('RandomForestClassifier', rnd), ('XGBClassifier', xgb)], voting='hard')\n",
    "# vots = VotingClassifier(estimators=[('RandomForestClassifier', rnd), ('XGBClassifier', xgb)], voting='soft')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
