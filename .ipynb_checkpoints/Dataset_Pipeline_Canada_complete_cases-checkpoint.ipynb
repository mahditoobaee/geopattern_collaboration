{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dangerous-lebanon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    div#notebook-container    { width: 95%; }\n",
       "    div#menubar-container     { width: 85%; }\n",
       "    div#maintoolbar-container { width: 99%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enlarging the screen is done!\n",
      "Libraries were imported successfully!\n",
      "Loading data from sql is done!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c681b58d224c9fa416bf69389a6b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing the data set:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enlarging the screen\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 85%; }\n",
    "    div#maintoolbar-container { width: 99%; }\n",
    "</style>\n",
    "\"\"\"))\n",
    "\n",
    "print ('Enlarging the screen is done!')\n",
    "\n",
    "# Importing libraries\n",
    "\n",
    "import numpy\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "print ('Libraries were imported successfully!')\n",
    "\n",
    "# Loading data from sql\n",
    "\n",
    "Server = 'LAPTOP-I7NEB9V3\\SQLEXPRESS'\n",
    "Database = 'Geopattern'\n",
    "Driver = 'ODBC Driver 17 for SQL Server'\n",
    "Database_Connection = f'mssql://@{Server}/{Database}?driver={Driver}'\n",
    "\n",
    "engine = create_engine(Database_Connection)\n",
    "connection = engine.connect()\n",
    "\n",
    "df_my_data = pd.read_sql_query (\n",
    "    \"select * from my_data\", connection)\n",
    "\n",
    "df_Authors = pd.read_sql_query (\n",
    "    \"select * from my_data_Authors\", connection)\n",
    "\n",
    "df_only_Canada = pd.read_sql_query (\n",
    "    \"select distinct my_data.EID, my_data.Author_ID, my_data.Year, my_data.Country from my_data join \\\n",
    "        (select distinct EID from my_data \\\n",
    "        except \\\n",
    "        (select distinct EID from my_data \\\n",
    "        where Country not like 'Canada' or Country is Null)) a \\\n",
    "        on my_data.EID = a.EID\", connection)\n",
    "\n",
    "df_only_Canada.Year = df_only_Canada.Year.astype(int)\n",
    "\n",
    "df_LDA = pd.read_csv(r'C:\\Users\\moham\\Dropbox\\QSE\\Thesis\\Geopattern\\My data\\df_LDA.csv')\n",
    "df_LDA.set_index('EID', inplace = True)\n",
    "\n",
    "print ('Loading data from sql is done!')\n",
    "\n",
    "width_ind = 3\n",
    "width_dep = 2\n",
    "\n",
    "df_data_set = pd.DataFrame()\n",
    "\n",
    "list_years = []\n",
    "\n",
    "for yrs in range(21 - width_ind - width_dep):\n",
    "    list_years.append(2000 + yrs)\n",
    "\n",
    "\n",
    "for yr in tqdm(list_years, desc = 'Preparing the data set'):\n",
    "\n",
    "\n",
    "    Ind_win_start = yr\n",
    "    Ind_win_end = Ind_win_start + width_ind - 1\n",
    "\n",
    "    dep_win_start = Ind_win_end + 1\n",
    "    dep_win_end = dep_win_start + width_dep - 1\n",
    "\n",
    "    df_ind = df_only_Canada[(df_only_Canada.Year > (Ind_win_start - 1)) & (df_only_Canada.Year <= Ind_win_end)]\n",
    "#     df_ind = df_only_Canada[(df_only_Canada.Year <= Ind_win_end)]\n",
    "    df_dep = df_only_Canada[(df_only_Canada.Year > (dep_win_start - 1)) & (df_only_Canada.Year <= dep_win_end)]\n",
    "\n",
    "    #     df_ind.rename({'EID' : 'EIDs_ind'}, axis = 1, inplace = True)\n",
    "    #     df_dep.rename({'EID' : 'EIDs_dep'}, axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "    # Targets.................................................................................................. \n",
    "\n",
    "    list_authors_dep = df_dep.Author_ID.unique().tolist()\n",
    "    list_authors_ind = df_ind.Author_ID.unique().tolist()\n",
    "    \n",
    "    list_authors = list(set(list_authors_dep).intersection(list_authors_ind))\n",
    "    list_authors.sort()\n",
    "\n",
    "    df_authors_dep = pd.DataFrame(data = list_authors, columns = ['Author_ID'])\n",
    "    df_authors_dep.insert(1,'EIDs','')\n",
    "\n",
    "    # df_authors_dep = df_authors_dep.merge(df_Authors, on = 'Author_ID', how = 'left')\n",
    "\n",
    "    authors_dep = []\n",
    "    for i in range (df_authors_dep.shape[0]):\n",
    "        for j in range (df_authors_dep.shape[0]):\n",
    "            if j > i:\n",
    "                authors_dep.append((df_authors_dep['Author_ID'][i] + df_authors_dep['Author_ID'][j], df_authors_dep['Author_ID'][i], df_authors_dep['Author_ID'][j]))\n",
    "\n",
    "    df_data_set_dep = pd.DataFrame(data = authors_dep, columns=['Author_1_2', 'Author_1', 'Author_2'])\n",
    "\n",
    "    df_data_set_dep = df_data_set_dep.set_index(['Author_1_2'])\n",
    "\n",
    "    df_data_set_dep.insert(2,'number_of_collaborations',0)\n",
    "    df_data_set_dep.insert(3,'collaboration_binary',0)\n",
    "\n",
    "    df_dep.reset_index(inplace = True)\n",
    "\n",
    "    df_authors_dep['EIDs'] = ''\n",
    "    for i in range (df_authors_dep.shape[0]):\n",
    "        for j in range (df_dep.shape[0]):\n",
    "            if df_authors_dep['Author_ID'][i] == df_dep['Author_ID'][j]:\n",
    "                df_authors_dep['EIDs'][i] = str (df_authors_dep['EIDs'][i]) + ';' + str (df_dep['EID'][j])\n",
    "\n",
    "    res = []\n",
    "\n",
    "    for i in range (df_authors_dep.shape[0]):\n",
    "        l = df_authors_dep['EIDs'][i].split(';')\n",
    "        l.remove('')\n",
    "        res.append((df_authors_dep['Author_ID'][i],set(l)))\n",
    "\n",
    "\n",
    "    collab_matrix = np.zeros((df_authors_dep.shape[0],df_authors_dep.shape[0]))\n",
    "\n",
    "    for i in range (len(res)):\n",
    "        for j in range (len(res)):\n",
    "            collab_matrix[i,j] = len(res[i][1].intersection(res[j][1]))\n",
    "\n",
    "    collab_list = []\n",
    "    for i in range (collab_matrix.shape[0]):\n",
    "        for j in range (collab_matrix.shape[0]):\n",
    "            if j > i:\n",
    "                if collab_matrix[i,j] != 0:\n",
    "                    collab_list.append((df_authors_dep['Author_ID'][i] + df_authors_dep['Author_ID'][j], df_authors_dep['Author_ID'][i], df_authors_dep['Author_ID'][j], collab_matrix[i,j]))\n",
    "\n",
    "    df_collab = pd.DataFrame(data = collab_list, columns=['Author_1_2', 'Author_1', 'Author_2', 'number_of_collaborations'])\n",
    "\n",
    "    df_collab = df_collab.set_index(['Author_1_2'])\n",
    "\n",
    "\n",
    "    for i in df_data_set_dep.index:\n",
    "        try:\n",
    "            df_data_set_dep.loc[i,'number_of_collaborations'] = df_collab.loc[i,'number_of_collaborations']\n",
    "        except:\n",
    "            df_data_set_dep.loc[i,'number_of_collaborations'] = 0\n",
    "\n",
    "\n",
    "    df_data_set_dep.collaboration_binary = df_data_set_dep.number_of_collaborations.map(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "\n",
    "    # Features............................................................................................\n",
    "\n",
    "    df_authors_ind = pd.DataFrame(data = list_authors, columns = ['Author_ID'])\n",
    "    df_authors_ind.insert(1,'EIDs','')\n",
    "    df_authors_ind.insert(2,'partners',0)\n",
    "    df_authors_ind.insert(3,'topic_1',0)\n",
    "    df_authors_ind.insert(4,'topic_2',0)\n",
    "    df_authors_ind.insert(5,'topic_3',0)\n",
    "    df_authors_ind.insert(6,'topic_4',0)\n",
    "    df_authors_ind.insert(7,'topic_5',0)\n",
    "    df_authors_ind.insert(8,'topic_6',0)\n",
    "    df_authors_ind.insert(9,'topic_7',0)\n",
    "    df_authors_ind.insert(10,'topic_8',0)\n",
    "    df_authors_ind.insert(11,'topic_9',0)\n",
    "\n",
    "\n",
    "    df_authors_ind = df_authors_ind.merge(df_Authors, on = 'Author_ID', how = 'left')\n",
    "\n",
    "    df_authors_ind.reset_index(inplace = True)\n",
    "    df_ind.reset_index(inplace = True)\n",
    "\n",
    "    df_authors_ind['EIDs'] = ''\n",
    "\n",
    "    for i in range (df_authors_ind.shape[0]):\n",
    "        for j in range (df_ind.shape[0]):\n",
    "            if df_authors_ind['Author_ID'][i] == df_ind['Author_ID'][j]:\n",
    "                df_authors_ind['EIDs'][i] = str (df_authors_ind['EIDs'][i]) + ';' + str (df_ind['EID'][j])\n",
    "                df_authors_ind.loc[i, 'topic_1'] += df_LDA.loc[str (df_ind['EID'][j]), 'topic_1']\n",
    "                df_authors_ind.loc[i, 'topic_2'] += df_LDA.loc[str (df_ind['EID'][j]), 'topic_2']\n",
    "                df_authors_ind.loc[i, 'topic_3'] += df_LDA.loc[str (df_ind['EID'][j]), 'topic_3']\n",
    "                df_authors_ind.loc[i, 'topic_4'] += df_LDA.loc[str (df_ind['EID'][j]), 'topic_4']\n",
    "                df_authors_ind.loc[i, 'topic_5'] += df_LDA.loc[str (df_ind['EID'][j]), 'topic_5']\n",
    "                df_authors_ind.loc[i, 'topic_6'] += df_LDA.loc[str (df_ind['EID'][j]), 'topic_6']\n",
    "                df_authors_ind.loc[i, 'topic_7'] += df_LDA.loc[str (df_ind['EID'][j]), 'topic_7']\n",
    "                df_authors_ind.loc[i, 'topic_8'] += df_LDA.loc[str (df_ind['EID'][j]), 'topic_8']\n",
    "                df_authors_ind.loc[i, 'topic_9'] += df_LDA.loc[str (df_ind['EID'][j]), 'topic_9']\n",
    "\n",
    "    for i in range (df_authors_ind.shape[0]):\n",
    "        summ = df_authors_ind.loc[i,'topic_1'] + df_authors_ind.loc[i,'topic_2'] + df_authors_ind.loc[i,'topic_3'] + df_authors_ind.loc[i,'topic_4'] + df_authors_ind.loc[i,'topic_5'] + df_authors_ind.loc[i,'topic_6'] + df_authors_ind.loc[i,'topic_7'] + df_authors_ind.loc[i,'topic_8'] + df_authors_ind.loc[i,'topic_9']\n",
    "        if summ > 0:\n",
    "            df_authors_ind.loc[i,'topic_1'] = df_authors_ind.loc[i,'topic_1']/summ\n",
    "            df_authors_ind.loc[i,'topic_2'] = df_authors_ind.loc[i,'topic_2']/summ\n",
    "            df_authors_ind.loc[i,'topic_3'] = df_authors_ind.loc[i,'topic_3']/summ\n",
    "            df_authors_ind.loc[i,'topic_4'] = df_authors_ind.loc[i,'topic_4']/summ\n",
    "            df_authors_ind.loc[i,'topic_5'] = df_authors_ind.loc[i,'topic_5']/summ\n",
    "            df_authors_ind.loc[i,'topic_6'] = df_authors_ind.loc[i,'topic_6']/summ\n",
    "            df_authors_ind.loc[i,'topic_7'] = df_authors_ind.loc[i,'topic_7']/summ\n",
    "            df_authors_ind.loc[i,'topic_8'] = df_authors_ind.loc[i,'topic_8']/summ\n",
    "            df_authors_ind.loc[i,'topic_9'] = df_authors_ind.loc[i,'topic_9']/summ\n",
    "\n",
    "    res = []\n",
    "\n",
    "    for i in range (df_authors_ind.shape[0]):\n",
    "        l = df_authors_ind['EIDs'][i].split(';')\n",
    "        l.remove('')\n",
    "        res.append((df_authors_ind['Author_ID'][i],set(l)))\n",
    "\n",
    "\n",
    "    collab_matrix = np.zeros((df_authors_ind.shape[0],df_authors_ind.shape[0]))\n",
    "\n",
    "    for i in range (len(res)):\n",
    "        for j in range (len(res)):\n",
    "            collab_matrix[i,j] = len(res[i][1].intersection(res[j][1]))\n",
    "\n",
    "    collab_list = []\n",
    "    for i in range (collab_matrix.shape[0]):\n",
    "        for j in range (collab_matrix.shape[0]):\n",
    "            if j > i:\n",
    "                if collab_matrix[i,j] != 0:\n",
    "                    collab_list.append((df_authors_ind['Author_ID'][i] + df_authors_ind['Author_ID'][j], df_authors_ind['Author_ID'][i], df_authors_ind['Author_ID'][j], collab_matrix[i,j]))\n",
    "\n",
    "    df_collab = pd.DataFrame(data = collab_list, columns=['Author_1_2', 'Author_1', 'Author_2', 'number_of_collaborations'])\n",
    "\n",
    "\n",
    "    df_collab = df_collab.set_index(['Author_1_2'])\n",
    "\n",
    "    df_authors_ind ['partners'] = ''\n",
    "    for i in range (collab_matrix.shape[0]):\n",
    "        for j in range (collab_matrix.shape[0]):\n",
    "            if i != j:\n",
    "                if collab_matrix[i,j] != 0:\n",
    "                    df_authors_ind ['partners'][i] = str (df_authors_ind ['partners'][i]) + ';' + str (df_authors_ind ['Author_ID'][j])\n",
    "\n",
    "\n",
    "    df_data_set_ind = pd.DataFrame(data = authors_dep, columns=['Author_1_2', 'Author_1', 'Author_2'])\n",
    "\n",
    "    df_data_set_ind = df_data_set_ind.set_index(['Author_1_2'])\n",
    "\n",
    "    df_data_set_ind.insert(2,'TENB',0)\n",
    "    df_data_set_ind.insert(3,'Cog_Dist', '')\n",
    "    df_data_set_ind.insert(4,'Geo_Dist',0)\n",
    "    df_data_set_ind.insert(5,'Prov_Border',0)\n",
    "    df_data_set_ind.insert(6,'NotContig',0)\n",
    "\n",
    "    # TENB\n",
    "\n",
    "    res_p = []\n",
    "\n",
    "    for i in range (df_authors_ind.shape[0]):\n",
    "        l = df_authors_ind['partners'][i].split(';')\n",
    "        l.remove('')\n",
    "        res_p.append((df_authors_ind['Author_ID'][i],set(l)))\n",
    "\n",
    "    common_partners_matrix = np.zeros((df_authors_ind.shape[0],df_authors_ind.shape[0]))\n",
    "\n",
    "    for i in range (len(res_p)):\n",
    "        for j in range (len(res_p)):\n",
    "            common_partners_matrix[i,j] = len(res_p[i][1].intersection(res_p[j][1]))\n",
    "\n",
    "\n",
    "    df_common_partners = pd.DataFrame([])\n",
    "    df_common_partners.insert(0,'Author_1_2','')\n",
    "    df_common_partners.insert(1,'Author_1','')\n",
    "    df_common_partners.insert(2,'Author_2','')\n",
    "    df_common_partners.insert(3,'Common_partners',{})\n",
    "    df_common_partners.insert(4,'TENB',float)\n",
    "\n",
    "\n",
    "    for i in range (common_partners_matrix.shape[0]):\n",
    "        for j in range (common_partners_matrix.shape[0]):\n",
    "            if j > i:\n",
    "                if common_partners_matrix[i,j] != 0:\n",
    "                    df_common_partners = df_common_partners.append({'Author_1_2': df_authors_ind['Author_ID'][i] + df_authors_ind['Author_ID'][j],'Author_1': df_authors_ind['Author_ID'][i], 'Author_2': df_authors_ind['Author_ID'][j], 'Common_partners': res_p[i][1].intersection(res_p[j][1])}, ignore_index = True)\n",
    "\n",
    "\n",
    "    number_of_articles = []\n",
    "    for i in range (collab_matrix.shape[0]):\n",
    "        number_of_articles.append((df_authors_ind['Author_ID'][i], collab_matrix[i,i]))\n",
    "\n",
    "    df_number_of_articles = pd.DataFrame(data = number_of_articles, columns=['Author', 'number_of_articles'])\n",
    "\n",
    "    df_number_of_articles_ = df_number_of_articles.set_index(['Author'])\n",
    "\n",
    "    df_collab_ = df_collab.set_index(['Author_1' , 'Author_2'])\n",
    "\n",
    "\n",
    "    for i in range (df_common_partners.shape[0]):\n",
    "        n = len(df_common_partners['Common_partners'][i])\n",
    "        list_ENB = []\n",
    "        for j in range (n):\n",
    "            common_partner = list (df_common_partners['Common_partners'][i])[j]\n",
    "            d = df_number_of_articles_.loc[common_partner,'number_of_articles']\n",
    "            num_article_common_partner = int(d)\n",
    "            Auth_1 = df_common_partners['Author_1'][i]\n",
    "            try:\n",
    "                x = df_collab_.loc[(Auth_1,common_partner),'number_of_collaborations']\n",
    "            except KeyError:\n",
    "                x = 0\n",
    "            if x != 0:\n",
    "                num_collab_Auth_1 = x\n",
    "            else:\n",
    "                num_collab_Auth_1 = df_collab_.loc[(common_partner,Auth_1),'number_of_collaborations']\n",
    "            Auth_2 = df_common_partners['Author_2'][i]\n",
    "            try:\n",
    "                y = df_collab_.loc[(Auth_2,common_partner),'number_of_collaborations']\n",
    "            except KeyError:\n",
    "                y = 0\n",
    "            if y != 0:\n",
    "                num_collab_Auth_2 = y\n",
    "            else:\n",
    "                num_collab_Auth_2 = df_collab_.loc[(common_partner,Auth_2),'number_of_collaborations']\n",
    "            ENB = ((int(num_collab_Auth_1)) * (int(num_collab_Auth_2))) / num_article_common_partner\n",
    "            list_ENB.append(ENB)\n",
    "            TENB = sum(list_ENB)\n",
    "        df_common_partners['TENB'][i] = TENB\n",
    "\n",
    "    df_common_partners = df_common_partners.set_index(['Author_1_2'])\n",
    "\n",
    "    for i in df_common_partners.index:\n",
    "        df_data_set_ind.loc[i,'TENB'] = df_common_partners.loc[i,'TENB']\n",
    "\n",
    "    df_authors_ind = df_authors_ind.set_index('Author_ID')\n",
    "\n",
    "\n",
    "    #Cog_Dist\n",
    "\n",
    "    for i in df_data_set_ind.index:\n",
    "        a1 = df_authors_ind.loc[df_data_set_ind.loc[i, 'Author_1'], ['topic_1', 'topic_2', 'topic_3', 'topic_4','topic_5', 'topic_6','topic_7', 'topic_8','topic_9']]\n",
    "        a1=a1.tolist()\n",
    "        a2 = df_authors_ind.loc[df_data_set_ind.loc[i, 'Author_2'], ['topic_1', 'topic_2', 'topic_3', 'topic_4','topic_5', 'topic_6','topic_7', 'topic_8','topic_9']]\n",
    "        a2=a2.tolist()\n",
    "        if a1 != [0,0,0,0,0,0,0,0,0] and a2 != [0,0,0,0,0,0,0,0,0]:\n",
    "            cor = numpy.corrcoef(a1, a2)\n",
    "            df_data_set_ind.loc[i, 'Cog_Dist'] = 1 - cor[0][1]\n",
    "\n",
    "    # Geo_Dist\n",
    "\n",
    "    import math\n",
    "\n",
    "    def Geo_Distance (lat_1,lon_1,lat_2,lon_2):\n",
    "        R = 6373.0\n",
    "        lat1 = math.radians(lat_1)\n",
    "        lon1 = math.radians(lon_1)\n",
    "        lat2 = math.radians(lat_2)\n",
    "        lon2 = math.radians(lon_2)\n",
    "        dlon = lon2 - lon1\n",
    "        dlat = lat2 - lat1\n",
    "        a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
    "        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "        distance = R * c\n",
    "        return distance\n",
    "\n",
    "    dist = []\n",
    "\n",
    "    for i in df_data_set_ind.index:\n",
    "        author_1 = df_data_set_ind.Author_1[i]\n",
    "        author_2 = df_data_set_ind.Author_2[i]\n",
    "        auth_1_lat = df_authors_ind.Latitude[author_1]\n",
    "        auth_1_lng = df_authors_ind.Longitude[author_1]\n",
    "        auth_2_lat = df_authors_ind.Latitude[author_2]\n",
    "        auth_2_lng = df_authors_ind.Longitude[author_2]\n",
    "\n",
    "        dist.append((author_1, author_2, Geo_Distance(auth_1_lat,auth_1_lng,auth_2_lat,auth_2_lng)))\n",
    "\n",
    "    df_geo_dist = pd.DataFrame(data = dist, columns=['Author_1', 'Author_2', 'GeoDist'])\n",
    "\n",
    "    df_geo_dist.insert(3,'Author_1_2','')\n",
    "\n",
    "    df_geo_dist.Author_1_2 = df_geo_dist.Author_1 + df_geo_dist.Author_2\n",
    "\n",
    "    df_geo_dist.set_index('Author_1_2', inplace = True)\n",
    "\n",
    "    df_data_set_ind.Geo_Dist = df_geo_dist.GeoDist\n",
    "\n",
    "    df_temp = pd.merge(df_data_set_ind, df_authors_ind, left_on = 'Author_1', right_index = True, how = 'left')\n",
    "    df_temp.rename({'Province_code':'Province_code_1'}, axis = 1, inplace = True)\n",
    "    df_temp.drop(['EIDs','partners', 'Aff_ID', 'Latitude', 'Longitude'], axis = 1, inplace = True)\n",
    "    df_temp2 = pd.merge(df_temp, df_authors_ind, left_on = 'Author_2', right_index = True, how = 'left')\n",
    "    df_temp2.drop(['EIDs','partners', 'Aff_ID', 'Latitude', 'Longitude'], axis = 1, inplace = True)\n",
    "    df_temp2.rename({'Province_code':'Province_code_2'}, axis = 1, inplace = True)\n",
    "\n",
    "    def comparison_(x, y):\n",
    "        if x == y:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    df_temp2.Prov_Border = df_temp2.apply(lambda x: comparison_(x.Province_code_1, x.Province_code_2), axis = 1)\n",
    "\n",
    "    df_data_set_ind.Prov_Border = df_temp2.Prov_Border\n",
    "\n",
    "    list_prov = list(df_authors_ind.Province.unique())\n",
    "\n",
    "    dic_contig = {'Nova Scotia':['New Brunswick'], \n",
    "                  'New Brunswick':['Nova Scotia','Quebec'], \n",
    "                  'Newfoundland and Labrador':['Quebec'], \n",
    "                  'Quebec':['New Brunswick', 'Newfoundland and Labrador', 'Ontario'], \n",
    "                  'Ontario':['Quebec', 'Manitoba'], \n",
    "                  'Manitoba':['Ontario','Saskatchewan'], \n",
    "                  'Saskatchewan':['Manitoba','Alberta'], \n",
    "                  'Alberta':['Saskatchewan','British Columbia'], \n",
    "                  'British Columbia':['Alberta']}\n",
    "\n",
    "    df_temp = pd.merge(df_data_set_ind, df_authors_ind, left_on = 'Author_1', right_index = True, how = 'left')\n",
    "    df_temp.rename({'Province':'Province_1'}, axis = 1, inplace = True)\n",
    "    df_temp.drop(['EIDs','partners', 'Aff_ID', 'Latitude', 'Longitude'], axis = 1, inplace = True)\n",
    "    df_temp2 = pd.merge(df_temp, df_authors_ind, left_on = 'Author_2', right_index = True, how = 'left')\n",
    "    df_temp2.drop(['EIDs','partners', 'Aff_ID', 'Latitude', 'Longitude'], axis = 1, inplace = True)\n",
    "    df_temp2.rename({'Province':'Province_2'}, axis = 1, inplace = True)\n",
    "\n",
    "    def contiguity_ (x, y):\n",
    "        c = dic_contig[x]\n",
    "        if c.count(y) == 1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    df_temp2.NotContig = df_temp2.apply(lambda x: contiguity_(x.Province_1, x.Province_2), axis = 1)\n",
    "\n",
    "    df_data_set_ind.NotContig = df_temp2.NotContig\n",
    "\n",
    "        # Province dummies\n",
    "\n",
    "    one_hot = pd.get_dummies(df_temp2[['Province_1','Province_2']])\n",
    "    df_data_set_ind = df_data_set_ind.join(one_hot)\n",
    "\n",
    "    df_data_set_ = df_data_set_dep.merge(df_data_set_ind, right_index = True, left_index = True, how = 'left')\n",
    "    df_data_set = pd.concat([df_data_set,df_data_set_])\n",
    "\n",
    "    \n",
    "df_data_set.reset_index(inplace = True)\n",
    "df_data_set.drop(['Author_1_y', 'Author_2_y'], axis = 1, inplace = True)\n",
    "df_data_set.rename({'Author_1_x' : 'Author_1'}, axis = 1, inplace = True)\n",
    "df_data_set.rename({'Author_2_x' : 'Author_2'}, axis = 1, inplace = True)\n",
    "df_data_set['Log_Geo_Dist'] = df_data_set.Geo_Dist.apply(lambda x :math.log1p(x))\n",
    "df_data_set.fillna(0, inplace = True)\n",
    "\n",
    "df_data_set.insert(27,'Top_regions',0)\n",
    "\n",
    "def Top_regions (reg1, reg2):\n",
    "    if reg1 == 1 and reg2 == 1:\n",
    "        return 1\n",
    "        \n",
    "df_data_set.Top_regions = df_data_set.apply(lambda x: Top_regions(x.Province_1_Quebec, x.Province_2_Quebec), axis = 1)\n",
    "df_data_set.Top_regions = df_data_set.apply(lambda x: Top_regions(x.Province_1_Ontario, x.Province_2_Ontario), axis = 1)\n",
    "df_data_set.Top_regions = df_data_set.apply(lambda x: Top_regions(x['Province_1_British Columbia'], x['Province_2_British Columbia']), axis = 1)\n",
    "\n",
    "df_data_set.Top_regions = df_data_set.apply(lambda x: Top_regions(x.Province_1_Quebec, x.Province_2_Ontario), axis = 1)\n",
    "df_data_set.Top_regions = df_data_set.apply(lambda x: Top_regions(x.Province_1_Ontario, x.Province_2_Quebec), axis = 1)\n",
    "\n",
    "df_data_set.Top_regions = df_data_set.apply(lambda x: Top_regions(x.Province_1_Quebec, x['Province_2_British Columbia']), axis = 1)\n",
    "df_data_set.Top_regions = df_data_set.apply(lambda x: Top_regions(x['Province_1_British Columbia'], x.Province_2_Quebec), axis = 1)\n",
    "\n",
    "df_data_set.Top_regions = df_data_set.apply(lambda x: Top_regions(x.Province_1_Ontario, x['Province_2_British Columbia']), axis = 1)\n",
    "df_data_set.Top_regions = df_data_set.apply(lambda x: Top_regions(x['Province_1_British Columbia'], x.Province_2_Ontario), axis = 1)\n",
    "\n",
    "df_data_set.Top_regions.fillna(0, inplace = True)\n",
    "\n",
    "df_data_set.Cog_Dist = df_data_set.Cog_Dist.map(lambda x: None if x == '' else x)\n",
    "\n",
    "\n",
    "df_data_set.insert(28,'dummy_Quebec',0)\n",
    "df_data_set.insert(29,'dummy_Ontario',0)\n",
    "df_data_set.insert(30,'dummy_British_Columbia',0)\n",
    "df_data_set.insert(31,'dummy_Alberta',0)\n",
    "df_data_set.insert(32,'dummy_Manitoba',0)\n",
    "df_data_set.insert(33,'dummy_Nova_Scotia',0)\n",
    "df_data_set.insert(34,'dummy_Saskatchewan',0)\n",
    "df_data_set.insert(35,'dummy_New_Brunswick',0)\n",
    "\n",
    "\n",
    "\n",
    "def dummy_province_ (reg1, reg2):\n",
    "    if reg1 == 1 or reg2 == 1:\n",
    "        return 1\n",
    "\n",
    "    \n",
    "df_data_set.dummy_Quebec = df_data_set.apply(lambda x: dummy_province_(x.Province_1_Quebec, x.Province_2_Quebec), axis = 1)\n",
    "df_data_set.dummy_Ontario = df_data_set.apply(lambda x: dummy_province_(x.Province_1_Ontario, x.Province_2_Ontario), axis = 1)\n",
    "df_data_set.dummy_British_Columbia = df_data_set.apply(lambda x: dummy_province_(x['Province_1_British Columbia'], x['Province_2_British Columbia']), axis = 1)\n",
    "df_data_set.dummy_Alberta = df_data_set.apply(lambda x: dummy_province_(x.Province_1_Alberta, x.Province_2_Alberta), axis = 1)\n",
    "df_data_set.dummy_Manitoba = df_data_set.apply(lambda x: dummy_province_(x.Province_1_Manitoba, x.Province_2_Manitoba), axis = 1)\n",
    "df_data_set.dummy_Nova_Scotia = df_data_set.apply(lambda x: dummy_province_(x['Province_1_Nova Scotia'], x['Province_2_Nova Scotia']), axis = 1)\n",
    "df_data_set.dummy_Saskatchewan = df_data_set.apply(lambda x: dummy_province_(x.Province_1_Saskatchewan, x.Province_2_Saskatchewan), axis = 1)\n",
    "df_data_set.dummy_New_Brunswick = df_data_set.apply(lambda x: dummy_province_(x['Province_1_New Brunswick'], x['Province_2_New Brunswick']), axis = 1)\n",
    "\n",
    "df_data_set.dummy_Quebec.fillna(0, inplace = True)\n",
    "df_data_set.dummy_Ontario.fillna(0, inplace = True)\n",
    "df_data_set.dummy_British_Columbia.fillna(0, inplace = True)\n",
    "df_data_set.dummy_Alberta.fillna(0, inplace = True)\n",
    "df_data_set.dummy_Manitoba.fillna(0, inplace = True)\n",
    "df_data_set.dummy_Nova_Scotia.fillna(0, inplace = True)\n",
    "df_data_set.dummy_Saskatchewan.fillna(0, inplace = True)\n",
    "df_data_set.dummy_New_Brunswick.fillna(0, inplace = True)\n",
    "\n",
    "df_data_set['Log_Geo_Dist X TENB'] = df_data_set['Log_Geo_Dist'] * df_data_set['TENB']\n",
    "df_data_set['Log_Geo_Dist_Sq'] = df_data_set['Log_Geo_Dist'] * df_data_set['Log_Geo_Dist']\n",
    "df_data_set['Log_Geo_Dist_Sq X TENB'] = df_data_set['Log_Geo_Dist_Sq'] * df_data_set['TENB']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07a6fcb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TENB</th>\n",
       "      <th>Cog_Dist</th>\n",
       "      <th>Log_Geo_Dist</th>\n",
       "      <th>Prov_Border</th>\n",
       "      <th>Top_regions</th>\n",
       "      <th>NotContig</th>\n",
       "      <th>Log_Geo_Dist X TENB</th>\n",
       "      <th>Log_Geo_Dist_Sq</th>\n",
       "      <th>Log_Geo_Dist_Sq X TENB</th>\n",
       "      <th>dummy_Quebec</th>\n",
       "      <th>dummy_Ontario</th>\n",
       "      <th>dummy_British_Columbia</th>\n",
       "      <th>dummy_Alberta</th>\n",
       "      <th>dummy_Manitoba</th>\n",
       "      <th>dummy_Nova_Scotia</th>\n",
       "      <th>dummy_Saskatchewan</th>\n",
       "      <th>dummy_New_Brunswick</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TENB</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.421814</td>\n",
       "      <td>-0.467064</td>\n",
       "      <td>-0.316806</td>\n",
       "      <td>-0.027308</td>\n",
       "      <td>0.127710</td>\n",
       "      <td>0.437271</td>\n",
       "      <td>-0.377707</td>\n",
       "      <td>0.308548</td>\n",
       "      <td>-0.196166</td>\n",
       "      <td>-0.170287</td>\n",
       "      <td>-0.065704</td>\n",
       "      <td>0.153842</td>\n",
       "      <td>0.036910</td>\n",
       "      <td>-0.058311</td>\n",
       "      <td>-0.063163</td>\n",
       "      <td>0.043542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cog_Dist</th>\n",
       "      <td>-0.421814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.326030</td>\n",
       "      <td>0.199841</td>\n",
       "      <td>0.034830</td>\n",
       "      <td>-0.088174</td>\n",
       "      <td>-0.184230</td>\n",
       "      <td>0.273359</td>\n",
       "      <td>-0.130569</td>\n",
       "      <td>0.064216</td>\n",
       "      <td>0.144296</td>\n",
       "      <td>0.019174</td>\n",
       "      <td>0.024299</td>\n",
       "      <td>-0.130228</td>\n",
       "      <td>-0.047158</td>\n",
       "      <td>0.027696</td>\n",
       "      <td>-0.006658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Log_Geo_Dist</th>\n",
       "      <td>-0.467064</td>\n",
       "      <td>0.326030</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.779708</td>\n",
       "      <td>0.106579</td>\n",
       "      <td>-0.044068</td>\n",
       "      <td>-0.028460</td>\n",
       "      <td>0.971230</td>\n",
       "      <td>0.033354</td>\n",
       "      <td>0.007137</td>\n",
       "      <td>0.192525</td>\n",
       "      <td>0.199132</td>\n",
       "      <td>0.239586</td>\n",
       "      <td>-0.001288</td>\n",
       "      <td>0.098664</td>\n",
       "      <td>0.152611</td>\n",
       "      <td>0.052280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prov_Border</th>\n",
       "      <td>-0.316806</td>\n",
       "      <td>0.199841</td>\n",
       "      <td>0.779708</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.077619</td>\n",
       "      <td>-0.362994</td>\n",
       "      <td>-0.037491</td>\n",
       "      <td>0.783819</td>\n",
       "      <td>0.014034</td>\n",
       "      <td>0.069314</td>\n",
       "      <td>0.267841</td>\n",
       "      <td>0.186753</td>\n",
       "      <td>0.227272</td>\n",
       "      <td>0.052587</td>\n",
       "      <td>0.156819</td>\n",
       "      <td>0.162866</td>\n",
       "      <td>0.115931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top_regions</th>\n",
       "      <td>-0.027308</td>\n",
       "      <td>0.034830</td>\n",
       "      <td>0.106579</td>\n",
       "      <td>0.077619</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.076431</td>\n",
       "      <td>-0.011897</td>\n",
       "      <td>0.134066</td>\n",
       "      <td>-0.008414</td>\n",
       "      <td>-0.165440</td>\n",
       "      <td>0.146841</td>\n",
       "      <td>0.415625</td>\n",
       "      <td>-0.096467</td>\n",
       "      <td>-0.025821</td>\n",
       "      <td>-0.034897</td>\n",
       "      <td>-0.037801</td>\n",
       "      <td>-0.024410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NotContig</th>\n",
       "      <td>0.127710</td>\n",
       "      <td>-0.088174</td>\n",
       "      <td>-0.044068</td>\n",
       "      <td>-0.362994</td>\n",
       "      <td>0.076431</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.055638</td>\n",
       "      <td>0.083269</td>\n",
       "      <td>0.039351</td>\n",
       "      <td>-0.249748</td>\n",
       "      <td>-0.433295</td>\n",
       "      <td>-0.019122</td>\n",
       "      <td>0.275536</td>\n",
       "      <td>0.050202</td>\n",
       "      <td>0.145234</td>\n",
       "      <td>0.076076</td>\n",
       "      <td>-0.108803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Log_Geo_Dist X TENB</th>\n",
       "      <td>0.437271</td>\n",
       "      <td>-0.184230</td>\n",
       "      <td>-0.028460</td>\n",
       "      <td>-0.037491</td>\n",
       "      <td>-0.011897</td>\n",
       "      <td>0.055638</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.014789</td>\n",
       "      <td>0.970669</td>\n",
       "      <td>-0.069146</td>\n",
       "      <td>-0.076500</td>\n",
       "      <td>-0.028625</td>\n",
       "      <td>0.066845</td>\n",
       "      <td>-0.018257</td>\n",
       "      <td>-0.025404</td>\n",
       "      <td>-0.027518</td>\n",
       "      <td>0.261047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Log_Geo_Dist_Sq</th>\n",
       "      <td>-0.377707</td>\n",
       "      <td>0.273359</td>\n",
       "      <td>0.971230</td>\n",
       "      <td>0.783819</td>\n",
       "      <td>0.134066</td>\n",
       "      <td>0.083269</td>\n",
       "      <td>-0.014789</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.043562</td>\n",
       "      <td>-0.042351</td>\n",
       "      <td>0.119775</td>\n",
       "      <td>0.230939</td>\n",
       "      <td>0.345534</td>\n",
       "      <td>0.012084</td>\n",
       "      <td>0.087407</td>\n",
       "      <td>0.174515</td>\n",
       "      <td>0.031797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Log_Geo_Dist_Sq X TENB</th>\n",
       "      <td>0.308548</td>\n",
       "      <td>-0.130569</td>\n",
       "      <td>0.033354</td>\n",
       "      <td>0.014034</td>\n",
       "      <td>-0.008414</td>\n",
       "      <td>0.039351</td>\n",
       "      <td>0.970669</td>\n",
       "      <td>0.043562</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.064005</td>\n",
       "      <td>-0.056773</td>\n",
       "      <td>-0.020245</td>\n",
       "      <td>0.065374</td>\n",
       "      <td>-0.013292</td>\n",
       "      <td>-0.017967</td>\n",
       "      <td>-0.019462</td>\n",
       "      <td>0.282274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy_Quebec</th>\n",
       "      <td>-0.196166</td>\n",
       "      <td>0.064216</td>\n",
       "      <td>0.007137</td>\n",
       "      <td>0.069314</td>\n",
       "      <td>-0.165440</td>\n",
       "      <td>-0.249748</td>\n",
       "      <td>-0.069146</td>\n",
       "      <td>-0.042351</td>\n",
       "      <td>-0.064005</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.283584</td>\n",
       "      <td>-0.168892</td>\n",
       "      <td>-0.354404</td>\n",
       "      <td>-0.197158</td>\n",
       "      <td>-0.091604</td>\n",
       "      <td>-0.077084</td>\n",
       "      <td>-0.066690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy_Ontario</th>\n",
       "      <td>-0.170287</td>\n",
       "      <td>0.144296</td>\n",
       "      <td>0.192525</td>\n",
       "      <td>0.267841</td>\n",
       "      <td>0.146841</td>\n",
       "      <td>-0.433295</td>\n",
       "      <td>-0.076500</td>\n",
       "      <td>0.119775</td>\n",
       "      <td>-0.056773</td>\n",
       "      <td>-0.283584</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.116125</td>\n",
       "      <td>-0.239093</td>\n",
       "      <td>-0.113098</td>\n",
       "      <td>-0.093861</td>\n",
       "      <td>-0.123088</td>\n",
       "      <td>-0.067091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy_British_Columbia</th>\n",
       "      <td>-0.065704</td>\n",
       "      <td>0.019174</td>\n",
       "      <td>0.199132</td>\n",
       "      <td>0.186753</td>\n",
       "      <td>0.415625</td>\n",
       "      <td>-0.019122</td>\n",
       "      <td>-0.028625</td>\n",
       "      <td>0.230939</td>\n",
       "      <td>-0.020245</td>\n",
       "      <td>-0.168892</td>\n",
       "      <td>-0.116125</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.046135</td>\n",
       "      <td>0.011595</td>\n",
       "      <td>-0.055805</td>\n",
       "      <td>-0.064643</td>\n",
       "      <td>-0.039316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy_Alberta</th>\n",
       "      <td>0.153842</td>\n",
       "      <td>0.024299</td>\n",
       "      <td>0.239586</td>\n",
       "      <td>0.227272</td>\n",
       "      <td>-0.096467</td>\n",
       "      <td>0.275536</td>\n",
       "      <td>0.066845</td>\n",
       "      <td>0.345534</td>\n",
       "      <td>0.065374</td>\n",
       "      <td>-0.354404</td>\n",
       "      <td>-0.239093</td>\n",
       "      <td>-0.046135</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.030701</td>\n",
       "      <td>-0.123701</td>\n",
       "      <td>-0.130876</td>\n",
       "      <td>-0.087350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy_Manitoba</th>\n",
       "      <td>0.036910</td>\n",
       "      <td>-0.130228</td>\n",
       "      <td>-0.001288</td>\n",
       "      <td>0.052587</td>\n",
       "      <td>-0.025821</td>\n",
       "      <td>0.050202</td>\n",
       "      <td>-0.018257</td>\n",
       "      <td>0.012084</td>\n",
       "      <td>-0.013292</td>\n",
       "      <td>-0.197158</td>\n",
       "      <td>-0.113098</td>\n",
       "      <td>0.011595</td>\n",
       "      <td>0.030701</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.055134</td>\n",
       "      <td>-0.059722</td>\n",
       "      <td>-0.038566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy_Nova_Scotia</th>\n",
       "      <td>-0.058311</td>\n",
       "      <td>-0.047158</td>\n",
       "      <td>0.098664</td>\n",
       "      <td>0.156819</td>\n",
       "      <td>-0.034897</td>\n",
       "      <td>0.145234</td>\n",
       "      <td>-0.025404</td>\n",
       "      <td>0.087407</td>\n",
       "      <td>-0.017967</td>\n",
       "      <td>-0.091604</td>\n",
       "      <td>-0.093861</td>\n",
       "      <td>-0.055805</td>\n",
       "      <td>-0.123701</td>\n",
       "      <td>-0.055134</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.022518</td>\n",
       "      <td>-0.009173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy_Saskatchewan</th>\n",
       "      <td>-0.063163</td>\n",
       "      <td>0.027696</td>\n",
       "      <td>0.152611</td>\n",
       "      <td>0.162866</td>\n",
       "      <td>-0.037801</td>\n",
       "      <td>0.076076</td>\n",
       "      <td>-0.027518</td>\n",
       "      <td>0.174515</td>\n",
       "      <td>-0.019462</td>\n",
       "      <td>-0.077084</td>\n",
       "      <td>-0.123088</td>\n",
       "      <td>-0.064643</td>\n",
       "      <td>-0.130876</td>\n",
       "      <td>-0.059722</td>\n",
       "      <td>-0.022518</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.016333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dummy_New_Brunswick</th>\n",
       "      <td>0.043542</td>\n",
       "      <td>-0.006658</td>\n",
       "      <td>0.052280</td>\n",
       "      <td>0.115931</td>\n",
       "      <td>-0.024410</td>\n",
       "      <td>-0.108803</td>\n",
       "      <td>0.261047</td>\n",
       "      <td>0.031797</td>\n",
       "      <td>0.282274</td>\n",
       "      <td>-0.066690</td>\n",
       "      <td>-0.067091</td>\n",
       "      <td>-0.039316</td>\n",
       "      <td>-0.087350</td>\n",
       "      <td>-0.038566</td>\n",
       "      <td>-0.009173</td>\n",
       "      <td>-0.016333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            TENB  Cog_Dist  Log_Geo_Dist  Prov_Border  \\\n",
       "TENB                    1.000000 -0.421814     -0.467064    -0.316806   \n",
       "Cog_Dist               -0.421814  1.000000      0.326030     0.199841   \n",
       "Log_Geo_Dist           -0.467064  0.326030      1.000000     0.779708   \n",
       "Prov_Border            -0.316806  0.199841      0.779708     1.000000   \n",
       "Top_regions            -0.027308  0.034830      0.106579     0.077619   \n",
       "NotContig               0.127710 -0.088174     -0.044068    -0.362994   \n",
       "Log_Geo_Dist X TENB     0.437271 -0.184230     -0.028460    -0.037491   \n",
       "Log_Geo_Dist_Sq        -0.377707  0.273359      0.971230     0.783819   \n",
       "Log_Geo_Dist_Sq X TENB  0.308548 -0.130569      0.033354     0.014034   \n",
       "dummy_Quebec           -0.196166  0.064216      0.007137     0.069314   \n",
       "dummy_Ontario          -0.170287  0.144296      0.192525     0.267841   \n",
       "dummy_British_Columbia -0.065704  0.019174      0.199132     0.186753   \n",
       "dummy_Alberta           0.153842  0.024299      0.239586     0.227272   \n",
       "dummy_Manitoba          0.036910 -0.130228     -0.001288     0.052587   \n",
       "dummy_Nova_Scotia      -0.058311 -0.047158      0.098664     0.156819   \n",
       "dummy_Saskatchewan     -0.063163  0.027696      0.152611     0.162866   \n",
       "dummy_New_Brunswick     0.043542 -0.006658      0.052280     0.115931   \n",
       "\n",
       "                        Top_regions  NotContig  Log_Geo_Dist X TENB  \\\n",
       "TENB                      -0.027308   0.127710             0.437271   \n",
       "Cog_Dist                   0.034830  -0.088174            -0.184230   \n",
       "Log_Geo_Dist               0.106579  -0.044068            -0.028460   \n",
       "Prov_Border                0.077619  -0.362994            -0.037491   \n",
       "Top_regions                1.000000   0.076431            -0.011897   \n",
       "NotContig                  0.076431   1.000000             0.055638   \n",
       "Log_Geo_Dist X TENB       -0.011897   0.055638             1.000000   \n",
       "Log_Geo_Dist_Sq            0.134066   0.083269            -0.014789   \n",
       "Log_Geo_Dist_Sq X TENB    -0.008414   0.039351             0.970669   \n",
       "dummy_Quebec              -0.165440  -0.249748            -0.069146   \n",
       "dummy_Ontario              0.146841  -0.433295            -0.076500   \n",
       "dummy_British_Columbia     0.415625  -0.019122            -0.028625   \n",
       "dummy_Alberta             -0.096467   0.275536             0.066845   \n",
       "dummy_Manitoba            -0.025821   0.050202            -0.018257   \n",
       "dummy_Nova_Scotia         -0.034897   0.145234            -0.025404   \n",
       "dummy_Saskatchewan        -0.037801   0.076076            -0.027518   \n",
       "dummy_New_Brunswick       -0.024410  -0.108803             0.261047   \n",
       "\n",
       "                        Log_Geo_Dist_Sq  Log_Geo_Dist_Sq X TENB  dummy_Quebec  \\\n",
       "TENB                          -0.377707                0.308548     -0.196166   \n",
       "Cog_Dist                       0.273359               -0.130569      0.064216   \n",
       "Log_Geo_Dist                   0.971230                0.033354      0.007137   \n",
       "Prov_Border                    0.783819                0.014034      0.069314   \n",
       "Top_regions                    0.134066               -0.008414     -0.165440   \n",
       "NotContig                      0.083269                0.039351     -0.249748   \n",
       "Log_Geo_Dist X TENB           -0.014789                0.970669     -0.069146   \n",
       "Log_Geo_Dist_Sq                1.000000                0.043562     -0.042351   \n",
       "Log_Geo_Dist_Sq X TENB         0.043562                1.000000     -0.064005   \n",
       "dummy_Quebec                  -0.042351               -0.064005      1.000000   \n",
       "dummy_Ontario                  0.119775               -0.056773     -0.283584   \n",
       "dummy_British_Columbia         0.230939               -0.020245     -0.168892   \n",
       "dummy_Alberta                  0.345534                0.065374     -0.354404   \n",
       "dummy_Manitoba                 0.012084               -0.013292     -0.197158   \n",
       "dummy_Nova_Scotia              0.087407               -0.017967     -0.091604   \n",
       "dummy_Saskatchewan             0.174515               -0.019462     -0.077084   \n",
       "dummy_New_Brunswick            0.031797                0.282274     -0.066690   \n",
       "\n",
       "                        dummy_Ontario  dummy_British_Columbia  dummy_Alberta  \\\n",
       "TENB                        -0.170287               -0.065704       0.153842   \n",
       "Cog_Dist                     0.144296                0.019174       0.024299   \n",
       "Log_Geo_Dist                 0.192525                0.199132       0.239586   \n",
       "Prov_Border                  0.267841                0.186753       0.227272   \n",
       "Top_regions                  0.146841                0.415625      -0.096467   \n",
       "NotContig                   -0.433295               -0.019122       0.275536   \n",
       "Log_Geo_Dist X TENB         -0.076500               -0.028625       0.066845   \n",
       "Log_Geo_Dist_Sq              0.119775                0.230939       0.345534   \n",
       "Log_Geo_Dist_Sq X TENB      -0.056773               -0.020245       0.065374   \n",
       "dummy_Quebec                -0.283584               -0.168892      -0.354404   \n",
       "dummy_Ontario                1.000000               -0.116125      -0.239093   \n",
       "dummy_British_Columbia      -0.116125                1.000000      -0.046135   \n",
       "dummy_Alberta               -0.239093               -0.046135       1.000000   \n",
       "dummy_Manitoba              -0.113098                0.011595       0.030701   \n",
       "dummy_Nova_Scotia           -0.093861               -0.055805      -0.123701   \n",
       "dummy_Saskatchewan          -0.123088               -0.064643      -0.130876   \n",
       "dummy_New_Brunswick         -0.067091               -0.039316      -0.087350   \n",
       "\n",
       "                        dummy_Manitoba  dummy_Nova_Scotia  dummy_Saskatchewan  \\\n",
       "TENB                          0.036910          -0.058311           -0.063163   \n",
       "Cog_Dist                     -0.130228          -0.047158            0.027696   \n",
       "Log_Geo_Dist                 -0.001288           0.098664            0.152611   \n",
       "Prov_Border                   0.052587           0.156819            0.162866   \n",
       "Top_regions                  -0.025821          -0.034897           -0.037801   \n",
       "NotContig                     0.050202           0.145234            0.076076   \n",
       "Log_Geo_Dist X TENB          -0.018257          -0.025404           -0.027518   \n",
       "Log_Geo_Dist_Sq               0.012084           0.087407            0.174515   \n",
       "Log_Geo_Dist_Sq X TENB       -0.013292          -0.017967           -0.019462   \n",
       "dummy_Quebec                 -0.197158          -0.091604           -0.077084   \n",
       "dummy_Ontario                -0.113098          -0.093861           -0.123088   \n",
       "dummy_British_Columbia        0.011595          -0.055805           -0.064643   \n",
       "dummy_Alberta                 0.030701          -0.123701           -0.130876   \n",
       "dummy_Manitoba                1.000000          -0.055134           -0.059722   \n",
       "dummy_Nova_Scotia            -0.055134           1.000000           -0.022518   \n",
       "dummy_Saskatchewan           -0.059722          -0.022518            1.000000   \n",
       "dummy_New_Brunswick          -0.038566          -0.009173           -0.016333   \n",
       "\n",
       "                        dummy_New_Brunswick  \n",
       "TENB                               0.043542  \n",
       "Cog_Dist                          -0.006658  \n",
       "Log_Geo_Dist                       0.052280  \n",
       "Prov_Border                        0.115931  \n",
       "Top_regions                       -0.024410  \n",
       "NotContig                         -0.108803  \n",
       "Log_Geo_Dist X TENB                0.261047  \n",
       "Log_Geo_Dist_Sq                    0.031797  \n",
       "Log_Geo_Dist_Sq X TENB             0.282274  \n",
       "dummy_Quebec                      -0.066690  \n",
       "dummy_Ontario                     -0.067091  \n",
       "dummy_British_Columbia            -0.039316  \n",
       "dummy_Alberta                     -0.087350  \n",
       "dummy_Manitoba                    -0.038566  \n",
       "dummy_Nova_Scotia                 -0.009173  \n",
       "dummy_Saskatchewan                -0.016333  \n",
       "dummy_New_Brunswick                1.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_data_set[['TENB', 'Cog_Dist', 'Log_Geo_Dist', 'Prov_Border', 'Top_regions','NotContig', 'Log_Geo_Dist X TENB', 'Log_Geo_Dist_Sq', 'Log_Geo_Dist_Sq X TENB',\n",
    "                'dummy_Quebec', 'dummy_Ontario',\n",
    "                'dummy_British_Columbia', 'dummy_Alberta', 'dummy_Manitoba',\n",
    "                'dummy_Nova_Scotia', 'dummy_Saskatchewan', 'dummy_New_Brunswick']]\n",
    "\n",
    "X.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d431e698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "ros = RandomOverSampler()\n",
    "scl = StandardScaler()\n",
    "smt = SMOTE()\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b6f41ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.052667\n",
      "         Iterations 12\n",
      "                           Results: Logit\n",
      "=====================================================================\n",
      "Model:              Logit                Pseudo R-squared: 0.772     \n",
      "Dependent Variable: collaboration_binary AIC:              114.8081  \n",
      "Date:               2021-08-16 23:47     BIC:              139.3218  \n",
      "No. Observations:   995                  Log-Likelihood:   -52.404   \n",
      "Df Model:           4                    LL-Null:          -229.39   \n",
      "Df Residuals:       990                  LLR p-value:      2.4228e-75\n",
      "Converged:          1.0000               Scale:            1.0000    \n",
      "No. Iterations:     12.0000                                          \n",
      "---------------------------------------------------------------------\n",
      "                      Coef.  Std.Err.    z    P>|z|   [0.025   0.975]\n",
      "---------------------------------------------------------------------\n",
      "const                 1.0101   0.4425  2.2826 0.0225   0.1428  1.8775\n",
      "TENB                  1.0234   0.4443  2.3033 0.0213   0.1526  1.8943\n",
      "Log_Geo_Dist         -0.4529   0.1137 -3.9827 0.0001  -0.6758 -0.2300\n",
      "Cog_Dist             -7.2105   2.0227 -3.5647 0.0004 -11.1750 -3.2460\n",
      "Log_Geo_Dist X TENB  -0.0614   0.1253 -0.4901 0.6240  -0.3070  0.1842\n",
      "=====================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scenario 1 : No preprocessing\n",
    "\n",
    "\n",
    "X1 = df_data_set[['Log_Geo_Dist']]\n",
    "X2 = df_data_set[['Log_Geo_Dist','Cog_Dist']]\n",
    "X3 = df_data_set[['Log_Geo_Dist','Cog_Dist','Prov_Border','Top_regions','NotContig']]\n",
    "\n",
    "X4 = df_data_set[['Log_Geo_Dist','Cog_Dist','Prov_Border','Top_regions','NotContig',\n",
    "                'dummy_Quebec', 'dummy_Ontario', 'dummy_British_Columbia', 'dummy_Alberta', 'dummy_Manitoba', 'dummy_Nova_Scotia', 'dummy_Saskatchewan']]\n",
    "\n",
    "X5 = df_data_set[['TENB','Log_Geo_Dist','Cog_Dist','Prov_Border','Top_regions','NotContig',\n",
    "                'dummy_Quebec', 'dummy_Ontario','dummy_British_Columbia', 'dummy_Alberta', 'dummy_Manitoba','dummy_Nova_Scotia', 'dummy_Saskatchewan']]\n",
    "\n",
    "X6 = df_data_set[['TENB','Log_Geo_Dist','Cog_Dist','Prov_Border','Top_regions','NotContig']]\n",
    "X7 = df_data_set[['TENB','Log_Geo_Dist','Cog_Dist', 'Log_Geo_Dist X TENB']]\n",
    "X8 = df_data_set[['TENB','Log_Geo_Dist','Cog_Dist', 'Log_Geo_Dist X TENB','Log_Geo_Dist_Sq X TENB']]\n",
    "\n",
    "X9 = df_data_set[['Log_Geo_Dist','Cog_Dist','Top_regions','NotContig']]\n",
    "X10 = df_data_set[['Log_Geo_Dist','Cog_Dist','Top_regions','NotContig',\n",
    "                'dummy_Quebec', 'dummy_Ontario','dummy_British_Columbia', 'dummy_Alberta', 'dummy_Manitoba','dummy_Nova_Scotia', 'dummy_Saskatchewan']]\n",
    "\n",
    "X11 = df_data_set[['TENB','Log_Geo_Dist','Cog_Dist']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X1 = sm.tools.tools.add_constant(X1, prepend=True, has_constant='add')\n",
    "X2 = sm.tools.tools.add_constant(X2, prepend=True, has_constant='add')\n",
    "X3 = sm.tools.tools.add_constant(X3, prepend=True, has_constant='add')\n",
    "X4 = sm.tools.tools.add_constant(X4, prepend=True, has_constant='add')\n",
    "X5 = sm.tools.tools.add_constant(X5, prepend=True, has_constant='add')\n",
    "X6 = sm.tools.tools.add_constant(X6, prepend=True, has_constant='add')\n",
    "X7 = sm.tools.tools.add_constant(X7, prepend=True, has_constant='add')\n",
    "X8 = sm.tools.tools.add_constant(X8, prepend=True, has_constant='add')\n",
    "X9 = sm.tools.tools.add_constant(X9, prepend=True, has_constant='add')\n",
    "X10 = sm.tools.tools.add_constant(X10, prepend=True, has_constant='add')\n",
    "X11 = sm.tools.tools.add_constant(X11, prepend=True, has_constant='add')\n",
    "\n",
    "\n",
    "X = [X7]\n",
    "\n",
    "y = df_data_set['collaboration_binary']\n",
    "\n",
    "for X in X:\n",
    "    logit_model=sm.Logit(y, X)\n",
    "    result=logit_model.fit(method_kwargs={\"warn_convergence\": False})\n",
    "    print(result.summary2())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
