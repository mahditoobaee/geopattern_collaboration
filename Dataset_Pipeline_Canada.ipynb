{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dangerous-lebanon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    div#notebook-container    { width: 95%; }\n",
       "    div#menubar-container     { width: 85%; }\n",
       "    div#maintoolbar-container { width: 99%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enlarging the screen is done!\n",
      "Libraries were imported successfully!\n",
      "Loading data from sql is done!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ccb035f1174c4da34c6145f3981ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing the data set:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enlarging the screen\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 85%; }\n",
    "    div#maintoolbar-container { width: 99%; }\n",
    "</style>\n",
    "\"\"\"))\n",
    "\n",
    "print ('Enlarging the screen is done!')\n",
    "\n",
    "# Importing libraries\n",
    "\n",
    "import numpy\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "print ('Libraries were imported successfully!')\n",
    "\n",
    "# Loading data from sql\n",
    "\n",
    "Server = 'LAPTOP-I7NEB9V3\\SQLEXPRESS'\n",
    "Database = 'Geopattern'\n",
    "Driver = 'ODBC Driver 17 for SQL Server'\n",
    "Database_Connection = f'mssql://@{Server}/{Database}?driver={Driver}'\n",
    "\n",
    "engine = create_engine(Database_Connection)\n",
    "connection = engine.connect()\n",
    "\n",
    "df_my_data = pd.read_sql_query (\n",
    "    \"select * from my_data\", connection)\n",
    "\n",
    "df_Authors = pd.read_sql_query (\n",
    "    \"select * from my_data_Authors\", connection)\n",
    "\n",
    "df_only_Canada = pd.read_sql_query (\n",
    "    \"select distinct my_data.EID, my_data.Author_ID, my_data.Year, my_data.Country from my_data join \\\n",
    "        (select distinct EID from my_data \\\n",
    "        except \\\n",
    "        (select distinct EID from my_data \\\n",
    "        where Country not like 'Canada' or Country is Null)) a \\\n",
    "        on my_data.EID = a.EID\", connection)\n",
    "\n",
    "df_only_Canada.Year = df_only_Canada.Year.astype(int)\n",
    "\n",
    "df_LDA = pd.read_csv(r'C:\\Users\\moham\\Dropbox\\QSE\\Thesis\\Geopattern\\My data\\df_LDA.csv')\n",
    "df_LDA.set_index('EID', inplace = True)\n",
    "\n",
    "print ('Loading data from sql is done!')\n",
    "\n",
    "width_ind = 3\n",
    "width_dep = 2\n",
    "\n",
    "df_data_set = pd.DataFrame()\n",
    "\n",
    "list_years = []\n",
    "\n",
    "for yrs in range(21 - width_ind - width_dep):\n",
    "    list_years.append(2000 + yrs)\n",
    "\n",
    "\n",
    "for yr in tqdm(list_years, desc = 'Preparing the data set'):\n",
    "\n",
    "\n",
    "    Ind_win_start = yr\n",
    "    Ind_win_end = Ind_win_start + width_ind - 1\n",
    "\n",
    "    dep_win_start = Ind_win_end + 1\n",
    "    dep_win_end = dep_win_start + width_dep - 1\n",
    "\n",
    "    df_ind = df_only_Canada[(df_only_Canada.Year > (Ind_win_start - 1)) & (df_only_Canada.Year <= Ind_win_end)]\n",
    "    df_dep = df_only_Canada[(df_only_Canada.Year > (dep_win_start - 1)) & (df_only_Canada.Year <= dep_win_end)]\n",
    "\n",
    "    #     df_ind.rename({'EID' : 'EIDs_ind'}, axis = 1, inplace = True)\n",
    "    #     df_dep.rename({'EID' : 'EIDs_dep'}, axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "    # Targets.................................................................................................. \n",
    "\n",
    "    list_authors_dep = df_dep.Author_ID.unique().tolist()\n",
    "    list_authors_dep.sort()\n",
    "\n",
    "    df_authors_dep = pd.DataFrame(data = list_authors_dep, columns = ['Author_ID'])\n",
    "    df_authors_dep.insert(1,'EIDs','')\n",
    "\n",
    "    # df_authors_dep = df_authors_dep.merge(df_Authors, on = 'Author_ID', how = 'left')\n",
    "\n",
    "    authors_dep = []\n",
    "    for i in range (df_authors_dep.shape[0]):\n",
    "        for j in range (df_authors_dep.shape[0]):\n",
    "            if j > i:\n",
    "                authors_dep.append((df_authors_dep['Author_ID'][i] + df_authors_dep['Author_ID'][j], df_authors_dep['Author_ID'][i], df_authors_dep['Author_ID'][j]))\n",
    "\n",
    "    df_data_set_dep = pd.DataFrame(data = authors_dep, columns=['Author_1_2', 'Author_1', 'Author_2'])\n",
    "\n",
    "    df_data_set_dep = df_data_set_dep.set_index(['Author_1_2'])\n",
    "\n",
    "    df_data_set_dep.insert(2,'number_of_collaborations',0)\n",
    "    df_data_set_dep.insert(3,'collaboration_binary',0)\n",
    "\n",
    "    df_dep.reset_index(inplace = True)\n",
    "\n",
    "    df_authors_dep['EIDs'] = ''\n",
    "    for i in range (df_authors_dep.shape[0]):\n",
    "        for j in range (df_dep.shape[0]):\n",
    "            if df_authors_dep['Author_ID'][i] == df_dep['Author_ID'][j]:\n",
    "                df_authors_dep['EIDs'][i] = str (df_authors_dep['EIDs'][i]) + ';' + str (df_dep['EID'][j])\n",
    "\n",
    "    res = []\n",
    "\n",
    "    for i in range (df_authors_dep.shape[0]):\n",
    "        l = df_authors_dep['EIDs'][i].split(';')\n",
    "        l.remove('')\n",
    "        res.append((df_authors_dep['Author_ID'][i],set(l)))\n",
    "\n",
    "\n",
    "    collab_matrix = np.zeros((df_authors_dep.shape[0],df_authors_dep.shape[0]))\n",
    "\n",
    "    for i in range (len(res)):\n",
    "        for j in range (len(res)):\n",
    "            collab_matrix[i,j] = len(res[i][1].intersection(res[j][1]))\n",
    "\n",
    "    collab_list = []\n",
    "    for i in range (collab_matrix.shape[0]):\n",
    "        for j in range (collab_matrix.shape[0]):\n",
    "            if j > i:\n",
    "                if collab_matrix[i,j] != 0:\n",
    "                    collab_list.append((df_authors_dep['Author_ID'][i] + df_authors_dep['Author_ID'][j], df_authors_dep['Author_ID'][i], df_authors_dep['Author_ID'][j], collab_matrix[i,j]))\n",
    "\n",
    "    df_collab = pd.DataFrame(data = collab_list, columns=['Author_1_2', 'Author_1', 'Author_2', 'number_of_collaborations'])\n",
    "\n",
    "    df_collab = df_collab.set_index(['Author_1_2'])\n",
    "\n",
    "\n",
    "    for i in df_data_set_dep.index:\n",
    "        try:\n",
    "            df_data_set_dep.loc[i,'number_of_collaborations'] = df_collab.loc[i,'number_of_collaborations']\n",
    "        except:\n",
    "            df_data_set_dep.loc[i,'number_of_collaborations'] = 0\n",
    "\n",
    "\n",
    "    df_data_set_dep.collaboration_binary = df_data_set_dep.number_of_collaborations.map(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "\n",
    "    # Features............................................................................................\n",
    "\n",
    "\n",
    "    list_authors_ind = df_dep.Author_ID.unique().tolist()\n",
    "    list_authors_ind.sort()\n",
    "\n",
    "\n",
    "    df_authors_ind = pd.DataFrame(data = list_authors_ind, columns = ['Author_ID'])\n",
    "    df_authors_ind.insert(1,'EIDs','')\n",
    "    df_authors_ind.insert(2,'partners',0)\n",
    "    df_authors_ind.insert(3,'topic_1',0)\n",
    "    df_authors_ind.insert(4,'topic_2',0)\n",
    "    df_authors_ind.insert(5,'topic_3',0)\n",
    "    df_authors_ind.insert(6,'topic_4',0)\n",
    "    df_authors_ind.insert(7,'topic_5',0)\n",
    "    df_authors_ind.insert(8,'topic_6',0)\n",
    "    df_authors_ind.insert(9,'topic_7',0)\n",
    "    df_authors_ind.insert(10,'topic_8',0)\n",
    "    df_authors_ind.insert(11,'topic_9',0)\n",
    "\n",
    "\n",
    "    df_authors_ind = df_authors_ind.merge(df_Authors, on = 'Author_ID', how = 'left')\n",
    "\n",
    "    df_authors_ind.reset_index(inplace = True)\n",
    "    df_ind.reset_index(inplace = True)\n",
    "\n",
    "    df_authors_ind['EIDs'] = ''\n",
    "\n",
    "    for i in range (df_authors_ind.shape[0]):\n",
    "        for j in range (df_ind.shape[0]):\n",
    "            if df_authors_ind['Author_ID'][i] == df_ind['Author_ID'][j]:\n",
    "                df_authors_ind['EIDs'][i] = str (df_authors_ind['EIDs'][i]) + ';' + str (df_ind['EID'][j])\n",
    "                df_authors_ind.loc[i, 'topic_1'] += df_LDA.loc[str (df_ind['EID'][j]), 'topic_1']\n",
    "                df_authors_ind.loc[i, 'topic_2'] += df_LDA.loc[str (df_ind['EID'][j]), 'topic_2']\n",
    "                df_authors_ind.loc[i, 'topic_3'] += df_LDA.loc[str (df_ind['EID'][j]), 'topic_3']\n",
    "                df_authors_ind.loc[i, 'topic_4'] += df_LDA.loc[str (df_ind['EID'][j]), 'topic_4']\n",
    "                df_authors_ind.loc[i, 'topic_5'] += df_LDA.loc[str (df_ind['EID'][j]), 'topic_5']\n",
    "                df_authors_ind.loc[i, 'topic_6'] += df_LDA.loc[str (df_ind['EID'][j]), 'topic_6']\n",
    "                df_authors_ind.loc[i, 'topic_7'] += df_LDA.loc[str (df_ind['EID'][j]), 'topic_7']\n",
    "                df_authors_ind.loc[i, 'topic_8'] += df_LDA.loc[str (df_ind['EID'][j]), 'topic_8']\n",
    "                df_authors_ind.loc[i, 'topic_9'] += df_LDA.loc[str (df_ind['EID'][j]), 'topic_9']\n",
    "\n",
    "    for i in range (df_authors_ind.shape[0]):\n",
    "        summ = df_authors_ind.loc[i,'topic_1'] + df_authors_ind.loc[i,'topic_2'] + df_authors_ind.loc[i,'topic_3'] + df_authors_ind.loc[i,'topic_4'] + df_authors_ind.loc[i,'topic_5'] + df_authors_ind.loc[i,'topic_6'] + df_authors_ind.loc[i,'topic_7'] + df_authors_ind.loc[i,'topic_8'] + df_authors_ind.loc[i,'topic_9']\n",
    "        if summ > 0:\n",
    "            df_authors_ind.loc[i,'topic_1'] = df_authors_ind.loc[i,'topic_1']/summ\n",
    "            df_authors_ind.loc[i,'topic_2'] = df_authors_ind.loc[i,'topic_2']/summ\n",
    "            df_authors_ind.loc[i,'topic_3'] = df_authors_ind.loc[i,'topic_3']/summ\n",
    "            df_authors_ind.loc[i,'topic_4'] = df_authors_ind.loc[i,'topic_4']/summ\n",
    "            df_authors_ind.loc[i,'topic_5'] = df_authors_ind.loc[i,'topic_5']/summ\n",
    "            df_authors_ind.loc[i,'topic_6'] = df_authors_ind.loc[i,'topic_6']/summ\n",
    "            df_authors_ind.loc[i,'topic_7'] = df_authors_ind.loc[i,'topic_7']/summ\n",
    "            df_authors_ind.loc[i,'topic_8'] = df_authors_ind.loc[i,'topic_8']/summ\n",
    "            df_authors_ind.loc[i,'topic_9'] = df_authors_ind.loc[i,'topic_9']/summ\n",
    "\n",
    "    res = []\n",
    "\n",
    "    for i in range (df_authors_ind.shape[0]):\n",
    "        l = df_authors_ind['EIDs'][i].split(';')\n",
    "        l.remove('')\n",
    "        res.append((df_authors_ind['Author_ID'][i],set(l)))\n",
    "\n",
    "\n",
    "    collab_matrix = np.zeros((df_authors_ind.shape[0],df_authors_ind.shape[0]))\n",
    "\n",
    "    for i in range (len(res)):\n",
    "        for j in range (len(res)):\n",
    "            collab_matrix[i,j] = len(res[i][1].intersection(res[j][1]))\n",
    "\n",
    "    collab_list = []\n",
    "    for i in range (collab_matrix.shape[0]):\n",
    "        for j in range (collab_matrix.shape[0]):\n",
    "            if j > i:\n",
    "                if collab_matrix[i,j] != 0:\n",
    "                    collab_list.append((df_authors_ind['Author_ID'][i] + df_authors_ind['Author_ID'][j], df_authors_ind['Author_ID'][i], df_authors_ind['Author_ID'][j], collab_matrix[i,j]))\n",
    "\n",
    "    df_collab = pd.DataFrame(data = collab_list, columns=['Author_1_2', 'Author_1', 'Author_2', 'number_of_collaborations'])\n",
    "\n",
    "\n",
    "    df_collab = df_collab.set_index(['Author_1_2'])\n",
    "\n",
    "    df_authors_ind ['partners'] = ''\n",
    "    for i in range (collab_matrix.shape[0]):\n",
    "        for j in range (collab_matrix.shape[0]):\n",
    "            if i != j:\n",
    "                if collab_matrix[i,j] != 0:\n",
    "                    df_authors_ind ['partners'][i] = str (df_authors_ind ['partners'][i]) + ';' + str (df_authors_ind ['Author_ID'][j])\n",
    "\n",
    "\n",
    "    df_data_set_ind = pd.DataFrame(data = authors_dep, columns=['Author_1_2', 'Author_1', 'Author_2'])\n",
    "\n",
    "    df_data_set_ind = df_data_set_ind.set_index(['Author_1_2'])\n",
    "\n",
    "    df_data_set_ind.insert(2,'TENB',0)\n",
    "    df_data_set_ind.insert(3,'Cog_Dist', '')\n",
    "    df_data_set_ind.insert(4,'Geo_Dist',0)\n",
    "    df_data_set_ind.insert(5,'Prov_Border',0)\n",
    "    df_data_set_ind.insert(6,'NotContig',0)\n",
    "\n",
    "    # TENB\n",
    "\n",
    "    res_p = []\n",
    "\n",
    "    for i in range (df_authors_ind.shape[0]):\n",
    "        l = df_authors_ind['partners'][i].split(';')\n",
    "        l.remove('')\n",
    "        res_p.append((df_authors_ind['Author_ID'][i],set(l)))\n",
    "\n",
    "    common_partners_matrix = np.zeros((df_authors_ind.shape[0],df_authors_ind.shape[0]))\n",
    "\n",
    "    for i in range (len(res_p)):\n",
    "        for j in range (len(res_p)):\n",
    "            common_partners_matrix[i,j] = len(res_p[i][1].intersection(res_p[j][1]))\n",
    "\n",
    "\n",
    "    df_common_partners = pd.DataFrame([])\n",
    "    df_common_partners.insert(0,'Author_1_2','')\n",
    "    df_common_partners.insert(1,'Author_1','')\n",
    "    df_common_partners.insert(2,'Author_2','')\n",
    "    df_common_partners.insert(3,'Common_partners',{})\n",
    "    df_common_partners.insert(4,'TENB',float)\n",
    "\n",
    "\n",
    "    for i in range (common_partners_matrix.shape[0]):\n",
    "        for j in range (common_partners_matrix.shape[0]):\n",
    "            if j > i:\n",
    "                if common_partners_matrix[i,j] != 0:\n",
    "                    df_common_partners = df_common_partners.append({'Author_1_2': df_authors_ind['Author_ID'][i] + df_authors_ind['Author_ID'][j],'Author_1': df_authors_ind['Author_ID'][i], 'Author_2': df_authors_ind['Author_ID'][j], 'Common_partners': res_p[i][1].intersection(res_p[j][1])}, ignore_index = True)\n",
    "\n",
    "\n",
    "    number_of_articles = []\n",
    "    for i in range (collab_matrix.shape[0]):\n",
    "        number_of_articles.append((df_authors_ind['Author_ID'][i], collab_matrix[i,i]))\n",
    "\n",
    "    df_number_of_articles = pd.DataFrame(data = number_of_articles, columns=['Author', 'number_of_articles'])\n",
    "\n",
    "    df_number_of_articles_ = df_number_of_articles.set_index(['Author'])\n",
    "\n",
    "    df_collab_ = df_collab.set_index(['Author_1' , 'Author_2'])\n",
    "\n",
    "\n",
    "    for i in range (df_common_partners.shape[0]):\n",
    "        n = len(df_common_partners['Common_partners'][i])\n",
    "        list_ENB = []\n",
    "        for j in range (n):\n",
    "            common_partner = list (df_common_partners['Common_partners'][i])[j]\n",
    "            d = df_number_of_articles_.loc[common_partner,'number_of_articles']\n",
    "            num_article_common_partner = int(d)\n",
    "            Auth_1 = df_common_partners['Author_1'][i]\n",
    "            try:\n",
    "                x = df_collab_.loc[(Auth_1,common_partner),'number_of_collaborations']\n",
    "            except KeyError:\n",
    "                x = 0\n",
    "            if x != 0:\n",
    "                num_collab_Auth_1 = x\n",
    "            else:\n",
    "                num_collab_Auth_1 = df_collab_.loc[(common_partner,Auth_1),'number_of_collaborations']\n",
    "            Auth_2 = df_common_partners['Author_2'][i]\n",
    "            try:\n",
    "                y = df_collab_.loc[(Auth_2,common_partner),'number_of_collaborations']\n",
    "            except KeyError:\n",
    "                y = 0\n",
    "            if y != 0:\n",
    "                num_collab_Auth_2 = y\n",
    "            else:\n",
    "                num_collab_Auth_2 = df_collab_.loc[(common_partner,Auth_2),'number_of_collaborations']\n",
    "            ENB = ((int(num_collab_Auth_1)) * (int(num_collab_Auth_2))) / num_article_common_partner\n",
    "            list_ENB.append(ENB)\n",
    "            TENB = sum(list_ENB)\n",
    "        df_common_partners['TENB'][i] = TENB\n",
    "\n",
    "    df_common_partners = df_common_partners.set_index(['Author_1_2'])\n",
    "\n",
    "    for i in df_common_partners.index:\n",
    "        df_data_set_ind.loc[i,'TENB'] = df_common_partners.loc[i,'TENB']\n",
    "\n",
    "    df_authors_ind = df_authors_ind.set_index('Author_ID')\n",
    "\n",
    "\n",
    "    #Cog_Dist\n",
    "\n",
    "    for i in df_data_set_ind.index:\n",
    "        a1 = df_authors_ind.loc[df_data_set_ind.loc[i, 'Author_1'], ['topic_1', 'topic_2', 'topic_3', 'topic_4','topic_5', 'topic_6','topic_7', 'topic_8','topic_9']]\n",
    "        a1=a1.tolist()\n",
    "        a2 = df_authors_ind.loc[df_data_set_ind.loc[i, 'Author_2'], ['topic_1', 'topic_2', 'topic_3', 'topic_4','topic_5', 'topic_6','topic_7', 'topic_8','topic_9']]\n",
    "        a2=a2.tolist()\n",
    "        if a1 != [0,0,0,0,0,0,0,0,0] and a2 != [0,0,0,0,0,0,0,0,0]:\n",
    "            cor = numpy.corrcoef(a1, a2)\n",
    "            df_data_set_ind.loc[i, 'Cog_Dist'] = 1 - cor[0][1]\n",
    "\n",
    "    # Geo_Dist\n",
    "\n",
    "    import math\n",
    "\n",
    "    def Geo_Distance (lat_1,lon_1,lat_2,lon_2):\n",
    "        R = 6373.0\n",
    "        lat1 = math.radians(lat_1)\n",
    "        lon1 = math.radians(lon_1)\n",
    "        lat2 = math.radians(lat_2)\n",
    "        lon2 = math.radians(lon_2)\n",
    "        dlon = lon2 - lon1\n",
    "        dlat = lat2 - lat1\n",
    "        a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
    "        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "        distance = R * c\n",
    "        return distance\n",
    "\n",
    "    dist = []\n",
    "\n",
    "    for i in df_data_set_ind.index:\n",
    "        author_1 = df_data_set_ind.Author_1[i]\n",
    "        author_2 = df_data_set_ind.Author_2[i]\n",
    "        auth_1_lat = df_authors_ind.Latitude[author_1]\n",
    "        auth_1_lng = df_authors_ind.Longitude[author_1]\n",
    "        auth_2_lat = df_authors_ind.Latitude[author_2]\n",
    "        auth_2_lng = df_authors_ind.Longitude[author_2]\n",
    "\n",
    "        dist.append((author_1, author_2, Geo_Distance(auth_1_lat,auth_1_lng,auth_2_lat,auth_2_lng)))\n",
    "\n",
    "    df_geo_dist = pd.DataFrame(data = dist, columns=['Author_1', 'Author_2', 'GeoDist'])\n",
    "\n",
    "    df_geo_dist.insert(3,'Author_1_2','')\n",
    "\n",
    "    df_geo_dist.Author_1_2 = df_geo_dist.Author_1 + df_geo_dist.Author_2\n",
    "\n",
    "    df_geo_dist.set_index('Author_1_2', inplace = True)\n",
    "\n",
    "    df_data_set_ind.Geo_Dist = df_geo_dist.GeoDist\n",
    "\n",
    "    df_temp = pd.merge(df_data_set_ind, df_authors_ind, left_on = 'Author_1', right_index = True, how = 'left')\n",
    "    df_temp.rename({'Province_code':'Province_code_1'}, axis = 1, inplace = True)\n",
    "    df_temp.drop(['EIDs','partners', 'Aff_ID', 'Latitude', 'Longitude'], axis = 1, inplace = True)\n",
    "    df_temp2 = pd.merge(df_temp, df_authors_ind, left_on = 'Author_2', right_index = True, how = 'left')\n",
    "    df_temp2.drop(['EIDs','partners', 'Aff_ID', 'Latitude', 'Longitude'], axis = 1, inplace = True)\n",
    "    df_temp2.rename({'Province_code':'Province_code_2'}, axis = 1, inplace = True)\n",
    "\n",
    "    def comparison_(x, y):\n",
    "        if x == y:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    df_temp2.Prov_Border = df_temp2.apply(lambda x: comparison_(x.Province_code_1, x.Province_code_2), axis = 1)\n",
    "\n",
    "    df_data_set_ind.Prov_Border = df_temp2.Prov_Border\n",
    "\n",
    "    list_prov = list(df_authors_ind.Province.unique())\n",
    "\n",
    "    dic_contig = {'Nova Scotia':['New Brunswick'], \n",
    "                  'New Brunswick':['Nova Scotia','Quebec'], \n",
    "                  'Newfoundland and Labrador':['Quebec'], \n",
    "                  'Quebec':['New Brunswick', 'Newfoundland and Labrador', 'Ontario'], \n",
    "                  'Ontario':['Quebec', 'Manitoba'], \n",
    "                  'Manitoba':['Ontario','Saskatchewan'], \n",
    "                  'Saskatchewan':['Manitoba','Alberta'], \n",
    "                  'Alberta':['Saskatchewan','British Columbia'], \n",
    "                  'British Columbia':['Alberta']}\n",
    "\n",
    "    df_temp = pd.merge(df_data_set_ind, df_authors_ind, left_on = 'Author_1', right_index = True, how = 'left')\n",
    "    df_temp.rename({'Province':'Province_1'}, axis = 1, inplace = True)\n",
    "    df_temp.drop(['EIDs','partners', 'Aff_ID', 'Latitude', 'Longitude'], axis = 1, inplace = True)\n",
    "    df_temp2 = pd.merge(df_temp, df_authors_ind, left_on = 'Author_2', right_index = True, how = 'left')\n",
    "    df_temp2.drop(['EIDs','partners', 'Aff_ID', 'Latitude', 'Longitude'], axis = 1, inplace = True)\n",
    "    df_temp2.rename({'Province':'Province_2'}, axis = 1, inplace = True)\n",
    "\n",
    "    def contiguity_ (x, y):\n",
    "        c = dic_contig[x]\n",
    "        if c.count(y) == 1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    df_temp2.NotContig = df_temp2.apply(lambda x: contiguity_(x.Province_1, x.Province_2), axis = 1)\n",
    "\n",
    "    df_data_set_ind.NotContig = df_temp2.NotContig\n",
    "\n",
    "        # Province dummies\n",
    "\n",
    "    one_hot = pd.get_dummies(df_temp2[['Province_1','Province_2']])\n",
    "    df_data_set_ind = df_data_set_ind.join(one_hot)\n",
    "\n",
    "    df_data_set_ = df_data_set_dep.merge(df_data_set_ind, right_index = True, left_index = True, how = 'left')\n",
    "    df_data_set = pd.concat([df_data_set,df_data_set_])\n",
    "\n",
    "    \n",
    "df_data_set.reset_index(inplace = True)\n",
    "df_data_set.drop(['Author_1_y', 'Author_2_y'], axis = 1, inplace = True)\n",
    "df_data_set.rename({'Author_1_x' : 'Author_1'}, axis = 1, inplace = True)\n",
    "df_data_set.rename({'Author_2_x' : 'Author_2'}, axis = 1, inplace = True)\n",
    "df_data_set['Log_Geo_Dist'] = df_data_set.Geo_Dist.apply(lambda x :math.log1p(x))\n",
    "df_data_set.fillna(0, inplace = True)\n",
    "\n",
    "df_data_set.insert(28,'Top_regions',0)\n",
    "\n",
    "def Top_regions (reg1, reg2):\n",
    "    if reg1 == 1 and reg2 == 1:\n",
    "        return 1\n",
    "        \n",
    "df_data_set.Top_regions = df_data_set.apply(lambda x: Top_regions(x.Province_1_Quebec, x.Province_2_Quebec), axis = 1)\n",
    "df_data_set.Top_regions = df_data_set.apply(lambda x: Top_regions(x.Province_1_Ontario, x.Province_2_Ontario), axis = 1)\n",
    "df_data_set.Top_regions = df_data_set.apply(lambda x: Top_regions(x['Province_1_British Columbia'], x['Province_2_British Columbia']), axis = 1)\n",
    "\n",
    "df_data_set.Top_regions = df_data_set.apply(lambda x: Top_regions(x.Province_1_Quebec, x.Province_2_Ontario), axis = 1)\n",
    "df_data_set.Top_regions = df_data_set.apply(lambda x: Top_regions(x.Province_1_Ontario, x.Province_2_Quebec), axis = 1)\n",
    "\n",
    "df_data_set.Top_regions = df_data_set.apply(lambda x: Top_regions(x.Province_1_Quebec, x['Province_2_British Columbia']), axis = 1)\n",
    "df_data_set.Top_regions = df_data_set.apply(lambda x: Top_regions(x['Province_1_British Columbia'], x.Province_2_Quebec), axis = 1)\n",
    "\n",
    "df_data_set.Top_regions = df_data_set.apply(lambda x: Top_regions(x.Province_1_Ontario, x['Province_2_British Columbia']), axis = 1)\n",
    "df_data_set.Top_regions = df_data_set.apply(lambda x: Top_regions(x['Province_1_British Columbia'], x.Province_2_Ontario), axis = 1)\n",
    "\n",
    "df_data_set.Top_regions.fillna(0, inplace = True)\n",
    "\n",
    "df_data_set.Cog_Dist = df_data_set.Cog_Dist.map(lambda x: None if x == '' else x)\n",
    "\n",
    "df_data_set.insert(29,'dummy_Quebec',0)\n",
    "df_data_set.insert(30,'dummy_Ontario',0)\n",
    "df_data_set.insert(31,'dummy_British_Columbia',0)\n",
    "df_data_set.insert(32,'dummy_Alberta',0)\n",
    "df_data_set.insert(33,'dummy_Manitoba',0)\n",
    "df_data_set.insert(34,'dummy_Nova_Scotia',0)\n",
    "df_data_set.insert(35,'dummy_Saskatchewan',0)\n",
    "df_data_set.insert(36,'dummy_New_Brunswick',0)\n",
    "df_data_set.insert(37,'dummy_Newfoundland_and_Labrador',0)\n",
    "\n",
    "def dummy_province_ (reg1, reg2):\n",
    "    if reg1 == 1 or reg2 == 1:\n",
    "        return 1\n",
    "\n",
    "df_data_set.dummy_Quebec = df_data_set.apply(lambda x: dummy_province_(x.Province_1_Quebec, x.Province_2_Quebec), axis = 1)\n",
    "df_data_set.dummy_Ontario = df_data_set.apply(lambda x: dummy_province_(x.Province_1_Ontario, x.Province_2_Ontario), axis = 1)\n",
    "df_data_set.dummy_British_Columbia = df_data_set.apply(lambda x: dummy_province_(x['Province_1_British Columbia'], x['Province_2_British Columbia']), axis = 1)\n",
    "df_data_set.dummy_Alberta = df_data_set.apply(lambda x: dummy_province_(x.Province_1_Alberta, x.Province_2_Alberta), axis = 1)\n",
    "df_data_set.dummy_Manitoba = df_data_set.apply(lambda x: dummy_province_(x.Province_1_Manitoba, x.Province_2_Manitoba), axis = 1)\n",
    "df_data_set.dummy_Nova_Scotia = df_data_set.apply(lambda x: dummy_province_(x['Province_1_Nova Scotia'], x['Province_2_Nova Scotia']), axis = 1)\n",
    "df_data_set.dummy_Saskatchewan = df_data_set.apply(lambda x: dummy_province_(x.Province_1_Saskatchewan, x.Province_2_Saskatchewan), axis = 1)\n",
    "df_data_set.dummy_New_Brunswick = df_data_set.apply(lambda x: dummy_province_(x['Province_1_New Brunswick'], x['Province_2_New Brunswick']), axis = 1)\n",
    "df_data_set.dummy_Newfoundland_and_Labrador = df_data_set.apply(lambda x: dummy_province_(x['Province_1_Newfoundland and Labrador'], x['Province_2_Newfoundland and Labrador']), axis = 1)\n",
    "\n",
    "df_data_set.dummy_Quebec.fillna(0, inplace = True)\n",
    "df_data_set.dummy_Ontario.fillna(0, inplace = True)\n",
    "df_data_set.dummy_British_Columbia.fillna(0, inplace = True)\n",
    "df_data_set.dummy_Alberta.fillna(0, inplace = True)\n",
    "df_data_set.dummy_Manitoba.fillna(0, inplace = True)\n",
    "df_data_set.dummy_Nova_Scotia.fillna(0, inplace = True)\n",
    "df_data_set.dummy_Saskatchewan.fillna(0, inplace = True)\n",
    "df_data_set.dummy_New_Brunswick.fillna(0, inplace = True)\n",
    "df_data_set.dummy_Newfoundland_and_Labrador.fillna(0, inplace = True)\n",
    "\n",
    "df_data_set['Log_TENB'] = df_data_set.TENB.apply(lambda x :math.log1p(x))\n",
    "df_data_set['Log_Geo_Dist X Log_TENB'] = df_data_set['Log_Geo_Dist'] * df_data_set['Log_TENB']\n",
    "df_data_set['Log_Geo_Dist_Sq'] = df_data_set['Log_Geo_Dist'] * df_data_set['Log_Geo_Dist']\n",
    "df_data_set['Log_Geo_Dist_Sq X Log_TENB'] = df_data_set['Log_Geo_Dist_Sq'] * df_data_set['Log_TENB']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c65274a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Author_1_2', 'Author_1', 'Author_2', 'number_of_collaborations',\n",
       "       'collaboration_binary', 'TENB', 'Cog_Dist', 'Geo_Dist', 'Prov_Border',\n",
       "       'NotContig', 'Province_1_British Columbia', 'Province_1_Manitoba',\n",
       "       'Province_1_Ontario', 'Province_1_Quebec',\n",
       "       'Province_2_British Columbia', 'Province_2_Manitoba',\n",
       "       'Province_2_Ontario', 'Province_2_Quebec', 'Province_1_Alberta',\n",
       "       'Province_1_Nova Scotia', 'Province_2_Alberta',\n",
       "       'Province_2_Nova Scotia', 'Province_1_Saskatchewan',\n",
       "       'Province_2_Saskatchewan', 'Province_1_New Brunswick',\n",
       "       'Province_2_New Brunswick', 'Province_1_Newfoundland and Labrador',\n",
       "       'Province_2_Newfoundland and Labrador', 'Top_regions', 'dummy_Quebec',\n",
       "       'dummy_Ontario', 'dummy_British_Columbia', 'dummy_Alberta',\n",
       "       'dummy_Manitoba', 'dummy_Nova_Scotia', 'dummy_Saskatchewan',\n",
       "       'dummy_New_Brunswick', 'dummy_Newfoundland_and_Labrador',\n",
       "       'Log_Geo_Dist', 'Log_TENB', 'Log_Geo_Dist X Log_TENB',\n",
       "       'Log_Geo_Dist_Sq', 'Log_Geo_Dist_Sq X Log_TENB'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_set.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d431e698",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.035189\n",
      "         Iterations 11\n",
      "                                   Results: Logit\n",
      "=====================================================================================\n",
      "Model:                    Logit                     Pseudo R-squared:      0.395     \n",
      "Dependent Variable:       collaboration_binary      AIC:                   31599.8319\n",
      "Date:                     2021-08-18 12:41          BIC:                   31820.1022\n",
      "No. Observations:         448434                    Log-Likelihood:        -15780.   \n",
      "Df Model:                 19                        LL-Null:               -26085.   \n",
      "Df Residuals:             448414                    LLR p-value:           0.0000    \n",
      "Converged:                1.0000                    Scale:                 1.0000    \n",
      "No. Iterations:           11.0000                                                    \n",
      "-------------------------------------------------------------------------------------\n",
      "                                      Coef.  Std.Err.    z     P>|z|   [0.025  0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "Log_Geo_Dist                         -0.8507   0.0140 -60.8591 0.0000 -0.8781 -0.8233\n",
      "Prov_Border                          -0.6742   0.1026  -6.5709 0.0000 -0.8753 -0.4731\n",
      "NotContig                            -0.9417   0.0366 -25.6972 0.0000 -1.0135 -0.8698\n",
      "Top_regions                           2.9204   0.3444   8.4783 0.0000  2.2453  3.5955\n",
      "Province_1_British Columbia          -0.3889   0.1632  -2.3830 0.0172 -0.7088 -0.0690\n",
      "Province_1_Manitoba                   0.8613   0.2466   3.4925 0.0005  0.3779  1.3446\n",
      "Province_1_Ontario                    0.2266   0.1855   1.2220 0.2217 -0.1369  0.5901\n",
      "Province_1_Quebec                     0.1254   0.1869   0.6713 0.5020 -0.2408  0.4917\n",
      "Province_2_British Columbia           0.2395   0.1615   1.4831 0.1380 -0.0770  0.5560\n",
      "Province_2_Manitoba                   0.4523   0.2404   1.8816 0.0599 -0.0188  0.9234\n",
      "Province_2_Ontario                   -0.9615   0.1825  -5.2686 0.0000 -1.3192 -0.6038\n",
      "Province_2_Quebec                    -0.8524   0.1831  -4.6548 0.0000 -1.2113 -0.4935\n",
      "Province_1_Nova Scotia                0.3875   0.3330   1.1637 0.2446 -0.2651  1.0400\n",
      "Province_2_Nova Scotia               -0.7308   0.3294  -2.2187 0.0265 -1.3764 -0.0852\n",
      "Province_1_Saskatchewan               0.2750   0.2009   1.3685 0.1712 -0.1188  0.6688\n",
      "Province_2_Saskatchewan               0.4286   0.1982   2.1624 0.0306  0.0401  0.8170\n",
      "Province_1_New Brunswick              1.3867   0.2894   4.7909 0.0000  0.8194  1.9540\n",
      "Province_2_New Brunswick             -0.3135   0.3226  -0.9719 0.3311 -0.9457  0.3187\n",
      "Province_1_Newfoundland and Labrador  1.3989   0.3709   3.7714 0.0002  0.6719  2.1259\n",
      "Province_2_Newfoundland and Labrador -0.9695   0.3998  -2.4247 0.0153 -1.7532 -0.1858\n",
      "=====================================================================================\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.035090\n",
      "         Iterations 11\n",
      "                                   Results: Logit\n",
      "=====================================================================================\n",
      "Model:                    Logit                     Pseudo R-squared:      0.397     \n",
      "Dependent Variable:       collaboration_binary      AIC:                   31508.9219\n",
      "Date:                     2021-08-18 12:41          BIC:                   31718.1787\n",
      "No. Observations:         448434                    Log-Likelihood:        -15735.   \n",
      "Df Model:                 18                        LL-Null:               -26085.   \n",
      "Df Residuals:             448415                    LLR p-value:           0.0000    \n",
      "Converged:                1.0000                    Scale:                 1.0000    \n",
      "No. Iterations:           11.0000                                                    \n",
      "-------------------------------------------------------------------------------------\n",
      "                                      Coef.  Std.Err.    z     P>|z|   [0.025  0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "Log_TENB                              4.6184   0.6839   6.7535 0.0000  3.2781  5.9588\n",
      "Log_Geo_Dist                         -0.8481   0.0139 -60.7979 0.0000 -0.8754 -0.8207\n",
      "NotContig                            -0.9656   0.0366 -26.3662 0.0000 -1.0374 -0.8938\n",
      "Prov_Border                          -0.6985   0.1024  -6.8215 0.0000 -0.8992 -0.4978\n",
      "Top_regions                           2.3227   0.3125   7.4323 0.0000  1.7102  2.9352\n",
      "Province_1_Ontario                    0.0099   0.1593   0.0619 0.9507 -0.3024  0.3221\n",
      "Province_1_Quebec                    -0.0795   0.1625  -0.4892 0.6247 -0.3980  0.2390\n",
      "Province_2_British Columbia          -0.1021   0.0565  -1.8082 0.0706 -0.2128  0.0086\n",
      "Province_2_Manitoba                   1.1536   0.1098  10.5102 0.0000  0.9385  1.3687\n",
      "Province_2_Ontario                   -0.7258   0.1601  -4.5348 0.0000 -1.0395 -0.4121\n",
      "Province_2_Quebec                    -0.6315   0.1620  -3.8981 0.0001 -0.9491 -0.3140\n",
      "Province_1_Nova Scotia                0.2044   0.3222   0.6342 0.5259 -0.4272  0.8359\n",
      "Province_2_Nova Scotia               -0.5239   0.3203  -1.6353 0.1020 -1.1517  0.1040\n",
      "Province_1_Saskatchewan               0.1567   0.1911   0.8199 0.4123 -0.2178  0.5311\n",
      "Province_2_Saskatchewan               0.5695   0.1899   2.9984 0.0027  0.1972  0.9417\n",
      "Province_1_New Brunswick              1.1804   0.2748   4.2958 0.0000  0.6419  1.7190\n",
      "Province_2_New Brunswick             -0.0894   0.3107  -0.2878 0.7735 -0.6985  0.5196\n",
      "Province_1_Newfoundland and Labrador  1.2264   0.3619   3.3886 0.0007  0.5171  1.9357\n",
      "Province_2_Newfoundland and Labrador -0.7730   0.3930  -1.9670 0.0492 -1.5432 -0.0028\n",
      "=====================================================================================\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.035088\n",
      "         Iterations 11\n",
      "                                   Results: Logit\n",
      "=====================================================================================\n",
      "Model:                    Logit                     Pseudo R-squared:      0.397     \n",
      "Dependent Variable:       collaboration_binary      AIC:                   31509.7489\n",
      "Date:                     2021-08-18 12:42          BIC:                   31730.0192\n",
      "No. Observations:         448434                    Log-Likelihood:        -15735.   \n",
      "Df Model:                 19                        LL-Null:               -26085.   \n",
      "Df Residuals:             448414                    LLR p-value:           0.0000    \n",
      "Converged:                1.0000                    Scale:                 1.0000    \n",
      "No. Iterations:           11.0000                                                    \n",
      "-------------------------------------------------------------------------------------\n",
      "                                      Coef.  Std.Err.    z     P>|z|   [0.025  0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "Log_TENB                              4.1373   0.7253   5.7043 0.0000  2.7157  5.5588\n",
      "Log_Geo_Dist                         -0.8483   0.0140 -60.8060 0.0000 -0.8756 -0.8209\n",
      "Log_Geo_Dist X Log_TENB               0.2132   0.1788   1.1924 0.2331 -0.1373  0.5637\n",
      "NotContig                            -0.9659   0.0366 -26.3702 0.0000 -1.0377 -0.8941\n",
      "Prov_Border                          -0.6980   0.1024  -6.8172 0.0000 -0.8987 -0.4973\n",
      "Top_regions                           2.3129   0.3126   7.3986 0.0000  1.7002  2.9256\n",
      "Province_1_Ontario                   -0.0007   0.1598  -0.0045 0.9964 -0.3139  0.3124\n",
      "Province_1_Quebec                    -0.0910   0.1630  -0.5584 0.5766 -0.4105  0.2285\n",
      "Province_2_British Columbia          -0.1015   0.0565  -1.7971 0.0723 -0.2122  0.0092\n",
      "Province_2_Manitoba                   1.1572   0.1098  10.5392 0.0000  0.9420  1.3724\n",
      "Province_2_Ontario                   -0.7145   0.1606  -4.4494 0.0000 -1.0293 -0.3998\n",
      "Province_2_Quebec                    -0.6193   0.1626  -3.8094 0.0001 -0.9380 -0.3007\n",
      "Province_1_Nova Scotia                0.1884   0.3230   0.5831 0.5598 -0.4448  0.8215\n",
      "Province_2_Nova Scotia               -0.5073   0.3212  -1.5794 0.1143 -1.1368  0.1222\n",
      "Province_1_Saskatchewan               0.1537   0.1912   0.8041 0.4213 -0.2210  0.5284\n",
      "Province_2_Saskatchewan               0.5729   0.1900   3.0146 0.0026  0.2004  0.9453\n",
      "Province_1_New Brunswick              1.1405   0.2782   4.1002 0.0000  0.5953  1.6857\n",
      "Province_2_New Brunswick             -0.0557   0.3120  -0.1785 0.8583 -0.6671  0.5557\n",
      "Province_1_Newfoundland and Labrador  1.2168   0.3622   3.3592 0.0008  0.5068  1.9268\n",
      "Province_2_Newfoundland and Labrador -0.7629   0.3933  -1.9396 0.0524 -1.5339  0.0080\n",
      "=====================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.035076\n",
      "         Iterations 11\n",
      "                                   Results: Logit\n",
      "=====================================================================================\n",
      "Model:                    Logit                     Pseudo R-squared:      0.397     \n",
      "Dependent Variable:       collaboration_binary      AIC:                   31500.8800\n",
      "Date:                     2021-08-18 12:42          BIC:                   31732.1638\n",
      "No. Observations:         448434                    Log-Likelihood:        -15729.   \n",
      "Df Model:                 20                        LL-Null:               -26085.   \n",
      "Df Residuals:             448413                    LLR p-value:           0.0000    \n",
      "Converged:                1.0000                    Scale:                 1.0000    \n",
      "No. Iterations:           11.0000                                                    \n",
      "-------------------------------------------------------------------------------------\n",
      "                                      Coef.  Std.Err.    z     P>|z|   [0.025  0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "Log_TENB                              3.0513   0.6867   4.4437 0.0000  1.7055  4.3971\n",
      "Log_Geo_Dist                         -0.8496   0.0140 -60.7906 0.0000 -0.8770 -0.8222\n",
      "Log_Geo_Dist X Log_TENB               3.9160   1.4436   2.7126 0.0067  1.0865  6.7454\n",
      "Log_Geo_Dist_Sq X Log_TENB           -0.4755   0.1978  -2.4047 0.0162 -0.8631 -0.0880\n",
      "NotContig                            -0.9628   0.0366 -26.2968 0.0000 -1.0346 -0.8910\n",
      "Prov_Border                          -0.6883   0.1025  -6.7155 0.0000 -0.8892 -0.4874\n",
      "Top_regions                           2.3266   0.3126   7.4420 0.0000  1.7139  2.9394\n",
      "Province_1_Ontario                    0.0126   0.1595   0.0789 0.9371 -0.3000  0.3251\n",
      "Province_1_Quebec                    -0.0767   0.1627  -0.4713 0.6374 -0.3956  0.2422\n",
      "Province_2_British Columbia          -0.1037   0.0565  -1.8358 0.0664 -0.2144  0.0070\n",
      "Province_2_Manitoba                   1.1551   0.1096  10.5392 0.0000  0.9403  1.3699\n",
      "Province_2_Ontario                   -0.7302   0.1602  -4.5570 0.0000 -1.0443 -0.4161\n",
      "Province_2_Quebec                    -0.6369   0.1622  -3.9260 0.0001 -0.9549 -0.3190\n",
      "Province_1_Nova Scotia                0.2089   0.3223   0.6480 0.5170 -0.4229  0.8406\n",
      "Province_2_Nova Scotia               -0.5308   0.3205  -1.6562 0.0977 -1.1588  0.0973\n",
      "Province_1_Saskatchewan               0.1562   0.1911   0.8172 0.4138 -0.2184  0.5307\n",
      "Province_2_Saskatchewan               0.5676   0.1900   2.9882 0.0028  0.1953  0.9399\n",
      "Province_1_New Brunswick              1.1941   0.2761   4.3244 0.0000  0.6529  1.7353\n",
      "Province_2_New Brunswick             -0.1022   0.3120  -0.3275 0.7433 -0.7137  0.5093\n",
      "Province_1_Newfoundland and Labrador  1.2302   0.3621   3.3970 0.0007  0.5204  1.9399\n",
      "Province_2_Newfoundland and Labrador -0.7791   0.3932  -1.9814 0.0475 -1.5498 -0.0084\n",
      "=====================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logit regression\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "ros = RandomOverSampler()\n",
    "scl = StandardScaler()\n",
    "smt = SMOTE()\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "\n",
    "X1 = df_data_set[['Log_Geo_Dist', 'Prov_Border', 'NotContig','Top_regions','Province_1_British Columbia', 'Province_1_Manitoba',\n",
    "       'Province_1_Ontario', 'Province_1_Quebec',\n",
    "       'Province_2_British Columbia', 'Province_2_Manitoba',\n",
    "       'Province_2_Ontario', 'Province_2_Quebec',\n",
    "       'Province_1_Nova Scotia',\n",
    "       'Province_2_Nova Scotia', 'Province_1_Saskatchewan',\n",
    "       'Province_2_Saskatchewan', 'Province_1_New Brunswick',\n",
    "       'Province_2_New Brunswick', 'Province_1_Newfoundland and Labrador',\n",
    "       'Province_2_Newfoundland and Labrador']]\n",
    "\n",
    "X2 = df_data_set[['Log_TENB', 'Log_Geo_Dist', 'NotContig', 'Prov_Border', 'Top_regions',\n",
    "       'Province_1_Ontario', 'Province_1_Quebec',\n",
    "       'Province_2_British Columbia', 'Province_2_Manitoba',\n",
    "       'Province_2_Ontario', 'Province_2_Quebec',\n",
    "       'Province_1_Nova Scotia',\n",
    "       'Province_2_Nova Scotia', 'Province_1_Saskatchewan',\n",
    "       'Province_2_Saskatchewan', 'Province_1_New Brunswick',\n",
    "       'Province_2_New Brunswick', 'Province_1_Newfoundland and Labrador',\n",
    "       'Province_2_Newfoundland and Labrador']]\n",
    "\n",
    "X3 = df_data_set[['Log_TENB','Log_Geo_Dist', 'Log_Geo_Dist X Log_TENB','NotContig', 'Prov_Border', 'Top_regions',\n",
    "       'Province_1_Ontario', 'Province_1_Quebec',\n",
    "       'Province_2_British Columbia', 'Province_2_Manitoba',\n",
    "       'Province_2_Ontario', 'Province_2_Quebec',\n",
    "       'Province_1_Nova Scotia',\n",
    "       'Province_2_Nova Scotia', 'Province_1_Saskatchewan',\n",
    "       'Province_2_Saskatchewan', 'Province_1_New Brunswick',\n",
    "       'Province_2_New Brunswick', 'Province_1_Newfoundland and Labrador',\n",
    "       'Province_2_Newfoundland and Labrador']]\n",
    "\n",
    "X4 = df_data_set[['Log_TENB','Log_Geo_Dist', 'Log_Geo_Dist X Log_TENB', 'Log_Geo_Dist_Sq X Log_TENB','NotContig', 'Prov_Border', 'Top_regions',\n",
    "       'Province_1_Ontario', 'Province_1_Quebec',\n",
    "       'Province_2_British Columbia', 'Province_2_Manitoba',\n",
    "       'Province_2_Ontario', 'Province_2_Quebec',\n",
    "       'Province_1_Nova Scotia',\n",
    "       'Province_2_Nova Scotia', 'Province_1_Saskatchewan',\n",
    "       'Province_2_Saskatchewan', 'Province_1_New Brunswick',\n",
    "       'Province_2_New Brunswick', 'Province_1_Newfoundland and Labrador',\n",
    "       'Province_2_Newfoundland and Labrador']]\n",
    "\n",
    "\n",
    "\n",
    "# X1 = sm.tools.tools.add_constant(X1, prepend=True, has_constant='add')\n",
    "# X2 = sm.tools.tools.add_constant(X2, prepend=True, has_constant='add')\n",
    "# X3 = sm.tools.tools.add_constant(X3, prepend=True, has_constant='add')\n",
    "# X4 = sm.tools.tools.add_constant(X4, prepend=True, has_constant='add')\n",
    "\n",
    "# X1_1 = sm.tools.tools.add_constant(X1_1, prepend=True, has_constant='add')\n",
    "# X1_2 = sm.tools.tools.add_constant(X1_2, prepend=True, has_constant='add')\n",
    "# X1_3 = sm.tools.tools.add_constant(X1_3, prepend=True, has_constant='add')\n",
    "\n",
    "# X_old = sm.tools.tools.add_constant(X_old, prepend=True, has_constant='add')\n",
    "\n",
    "X = [X1, X2, X3, X4]\n",
    "\n",
    "y = df_data_set['collaboration_binary']\n",
    "\n",
    "for X in X:\n",
    "    logit_model=sm.Logit(y, X)\n",
    "    result=logit_model.fit(method_kwargs={\"warn_convergence\": False})\n",
    "    print(result.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21b1569e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset {0: 443737, 1: 4697}\n",
      "Dataset Test {0: 44374, 1: 470}\n",
      "F1_Score: 0.3311965811965812\n",
      "Presicion: 0.1988879384088965\n",
      "Recall: 0.9893617021276596\n",
      "Accuracy: 0.9581214878244582\n",
      "confusion_matrix: [[42501  1873]\n",
      " [    5   465]]\n",
      "F1_Score: 0.5213004484304932\n",
      "Presicion: 0.3538812785388128\n",
      "Recall: 0.9893617021276596\n",
      "Accuracy: 0.9809562037284809\n",
      "confusion_matrix: [[43525   849]\n",
      " [    5   465]]\n"
     ]
    }
   ],
   "source": [
    "# Classification\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_score, GridSearchCV, RandomizedSearchCV, KFold, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline as Pipeline, make_pipeline as make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.utils import resample\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import NearMiss, RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as Imbpipeline\n",
    "from imblearn.pipeline import make_pipeline as Imb_make_pipeline\n",
    "\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# X1 = df_data_set[['Log_Geo_Dist', 'NotContig', 'Prov_Border', 'Top_regions',\n",
    "#                 'dummy_Quebec', 'dummy_Ontario', 'dummy_British_Columbia', 'dummy_Alberta', 'dummy_Manitoba', 'dummy_Nova_Scotia', 'dummy_Saskatchewan', 'dummy_New_Brunswick']]\n",
    "\n",
    "X2 = df_data_set[['Log_TENB', 'Log_Geo_Dist', 'NotContig', 'Prov_Border', 'Top_regions',\n",
    "                'dummy_Quebec', 'dummy_Ontario', 'dummy_British_Columbia', 'dummy_Alberta', 'dummy_Manitoba', 'dummy_Nova_Scotia', 'dummy_Saskatchewan', 'dummy_New_Brunswick']]\n",
    "\n",
    "# X3 = df_data_set[['Log_TENB','Log_Geo_Dist', 'Log_Geo_Dist X Log_TENB','NotContig', 'Prov_Border', 'Top_regions',\n",
    "#                 'dummy_Quebec', 'dummy_Ontario', 'dummy_British_Columbia', 'dummy_Alberta', 'dummy_Manitoba', 'dummy_Nova_Scotia', 'dummy_Saskatchewan', 'dummy_New_Brunswick']]\n",
    "\n",
    "X4 = df_data_set[['Log_TENB','Log_Geo_Dist', 'Log_Geo_Dist X Log_TENB', 'Log_Geo_Dist_Sq X Log_TENB','NotContig', 'Prov_Border', 'Top_regions',\n",
    "                'dummy_Quebec', 'dummy_Ontario', 'dummy_British_Columbia', 'dummy_Alberta', 'dummy_Manitoba', 'dummy_Nova_Scotia', 'dummy_Saskatchewan', 'dummy_New_Brunswick']]\n",
    "\n",
    "\n",
    "y = df_data_set['collaboration_binary']\n",
    "\n",
    "unique, count = np.unique (y, return_counts = True)\n",
    "\n",
    "y_value_count = {k : v for (k,v) in zip(unique,count)}\n",
    "\n",
    "print ('Dataset', y_value_count)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X2, y, test_size = 0.1, random_state = 123, stratify = y)\n",
    "\n",
    "unique, count = np.unique (y_test, return_counts = True)\n",
    "\n",
    "y_value_count = {k : v for (k,v) in zip(unique,count)}\n",
    "\n",
    "print ('Dataset Test', y_value_count)\n",
    "\n",
    "logreg = LogisticRegression(max_iter = 500)\n",
    "rnd = RandomForestClassifier(random_state = 42, n_jobs = 6)\n",
    "gbc = GradientBoostingClassifier()\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric = 'logloss')\n",
    "knn = KNeighborsClassifier()\n",
    "rus = RandomUnderSampler()\n",
    "ros = RandomOverSampler()\n",
    "sm = SMOTE()\n",
    "smtk = SMOTETomek(n_jobs = 6)\n",
    "smnn = SMOTEENN(n_jobs = 6)\n",
    "scl = StandardScaler()\n",
    "feature_selection_selbest = SelectKBest(chi2, k=7)\n",
    "feature_selection_selmodel = SelectFromModel(logreg)\n",
    "\n",
    "pipe1 = Pipeline(steps = [['Classifier', xgb]])\n",
    "\n",
    "pipe2 = Pipeline(steps = [['standard_scalar', scl], ['Classifier', xgb]])\n",
    "\n",
    "pipe3 = Imbpipeline(steps = [['random_over_sampler', ros], ['Classifier', xgb]])\n",
    "\n",
    "pipe4 = Imbpipeline(steps = [['random_over_sampler', ros], ['Classifier', rnd]])\n",
    "\n",
    "\n",
    "pipes = [pipe3, pipe4]\n",
    "\n",
    "for pipe in pipes:\n",
    "    model = pipe\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print ('F1_Score:', f1_score(y_test, y_pred))\n",
    "    print ('Presicion:' , precision_score(y_test, y_pred))\n",
    "    print ('Recall:' , recall_score(y_test, y_pred))\n",
    "    print ('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "    print ('confusion_matrix:', confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c95650fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.34798187e-04, 6.79578454e-01, 2.07308768e-02, 2.28235907e-01,\n",
       "       2.59880605e-03, 1.54763040e-02, 2.63097262e-02, 7.48756477e-03,\n",
       "       9.25433338e-03, 2.07101309e-03, 3.30648760e-03, 2.78575893e-03,\n",
       "       1.52997080e-03])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd.feature_importances_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
