{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hired-richards",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    div#notebook-container    { width: 95%; }\n",
       "    div#menubar-container     { width: 85%; }\n",
       "    div#maintoolbar-container { width: 99%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 85%; }\n",
    "    div#maintoolbar-container { width: 99%; }\n",
    "</style>\n",
    "\"\"\"))\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "from scipy.stats import uniform\n",
    "import statsmodels.api as sm\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_score, cross_val_predict, GridSearchCV, RandomizedSearchCV, KFold, StratifiedKFold, validation_curve\n",
    "from sklearn.pipeline import Pipeline as Pipeline, make_pipeline as make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.utils import resample\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler, SMOTENC\n",
    "from imblearn.under_sampling import NearMiss, RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as Imbpipeline\n",
    "from imblearn.pipeline import make_pipeline as Imb_make_pipeline\n",
    "\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "df_data_set_2010 = pd.read_csv(r'C:\\Users\\moham\\Dropbox\\QSE\\Thesis\\Geopattern\\My data\\df_data_set_Europe_North_America_2003_2010.csv')\n",
    "df_data_set_2015 = pd.read_csv(r'C:\\Users\\moham\\Dropbox\\QSE\\Thesis\\Geopattern\\My data\\df_data_set_Europe_North_America_2011_2015.csv')\n",
    "df_data_set_2017 = pd.read_csv(r'C:\\Users\\moham\\Dropbox\\QSE\\Thesis\\Geopattern\\My data\\df_data_set_Europe_North_America_2016_2017.csv')\n",
    "df_data_set_2018 = pd.read_csv(r'C:\\Users\\moham\\Dropbox\\QSE\\Thesis\\Geopattern\\My data\\df_data_set_Europe_North_America_2018.csv')\n",
    "\n",
    "frames = [df_data_set_2010, \n",
    "          df_data_set_2015, \n",
    "          df_data_set_2017, \n",
    "          df_data_set_2018]\n",
    "\n",
    "df_data_set = pd.concat(frames)\n",
    "\n",
    "df_data_set['Log_TENB'] = df_data_set.TENB.apply(lambda x :math.log1p(x))\n",
    "df_data_set['Sq_Log_Geo_Dist'] = df_data_set['Log_Geo_Dist'] * df_data_set['Log_Geo_Dist']\n",
    "df_data_set['Log_Geo_Dist X Log_TENB'] = df_data_set['Log_Geo_Dist'] * df_data_set['Log_TENB']\n",
    "df_data_set['Sq_Log_Geo_Dist X Log_TENB'] = df_data_set['Sq_Log_Geo_Dist'] * df_data_set['Log_TENB']\n",
    "df_data_set['Sq_Geo_Dist'] = df_data_set['Geo_Dist'] * df_data_set['Geo_Dist']\n",
    "df_data_set['Log_Sq_Geo_Dist'] = df_data_set.Sq_Geo_Dist.apply(lambda x :math.log1p(x))\n",
    "df_data_set['Log_Sq_Geo_Dist X Log_TENB'] = df_data_set['Log_Sq_Geo_Dist'] * df_data_set['Log_TENB']\n",
    "df_data_set['Diff_Country X Log_TENB'] = df_data_set['Diff_Country'] * df_data_set['Log_TENB']\n",
    "df_data_set['Diff_Continent X Log_TENB'] = df_data_set['Diff_Continent'] * df_data_set['Log_TENB']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a45cb54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [  'TENB',\n",
    "             'Geo_Dist',\n",
    "             'Cog_Dist',\n",
    "             'Top_regions',\n",
    "             'Diff_Country', \n",
    "             'Diff_Continent',\n",
    "             'Not_Contig',\n",
    "             'Log_Geo_Dist',\n",
    "             'Log_TENB',\n",
    "             'Sq_Log_Geo_Dist',\n",
    "             'Log_Sq_Geo_Dist',\n",
    "             'Log_Geo_Dist X Log_TENB',\n",
    "             'Diff_Country X Log_TENB',\n",
    "             'Diff_Continent X Log_TENB',\n",
    "             'Sq_Log_Geo_Dist X Log_TENB', \n",
    "             'Log_Sq_Geo_Dist X Log_TENB',\n",
    "             'Sq_Geo_Dist']\n",
    "\n",
    "regional_dummies = [ 'Country_1_Canada', 'Country_1_France', 'Country_1_Germany',\n",
    "       'Country_1_Greece', 'Country_1_Ireland', 'Country_1_Italy',\n",
    "       'Country_1_Portugal', 'Country_1_Spain', 'Country_1_Switzerland',\n",
    "       'Country_1_United Kingdom', 'Country_1_United States',\n",
    "       'Country_2_Canada', 'Country_2_France', 'Country_2_Germany',\n",
    "       'Country_2_Greece', 'Country_2_Ireland', 'Country_2_Italy',\n",
    "       'Country_2_Portugal', 'Country_2_Spain', 'Country_2_Switzerland',\n",
    "       'Country_2_United Kingdom', 'Country_2_United States',\n",
    "       'Country_1_Austria', 'Country_1_Netherlands', 'Country_1_Russia',\n",
    "       'Country_1_Slovenia', 'Country_1_Sweden', 'Country_2_Austria',\n",
    "       'Country_2_Netherlands', 'Country_2_Russia', 'Country_2_Slovenia',\n",
    "       'Country_2_Sweden', 'Country_1_Denmark', 'Country_2_Denmark',\n",
    "       'Country_1_Belgium', 'Country_1_Finland', 'Country_1_Norway',\n",
    "       'Country_2_Belgium', 'Country_2_Finland', 'Country_2_Norway',\n",
    "       'Country_1_Croatia', 'Country_1_Hungary', 'Country_1_Poland',\n",
    "       'Country_2_Croatia', 'Country_2_Hungary', 'Country_2_Poland',\n",
    "       'Country_1_Cyprus', 'Country_1_Slovakia', 'Country_2_Cyprus',\n",
    "       'Country_2_Slovakia','Country_1_Romania', 'Country_1_Serbia', 'Country_2_Romania',\n",
    "       'Country_2_Serbia', 'Country_1_Luxembourg', 'Country_2_Luxembourg',\n",
    "       'Country_1_Iceland', 'Country_2_Iceland', 'Country_1_Estonia',\n",
    "       'Country_2_Estonia', 'Country_1_Bulgaria', 'Country_2_Bulgaria',\n",
    "       'Country_1_Lithuania', 'Country_2_Lithuania',\n",
    "       'Country_1_Bosnia and Herzegovina', 'Country_1_Ukraine',\n",
    "       'Country_2_Bosnia and Herzegovina', 'Country_2_Ukraine']\n",
    "\n",
    "\n",
    "features = [  'Log_Geo_Dist',\n",
    "              'Log_TENB',\n",
    "              'Cog_Dist',\n",
    "              'Diff_Country', \n",
    "              'Diff_Continent',\n",
    "              'Not_Contig',\n",
    "              'Top_regions']\n",
    "target = ['collaboration_binary']\n",
    "\n",
    "X_clf = df_data_set[features]\n",
    "y = df_data_set.collaboration_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "17146884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002283473915461475"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_set[features+target].collaboration_binary.value_counts()\n",
    "\n",
    "5116/2240446"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd9ceea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset {0: 2240446, 1: 5116}\n",
      "Dataset Test {0: 336068, 1: 767}\n"
     ]
    }
   ],
   "source": [
    "unique, count = np.unique (y, return_counts = True)\n",
    "\n",
    "y_value_count = {k : v for (k,v) in zip(unique,count)}\n",
    "\n",
    "print ('Dataset', y_value_count)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_clf, y, test_size = 0.15, random_state = 123, stratify = y)\n",
    "\n",
    "unique, count = np.unique (y_test, return_counts = True)\n",
    "\n",
    "y_value_count = {k : v for (k,v) in zip(unique,count)}\n",
    "\n",
    "print ('Dataset Test', y_value_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-tyler",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(max_iter = 500)\n",
    "rnd = RandomForestClassifier(random_state = 123, n_jobs = -1)\n",
    "gbc = GradientBoostingClassifier(random_state = 123)\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric = 'logloss', random_state = 123, n_jobs = -1)\n",
    "knn = KNeighborsClassifier(n_jobs = -1)\n",
    "rus = RandomUnderSampler()\n",
    "ros = RandomOverSampler()\n",
    "smt = SMOTENC(categorical_features = [3,4,5,6], random_state = 123, n_jobs = -1)\n",
    "smtk = SMOTETomek(n_jobs = 6)\n",
    "smnn = SMOTEENN(n_jobs = 6)\n",
    "scl = StandardScaler()\n",
    "feature_selection_selbest = SelectKBest(chi2, k=7)\n",
    "feature_selection_selmodel = SelectFromModel(rnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e0e3e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# baseline selection\n",
    "\n",
    "clfs = [logreg, rnd, gbc, xgb, knn]\n",
    "\n",
    "for clf in clfs:\n",
    "    \n",
    "    print (f'The result for {clf} is:')\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print ('F1_Score:', f1_score(y_test, y_pred))\n",
    "    print ('Presicion:' , precision_score(y_test, y_pred))\n",
    "    print ('Recall:' , recall_score(y_test, y_pred))\n",
    "    print ('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "    print ('roc_auc_score:', roc_auc_score(y_test, y_pred))\n",
    "    print ('confusion_matrix:')\n",
    "    print (confusion_matrix(y_test, y_pred))\n",
    "    print ('Classification Report:')\n",
    "    print (classification_report(y_test, y_pred))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2506d2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline (RandomForest)\n",
    "\n",
    "pipe_fs = Pipeline(steps = [['feature_selection', feature_selection_selmodel], ['RandomForestClassifier', rnd]])\n",
    "pipe_scl = Pipeline(steps = [['SCL', scl], ['RandomForestClassifier', rnd]])\n",
    "pipe_smt = Imbpipeline(steps = [['SMOTENC', smt], ['RandomForestClassifier', rnd]])\n",
    "pipe_ros = Imbpipeline(steps = [['RandomOverSampler', ros], ['RandomForestClassifier', rnd]])\n",
    "pipe_rus = Imbpipeline(steps = [['RandomUnderSampler', rus], ['RandomForestClassifier', rnd]])\n",
    "\n",
    "\n",
    "clfs = [rnd, pipe_fs, pipe_scl, pipe_smt, pipe_ros, pipe_rus]\n",
    "\n",
    "for clf in clfs:\n",
    "    \n",
    "    print (f'The result for {clf} is:')\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print ('F1_Score:', f1_score(y_test, y_pred))\n",
    "    print ('Presicion:' , precision_score(y_test, y_pred))\n",
    "    print ('Recall:' , recall_score(y_test, y_pred))\n",
    "    print ('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "    print ('roc_auc_score:', roc_auc_score(y_test, y_pred))\n",
    "    print ('confusion_matrix:')\n",
    "    print (confusion_matrix(y_test, y_pred))\n",
    "    print ('Classification Report:')\n",
    "    print (classification_report(y_test, y_pred))\n",
    "    print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57490d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning - Randomforestclassifier - RandomSearch\n",
    "\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start = 50, stop = 300, num = 26)]\n",
    "\n",
    "criterion = ['gini', 'entropy']\n",
    "\n",
    "max_features = ['auto', 'sqrt', .1, .2, .3, .4, .5]\n",
    "\n",
    "max_depth = [int(x) for x in np.linspace(40, 60, num = 20)]\n",
    "max_depth.append(None)\n",
    "\n",
    "min_samples_split = [2,3,4,5,6,7,8,9]\n",
    "\n",
    "min_samples_leaf = [1,2]\n",
    "\n",
    "bootstrap = [True, False]\n",
    "\n",
    "param_dist = { 'n_estimators': n_estimators,\n",
    "               'criterion': criterion,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "# print('Parameters used in the grid:\\n')\n",
    "# pprint(param_dist)\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 5)\n",
    "\n",
    "rnd_srch_model = RandomizedSearchCV(estimator = rnd,\n",
    "                           param_distributions = param_dist, \n",
    "                           cv = skf, n_jobs = -1, \n",
    "                           verbose = 1,\n",
    "                           n_iter = 200,\n",
    "                           scoring = 'f1',\n",
    "                           random_state = 123)\n",
    "\n",
    "rnd_srch_model.fit(X_train, y_train)\n",
    "# y_pred = rnd_srch_model.best_estimator_.predict(X_test)\n",
    "print ('Best Parameters', rnd_srch_model.best_params_)\n",
    "print ('Best Score', rnd_srch_model.best_score_)\n",
    "\n",
    "# print ('Presicion:' , precision_score(y_test, y_pred))\n",
    "# print ('Recall:' , recall_score(y_test, y_pred))\n",
    "# print ('F1_Score:', f1_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "# Best Parameters {'n_estimators': 170, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 55, 'criterion': 'gini', 'bootstrap': True}\n",
    "# Best Score 0.8852592313348533"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36af29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Search Best parameters result (test data)\n",
    "\n",
    "rnd_best = RandomForestClassifier(random_state = 123, n_jobs = -1,\n",
    "                                       n_estimators = 170,\n",
    "                                       min_samples_split = 5,\n",
    "                                       min_samples_leaf = 1,\n",
    "                                       max_features = 'sqrt',\n",
    "                                       criterion = 'gini',\n",
    "                                       max_depth = 55,\n",
    "                                       bootstrap = True)\n",
    "\n",
    "rnd_best.fit(X_train, y_train)\n",
    "y_pred_best = rnd_best.predict(X_test)\n",
    "print ('F1_Score:', f1_score(y_test, y_pred_best))\n",
    "print ('Presicion:' , precision_score(y_test, y_pred_best))\n",
    "print ('Recall:' , recall_score(y_test, y_pred_best))\n",
    "print ('Accuracy:', accuracy_score(y_test, y_pred_best))\n",
    "print ('confusion_matrix:')\n",
    "print (confusion_matrix(y_test, y_pred_best))\n",
    "print ('Classification Report:')\n",
    "print (classification_report(y_test, y_pred_best))\n",
    "\n",
    "# rfc_cv_score = cross_val_score(rnd_best, X_train, y_train, cv = skf, scoring = 'f1')\n",
    "\n",
    "# print ('F1_Score_cv_score:', rfc_cv_score.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bdce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning - Randomforestclassifier - Grid search\n",
    "\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start = 160, stop = 180, num = 3)]\n",
    "\n",
    "max_features = ['auto','sqrt']\n",
    "\n",
    "max_depth = [int(x) for x in np.linspace(53, 57, num = 5)]\n",
    "max_depth.append(None)\n",
    "\n",
    "min_samples_split = [4,5,6]\n",
    "\n",
    "min_samples_leaf = [1]\n",
    "\n",
    "bootstrap = [True, False]\n",
    "\n",
    "criterion = ['gini', 'entropy']\n",
    "\n",
    "\n",
    "param_grid = { 'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap,\n",
    "               'criterion' : criterion}\n",
    "\n",
    "print('Parameters used in the grid:\\n')\n",
    "pprint(param_grid)\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 5)\n",
    "\n",
    "grid_model = GridSearchCV(estimator = rnd,\n",
    "                         param_grid = param_grid, \n",
    "                         cv = skf, n_jobs = -1, \n",
    "                         verbose = 1,\n",
    "                         scoring = 'f1')\n",
    "\n",
    "grid_model.fit(X_train, y_train)\n",
    "\n",
    "print ('Best Parameters', grid_model.best_params_)\n",
    "print ('Best Score', grid_model.best_score_)\n",
    "\n",
    "\n",
    "# Best Parameters {'bootstrap': True, 'criterion': 'gini', 'max_depth': 53, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 170}\n",
    "# Best Score 0.8852592313348533"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4177b5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid Search Best parameters result (test data)\n",
    "\n",
    "grd_best = RandomForestClassifier(random_state = 123, n_jobs = -1,\n",
    "                                       criterion = 'gini',\n",
    "                                       n_estimators = 170,\n",
    "                                       min_samples_split = 5,\n",
    "                                       min_samples_leaf = 1,\n",
    "                                       max_features = 'auto',\n",
    "                                       max_depth = 53,\n",
    "                                       bootstrap = True)\n",
    "\n",
    "grd_best.fit(X_train, y_train)\n",
    "y_pred_best_grd = grd_best.predict(X_test)\n",
    "print ('F1_Score:', f1_score(y_test, y_pred_best_grd))\n",
    "print ('Presicion:' , precision_score(y_test, y_pred_best_grd))\n",
    "print ('Recall:' , recall_score(y_test, y_pred_best_grd))\n",
    "print ('Accuracy:', accuracy_score(y_test, y_pred_best_grd))\n",
    "print ('roc_auc_score:', roc_auc_score(y_test, y_pred_best_grd))\n",
    "print ('confusion_matrix:')\n",
    "print (confusion_matrix(y_test, y_pred_best_grd))\n",
    "print ('Classification Report:')\n",
    "print (classification_report(y_test, y_pred_best_grd))\n",
    "\n",
    "# rfc_cv_score = cross_val_score(rnd_best, X_train, y_train, cv = skf, scoring = 'f1')\n",
    "\n",
    "# print ('F1_Score_cv_score:', rfc_cv_score.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b873788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance - mean decrease in impurity\n",
    "\n",
    "importances = grd_best.feature_importances_ * 100\n",
    "\n",
    "\n",
    "df_importance = pd.DataFrame()\n",
    "\n",
    "df_importance.insert(0,'Feature','')\n",
    "df_importance.insert(1,'importance',0)\n",
    "\n",
    "j = 0\n",
    "for i in columns_clf:\n",
    "    df_importance.loc[j,'Feature'] = i\n",
    "    df_importance.loc[j,'importance'] = importances[j]\n",
    "    j += 1\n",
    "\n",
    "df_importance.sort_values(by=['importance'], ascending=False, inplace = True)\n",
    "\n",
    "df_importance[:10].plot(x = 'Feature', y = 'importance', kind = 'bar', rot = 90, ylabel = 'mean decrease in impurity (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a3988d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Logit regression\n",
    "\n",
    "\n",
    "columns_reg0 = [ 'Log_Geo_Dist',\n",
    "                 'Prov_Border',\n",
    "                 'Country_Border',\n",
    "                 'NotContig',\n",
    "                 'Top_regions']\n",
    "\n",
    "columns_reg1 = [ 'Log_Geo_Dist',\n",
    "                 'Log_TENB',\n",
    "                 'Cog_Dist',\n",
    "                 'Prov_Border',\n",
    "                 'Country_Border',\n",
    "                 'NotContig',\n",
    "                 'Top_regions']\n",
    "\n",
    "columns_reg2 = [ 'Log_Geo_Dist',\n",
    "                 'Log_TENB',\n",
    "                 'Log_Geo_Dist X Log_TENB', \n",
    "                 'Cog_Dist',\n",
    "                 'Prov_Border',\n",
    "                 'Country_Border',\n",
    "                 'NotContig',\n",
    "                 'Top_regions']\n",
    "\n",
    "\n",
    "#              'Prov_Border X Cog_Dist',\n",
    "#              'Country_Border X Log_TENB',\n",
    "#              'Country_Border X Cog_Dist',\n",
    "\n",
    "columns_reg21 = ['Log_Geo_Dist',\n",
    "                 'Log_TENB',\n",
    "                 'Prov_Border X Log_TENB',\n",
    "                 'Cog_Dist',\n",
    "                 'Prov_Border',\n",
    "                 'Country_Border',\n",
    "                 'NotContig',\n",
    "                 'Top_regions']\n",
    "\n",
    "columns_reg22 = ['Log_Geo_Dist',\n",
    "                 'Log_TENB',\n",
    "                 'Country_Border X Log_TENB',\n",
    "                 'Cog_Dist',\n",
    "                 'Prov_Border',\n",
    "                 'Country_Border',\n",
    "                 'NotContig',\n",
    "                 'Top_regions']\n",
    "\n",
    "columns_reg3 = [ 'Log_Geo_Dist',\n",
    "                 'Log_TENB',\n",
    "                 'Log_Geo_Dist X Cog_Dist',\n",
    "                 'Cog_Dist',\n",
    "                 'Prov_Border',\n",
    "                 'Country_Border',\n",
    "                 'NotContig']\n",
    "\n",
    "columns_reg4 = [ 'Log_Geo_Dist',\n",
    "                 'Log_TENB',\n",
    "                 'Cog_Dist',\n",
    "                 'Prov_Border',\n",
    "                 'Prov_Border X Log_TENB',\n",
    "                 'Country_Border',\n",
    "                 'NotContig']\n",
    "\n",
    "columns_reg5 = [ 'Log_Geo_Dist',\n",
    "                 'Log_TENB',\n",
    "                 'Cog_Dist',\n",
    "                 'Prov_Border',\n",
    "                 'Prov_Border X Cog_Dist',\n",
    "                 'Country_Border',\n",
    "                 'NotContig']\n",
    "\n",
    "columns_reg6 = [ 'Log_Geo_Dist',\n",
    "                 'Log_TENB',\n",
    "                 'Cog_Dist',\n",
    "                 'Prov_Border',\n",
    "                 'Country_Border',\n",
    "                 'Country_Border X Log_TENB',\n",
    "                 'NotContig']\n",
    "\n",
    "columns_reg7 = [ 'Log_Geo_Dist',\n",
    "                 'Log_TENB',\n",
    "                 'Cog_Dist',\n",
    "                 'Prov_Border',\n",
    "                 'Country_Border',\n",
    "                 'Country_Border X Cog_Dist',\n",
    "                 'NotContig']\n",
    "\n",
    "X_reg0 = df_data_set[columns_reg0]\n",
    "X_reg1 = df_data_set[columns_reg1]\n",
    "X_reg2 = df_data_set[columns_reg2]\n",
    "X_reg21 = df_data_set[columns_reg21]\n",
    "X_reg22 = df_data_set[columns_reg22]\n",
    "X_reg3 = df_data_set[columns_reg3]\n",
    "X_reg4 = df_data_set[columns_reg4]\n",
    "X_reg5 = df_data_set[columns_reg5]\n",
    "X_reg6 = df_data_set[columns_reg6]\n",
    "X_reg7 = df_data_set[columns_reg7]\n",
    "\n",
    "X_reg_0 = sm.tools.tools.add_constant(X_reg0, prepend=True, has_constant='add')\n",
    "X_reg_1 = sm.tools.tools.add_constant(X_reg1, prepend=True, has_constant='add')\n",
    "X_reg_2 = sm.tools.tools.add_constant(X_reg2, prepend=True, has_constant='add')\n",
    "X_reg_3 = sm.tools.tools.add_constant(X_reg3, prepend=True, has_constant='add')\n",
    "X_reg_4 = sm.tools.tools.add_constant(X_reg4, prepend=True, has_constant='add')\n",
    "X_reg_5 = sm.tools.tools.add_constant(X_reg5, prepend=True, has_constant='add')\n",
    "X_reg_6 = sm.tools.tools.add_constant(X_reg6, prepend=True, has_constant='add')\n",
    "X_reg_7 = sm.tools.tools.add_constant(X_reg7, prepend=True, has_constant='add')\n",
    "\n",
    "\n",
    "xx = [X_reg3, X_reg5, X_reg7]\n",
    "\n",
    "for xx in xx:\n",
    "    logit_model=sm.Logit(y, xx)\n",
    "    result=logit_model.fit(method_kwargs={\"warn_convergence\": False})\n",
    "    print(result.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6566bba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation curve - n_estimators\n",
    "\n",
    "param_range = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 3)\n",
    "\n",
    "train_scores, test_scores = validation_curve(rnd, X_train, y_train, \n",
    "                           param_name = 'n_estimators', \n",
    "                           param_range = param_range,\n",
    "                           cv = skf, n_jobs = -1, verbose = 30, scoring = 'f1')\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.title(\"Validation Curve with RandomForest\")\n",
    "plt.xlabel(r\"$\\gamma$\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(.85, 1.05)\n",
    "\n",
    "lw = 2\n",
    "plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "test_scores_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7567d640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation curve - max_depth\n",
    "\n",
    "max_depth = [int(x) for x in np.linspace(0, 150, num = 30)]\n",
    "max_depth.append(None)\n",
    "\n",
    "param_range = max_depth\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 3)\n",
    "\n",
    "train_scores, test_scores = validation_curve(rnd, X_train, y_train, \n",
    "                           param_name = 'max_depth', \n",
    "                           param_range = param_range,\n",
    "                           cv = skf, n_jobs = -1, verbose = 45, scoring = 'f1')\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.title(\"Validation Curve with RandomForest\")\n",
    "plt.xlabel(r\"$\\gamma$\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(.8, 1)\n",
    "\n",
    "lw = 2\n",
    "plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "# plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "#                  train_scores_mean + train_scores_std, alpha=0.2,\n",
    "#                  color=\"darkorange\", lw=lw)\n",
    "plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "# plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "#                  test_scores_mean + test_scores_std, alpha=0.2,\n",
    "#                  color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "test_scores_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc9ee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation curve - min_sample_split\n",
    "\n",
    "min_sample_split = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "param_range = min_sample_split\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 3)\n",
    "\n",
    "train_scores, test_scores = validation_curve(rnd, X_train, y_train, \n",
    "                           param_name = 'min_samples_split', \n",
    "                           param_range = param_range,\n",
    "                           cv = skf, n_jobs = -1, verbose = 30, scoring = 'f1')\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.title(\"Validation Curve with RandomForest\")\n",
    "plt.xlabel(r\"$\\gamma$\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(.8, 1)\n",
    "\n",
    "lw = 2\n",
    "plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "test_scores_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0c64f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation curve - min_sample_leaf\n",
    "\n",
    "min_sample_leaf = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "param_range = min_sample_leaf\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 3)\n",
    "\n",
    "train_scores, test_scores = validation_curve(rnd, X_train, y_train, \n",
    "                           param_name = 'min_samples_leaf', \n",
    "                           param_range = param_range,\n",
    "                           cv = skf, n_jobs = -1, verbose = 50, scoring = 'f1')\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.title(\"Validation Curve with RandomForest\")\n",
    "plt.xlabel(r\"$\\gamma$\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(.8, 1)\n",
    "\n",
    "lw = 2\n",
    "plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "test_scores_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e21cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation curve - bootstrap\n",
    "\n",
    "bootstrap = [True, False]\n",
    "\n",
    "param_range = bootstrap\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 3)\n",
    "\n",
    "train_scores, test_scores = validation_curve(rnd, X_train, y_train, \n",
    "                           param_name = 'bootstrap', \n",
    "                           param_range = param_range,\n",
    "                           cv = skf, n_jobs = -1, verbose = 6, scoring = 'f1')\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.title(\"Validation Curve with RandomForest\")\n",
    "plt.xlabel(r\"$\\gamma$\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(.8, 1)\n",
    "\n",
    "lw = 2\n",
    "plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "test_scores_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb60f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation curve - criterion\n",
    "\n",
    "criterion = ['gini', 'entropy']\n",
    "\n",
    "param_range = criterion\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 3)\n",
    "\n",
    "train_scores, test_scores = validation_curve(rnd, X_train, y_train, \n",
    "                           param_name = 'criterion', \n",
    "                           param_range = param_range,\n",
    "                           cv = skf, n_jobs = -1, verbose = 6, scoring = 'f1')\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.title(\"Validation Curve with RandomForest\")\n",
    "plt.xlabel(r\"$\\gamma$\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(.8, 1)\n",
    "\n",
    "lw = 2\n",
    "plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "test_scores_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f477825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation curve - max_features\n",
    "\n",
    "max_features = ['auto', 'sqrt', .1, .2, .3, .4, .5, .6, .7, .8, .9]\n",
    "\n",
    "param_range = max_features\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 3)\n",
    "\n",
    "train_scores, test_scores = validation_curve(rnd, X_train, y_train, \n",
    "                           param_name = 'max_features', \n",
    "                           param_range = param_range,\n",
    "                           cv = skf, n_jobs = -1, verbose = 6, scoring = 'f1')\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.title(\"Validation Curve with RandomForest\")\n",
    "plt.xlabel(r\"$\\gamma$\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(.8, 1)\n",
    "\n",
    "lw = 2\n",
    "plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "test_scores_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8f7be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "\n",
    "print ('Original data size: ', X.shape)\n",
    "\n",
    "var = VarianceThreshold(threshold=(.9 * (1 - .9)))\n",
    "var_fit = var.fit_transform(X)\n",
    "\n",
    "print ('Low variance removal: ', var_fit.shape)\n",
    "\n",
    "feature_idx = var.get_support()\n",
    "my_features_var = X.columns[feature_idx].tolist()\n",
    "print ('Variance threshold list of features:', my_features_var)\n",
    "\n",
    "selbest = SelectKBest(chi2, k=40)\n",
    "selbest_fit = selbest.fit_transform(X, y)\n",
    "print ('SelectKBest: ', selbest_fit.shape)\n",
    "feature_idx = selbest.get_support()\n",
    "my_features_selbest = X.columns[feature_idx].tolist()\n",
    "print ('SelectKBest list of features:', my_features_selbest)\n",
    "\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "\n",
    "selmodel = SelectFromModel(logreg).fit(X, y)\n",
    "X_new = selmodel.transform(X)\n",
    "print ('SelectFromModel', X_new.shape)\n",
    "\n",
    "feature_idx = selmodel.get_support()\n",
    "my_features_selmodel = X.columns[feature_idx].tolist()\n",
    "print ('SelectKBest list of features:', my_features_selmodel)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
